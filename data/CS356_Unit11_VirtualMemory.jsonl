{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 1, "title": "Unit 11: Virtual Memory", "summary": "Title slide introducing the virtual memory unit with the tagline 'To Cache and Protect'", "main_text": "Unit 11: Virtual Memory\nTo Cache and Protect", "notes_text": null, "keywords": ["virtual memory", "cache", "protection", "CS356"], "images": [{"type": "logo", "description": "CS356 course shield logo with Trojan mascot"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "title_slide"}, "section": "Introduction", "metadata": {"course": "CS356", "unit": 11, "topics": ["virtual memory overview"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 2, "title": "Virtual Memory Idea", "summary": "Section divider introducing the core concept of virtual memory", "main_text": "Virtual Memory Idea", "notes_text": null, "keywords": ["virtual memory", "concept", "introduction"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "Virtual Memory Idea", "metadata": {"course": "CS356", "unit": 11, "topics": ["virtual memory concept"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 3, "title": "Motivation", "summary": "Explains why virtual memory is needed: multiple processes share CPU and memory, creating two problems - protecting processes from each other and handling insufficient memory by using disk storage", "main_text": "Multiple processes take turns executing instructions on CPU cores and share the main memory. This creates two issues:\n1. What if some process tries to access memory used by another?\n   ⇒ Insulate different processes\n2. What if there is not enough memory for all processes?\n   ⇒ Cache data in memory and store the rest on disk", "notes_text": null, "keywords": ["processes", "CPU", "memory sharing", "protection", "disk storage", "cache", "process isolation"], "images": [{"type": "diagram", "description": "CPU with multiple cores showing Process 1, Process 2, Process 3 with their memory layouts (Code, Data, Heap, Stack) and main memory unable to fit all processes"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "concept_with_diagram"}, "section": "Virtual Memory Idea", "metadata": {"course": "CS356", "unit": 11, "topics": ["process isolation", "memory management", "disk swapping"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 4, "title": "Problems of Physical Addresses", "summary": "Identifies three critical problems when processes use physical addresses directly: cannot reference data moved to disk, can see other processes' data, and prevents OS from rearranging memory", "main_text": "Processes using physical addresses...\n• ...wouldn't be able to reference data moved to disk (only data in memory has physical addr.)\n• ...would see data of other processes in their memory address space\n• ...wouldn't allow the OS to rearrange data in memory\n\nExample: Process A has allocated two pages (4 kB blocks of memory): P1 is in memory but P2 is only on disk", "notes_text": null, "keywords": ["physical addresses", "disk", "process isolation", "memory management", "pages", "security"], "images": [{"type": "system_diagram", "description": "Computer architecture showing CPU with dual cores, L3 cache, main memory (RAM) with pages P0 and P1, and disk storage via I/O bus"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "problem_statement_with_diagram"}, "section": "Virtual Memory Idea", "metadata": {"course": "CS356", "unit": 11, "topics": ["physical addressing problems", "memory pages"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 5, "title": "Idea of Virtual Addresses", "summary": "Introduces the solution: give each process an independent virtual address space that the CPU and OS map to physical addresses", "main_text": "An independent virtual address space for each process ... CPU & OS map virtual to physical addresses", "notes_text": null, "keywords": ["virtual addresses", "virtual address space", "VAS", "address mapping", "MMU", "translation"], "images": [{"type": "diagram", "description": "Shows two processes (A and B) each with their own virtual address space (VP0-VP3) mapping to shared physical memory and disk, with CPU architecture"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "solution_with_mapping_diagram"}, "section": "Virtual Memory Idea", "metadata": {"course": "CS356", "unit": 11, "topics": ["virtual addressing", "address translation", "virtual address space"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 6, "title": "Advantages of Virtual Addresses", "summary": "Lists three key benefits: can reference data not in memory (via page faults), processes cannot see each other's data, and OS can freely rearrange physical memory", "main_text": "Processes using virtual addresses...\n• ...can reference data that is not in memory (e.g., VP1) because it has a virtual address\n  ○ The CPU generates a 'page fault' and the OS loads the data from disk\n• ...cannot see data of other processes, e.g., P0 is not accessible through any of the virtual addresses available to Process A\n• ...don't rely on the exact physical location in memory, so the OS can move data in memory and change the mapping (e.g., VP1 to P1)", "notes_text": null, "keywords": ["virtual addresses", "page fault", "process isolation", "memory flexibility", "OS control", "security"], "images": [{"type": "diagram", "description": "Virtual address space of Process A showing virtual pages mapping to physical memory and disk, with arrows indicating page fault handling"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "benefits_with_diagram"}, "section": "Virtual Memory Idea", "metadata": {"course": "CS356", "unit": 11, "topics": ["virtual addressing benefits", "page faults", "memory protection"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 7, "title": "Virtual Address Space", "summary": "Defines the virtual address space (VAS) as the complete set of addresses available to a process, shared by all threads but protected from other processes. Clarifies that Process = Threads + VAS, and Thread = registers + stack", "main_text": "Everything is easy from the point of view of a process... all memory addresses are available to the process!\n\nThis is the virtual address space (VAS), shared by all threads in the process but protected from other processes:\n\nProcess = Running instance of a program = Threads + VAS\nThread = register values + stack\n\nEach thread has its own stack, but all memory of the process is accessible! (Use locks to avoid data races...)", "notes_text": null, "keywords": ["virtual address space", "VAS", "process", "thread", "stack", "shared memory", "data races", "locks"], "images": [{"type": "memory_layout", "description": "Diagram showing VAS from 0x00000000 to 0xffffffff with sections: Code, Data, Heap, Stack1, Stack2, Mapped I/O"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "definition_with_memory_layout"}, "section": "Virtual Memory Idea", "metadata": {"course": "CS356", "unit": 11, "topics": ["virtual address space", "process model", "thread model", "memory layout"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 8, "title": "Mapping VAS to PAS", "summary": "Explains how virtual address spaces are divided into pages (usually 4 kB) that map to frames in physical memory. Pages can be unallocated, uncached (on disk), or cached (in memory). Introduces page faults as the mechanism for loading uncached pages", "main_text": "Virtual address spaces are broken into blocks called 'pages' (usually 4 kB)\nPhysical address space is broken into blocks called 'frames' (same size)\n\nVAS usually much larger: pages can be\n• Unallocated: Never accessed (yet), e.g., unused portions of stack/heap\n• Uncached: Allocated but on disk\n• Cached: Allocated and in memory\n\nOS keeps LRU pages in memory frames\n\nPage Fault: When a process accesses a page that is not in memory, the OS loads it into a frame", "notes_text": null, "keywords": ["pages", "frames", "page size", "4KB", "unallocated", "uncached", "cached", "LRU", "page fault", "swap file"], "images": [{"type": "complex_diagram", "description": "Shows three virtual address spaces (Process A, B, C) mapping to physical address space with numbered frames (0-6), plus swap file on disk. Color-coded to show cached vs uncached pages"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "technical_diagram"}, "section": "Virtual Memory Idea", "metadata": {"course": "CS356", "unit": 11, "topics": ["pages", "frames", "page tables", "memory mapping", "page replacement"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 9, "title": "Address Translation", "summary": "Section divider introducing the mechanism of translating virtual addresses to physical addresses", "main_text": "Address Translation", "notes_text": null, "keywords": ["address translation", "virtual to physical", "MMU"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "Address Translation", "metadata": {"course": "CS356", "unit": 11, "topics": ["address translation"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 10, "title": "CPU Translates using Page Tables", "summary": "Shows how the CPU uses page tables to translate virtual addresses to physical addresses. Each process has its own page table maintained by the OS and read by the MMU (Memory Management Unit)", "main_text": "CPU Memory Management Unit (MMU) reads page tables prepared by OS (in memory)\n\nExample instruction: movl 0x40008, %eax\nVirtual Address 0x40008 (within a page)\nPhysical Addr 0x6008 (within frame where page was saved)", "notes_text": null, "keywords": ["CPU", "MMU", "Memory Management Unit", "page table", "address translation", "PTE", "page table entry"], "images": [{"type": "complex_diagram", "description": "Shows three process virtual address spaces mapping through their respective page tables to physical memory frames and swap file. Page tables show mappings like 'Page 0 (rx) ⇒ Frame 6', with valid/invalid entries"}], "layout": {"num_text_boxes": 3, "num_images": 1, "dominant_visual_type": "system_architecture"}, "section": "Address Translation", "metadata": {"course": "CS356", "unit": 11, "topics": ["page tables", "MMU", "translation process", "PTEs"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 11, "title": "Address Translation", "summary": "Detailed breakdown of address translation process: Virtual address is split into VPN (Virtual Page Number) and page offset. VPN indexes the page table to get PFN (Physical Frame Number). Physical address = PFN | Page Offset", "main_text": "1. Virtual address 0x000022d8 (VA) is split into virtual page number 0x00002 (VPN) and page offset 0x2d8 (PO, 'offset within page')\n\n2. VPN is used as an index into the page table to read the physical frame number 0x0021b (PFN)\n\n3. Physical address = PFN | PO = 0x0021b2d8\n\nFor 4 kB pages:\npage offset bits = log2(4k) = 12\nVPN bits = VA bits - 12 = 20\n⇒ 2^20 entries ... 4 MB if 4 bytes/entry\n\nPTBR = Page Table Base Register (CR3 on Intel)", "notes_text": null, "keywords": ["VPN", "virtual page number", "PFN", "physical frame number", "page offset", "PTBR", "CR3", "page table entries", "address translation"], "images": [{"type": "technical_diagram", "description": "Bit-level breakdown showing 32-bit virtual address split into 20-bit VPN and 12-bit offset, page table lookup, and resulting 32-bit physical address. Shows PTE structure with rwx permission bits"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "technical_detail"}, "section": "Address Translation", "metadata": {"course": "CS356", "unit": 11, "topics": ["VPN", "PFN", "page offset", "address bits", "PTBR"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 12, "title": "Example", "summary": "Works through a concrete example with 8-bit VAs, 10-bit PAs, and 32-byte pages. Shows the translation process step-by-step", "main_text": "System with 8-bit VAs, 10-bit PAs, and 32-byte pages\n\nVA split into VPN (bits 7-5) and Offset (bits 4-0)\nPage Table lookup uses VPN as index\nPA formed from PFN (from page table) and Offset", "notes_text": null, "keywords": ["example", "address translation", "VPN", "PFN", "page table lookup"], "images": [{"type": "worked_example", "description": "Detailed diagram showing VA 0x2D being translated through page table to PA, with VPN extraction, page table with 8 entries showing valid bits and PFNs, physical memory layout, and virtual address space layout"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "worked_example"}, "section": "Address Translation", "metadata": {"course": "CS356", "unit": 11, "topics": ["address translation example", "page table lookup"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 13, "title": "Exercise", "summary": "Practice exercise for students to work through address translation problems with the same system parameters", "main_text": "• Consider the same system with 8-bit VAs, 10-bit PAs, and 32-byte pages.\n• Fill in the table below showing the corresponding physical or virtual address based on the other. If no translation can be made, indicate 'INVALID'\n\nGiven page table with entries:\nVPN 0: V=0, Entry=0x0E\nVPN 1: V=1, Entry=0x1E\nVPN 2: V=1, Entry=0x16\nVPN 3: V=1, Entry=0x06\nVPN 4: V=0, Entry=0x0B\nVPN 5: V=1, Entry=0x1F\nVPN 6: V=0, Entry=0x15\nVPN 7: V=0, Entry=0x0A\n\nAnswers:\nVA 0x2D = 0010 1101 → PA 0x3CD\nVA 0x7A = 0111 1010 → PA 0x0DA\nVA 0xEF = 1110 1111 → INVALID\nPA 0x3E8 → VA 0xA8", "notes_text": null, "keywords": ["exercise", "practice", "address translation", "valid bit", "invalid pages"], "images": [{"type": "table", "description": "Page table with V bit and Entry columns, plus exercise table with VA and PA columns to fill in"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "exercise"}, "section": "Address Translation", "metadata": {"course": "CS356", "unit": 11, "topics": ["address translation practice"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 14, "title": "Context Switch & PT", "summary": "Explains that each process needs its own page table, and on context switch the OS updates the PTBR to point to the new process's page table. Example shows how Linux stores PTBR in task_struct", "main_text": "Each process has its own VAS ⇒ needs its own PT\n\n• On context switch to new process, the OS modifies the PTBR to point to a new PT\n• E.g., Linux saves the PTBR in the 'task_struct' of the process", "notes_text": null, "keywords": ["context switch", "page table", "PTBR", "task_struct", "process switching", "multiple page tables"], "images": [{"type": "system_diagram", "description": "Complex diagram showing CPU with MMU performing address translation, two process page tables (PT1 and PT2) in OS kernel memory, physical memory with code/data/stack pages, and task structs containing PTBR values"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "system_architecture"}, "section": "Address Translation", "metadata": {"course": "CS356", "unit": 11, "topics": ["context switching", "page table management", "PTBR"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 15, "title": "Additional data in PTEs", "summary": "Describes the additional bits stored in page table entries beyond just the PFN: Valid/Present bit, Protection bits (rwx), Modified/Dirty bit, Referenced bit for LRU, and Cacheable bit", "main_text": "PTEs usually require 4 or 8 bytes each\n\n• Valid bit: Whether the page is in memory. If 0, the rest of the PTE is used by OS to store the location of the page on disk\n\n• Modified/Dirty: Whether it was modified. To implement a write back policy\n\n• Referenced Bit: used for pseudo-LRU, e.g.\n  ○ If a page is referenced, set referenced bit to 1\n  ○ Keep a counter in an OS data structure for each page ... periodically:\n    - If bit is 0 (not accessed), add 1 to counter\n    - If bit is 1 (accessed) set counter to 0\n    - Do this for all pages, then set bits to 0\n  ○ Evict page with max counter\n\n• Protection: Read/Write/eXecute", "notes_text": null, "keywords": ["PTE", "page table entry", "valid bit", "dirty bit", "modified bit", "referenced bit", "LRU", "protection bits", "rwx", "write back"], "images": [{"type": "diagram", "description": "PTE structure showing Page Frame Number field and control bits: Valid/Present, Modified/Dirty, Referenced, Cacheable, Protection Bits (rwx)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "data_structure"}, "section": "Address Translation", "metadata": {"course": "CS356", "unit": 11, "topics": ["PTE structure", "page replacement", "memory protection", "LRU algorithm"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 16, "title": "Saving Space with Multilevel Page Tables", "summary": "Section divider introducing multilevel page tables as a space-saving technique", "main_text": "Saving Space with Multilevel Page Tables", "notes_text": null, "keywords": ["multilevel page tables", "space optimization", "hierarchical page tables"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "Multilevel Page Tables", "metadata": {"course": "CS356", "unit": 11, "topics": ["multilevel page tables"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 17, "title": "Single-Level Page Tables", "summary": "Identifies the problem with single-level page tables: one row for each possible virtual page means mostly invalid entries and wasted memory, especially with one table per process. Uses phone book analogy", "main_text": "One row (PTE) for each virtual page\n\n• Most entries are usually invalid, since a process uses just small fraction of their VA spaces\n• Lots of wasted memory, especially since the OS must allocate a page table for each process\n\nAnalogy of phone number to user id translation: we are allocating an entry for each possible phone number!", "notes_text": null, "keywords": ["single-level page tables", "memory waste", "sparse address space", "page table size problem"], "images": [{"type": "diagram", "description": "Simple page table showing all possible VPNs with mostly invalid entries (v=0), plus phone book analogy showing entries for all possible phone numbers"}], "layout": {"num_text_boxes": 2, "num_images": 2, "dominant_visual_type": "problem_statement"}, "section": "Multilevel Page Tables", "metadata": {"course": "CS356", "unit": 11, "topics": ["single-level page tables", "memory overhead"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 18, "title": "2-Level Page Tables: Analogy", "summary": "Introduces two-level page tables using phone number analogy: 1st level indexes by area code, 2nd level by local number. Only allocate 2nd level tables for area codes actually used. Dramatically reduces space: 10^3 + 2×10^7 entries instead of 10^10", "main_text": "1st Level Table (Page Directory)\n• One row for each area code\n• Each row can either be NULL or point to 2nd level table\n\n2nd Level Tables\n• Allocated if at least one contact has a specific area code\n• Local phone number used as index\n• Rows have the actual contact ids\n\nAdvantage: If only 2 area codes are used, then only 10^3 + 2×10^7 entries are allocated, instead of 10^10", "notes_text": null, "keywords": ["two-level page tables", "page directory", "hierarchical structure", "space savings", "sparse allocation"], "images": [{"type": "diagram", "description": "Phone book analogy showing 1st level indexed by area code (000, 213, 323, etc.) with pointers to 2nd level tables indexed by local phone number, with NULL entries for unused area codes"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "analogy_diagram"}, "section": "Multilevel Page Tables", "metadata": {"course": "CS356", "unit": 11, "topics": ["two-level page tables", "page directory", "memory efficiency"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 19, "title": "3-Level Page Tables: Analogy", "summary": "Extends to three levels: area code, first 3 digits of local number, last 4 digits. For sparse usage (only 213-740-xxxx and 323-821-xxxx), further reduces space to 10^3 + 2×10^3 + 2×10^4 entries", "main_text": "We split phone numbers into 3 fields\n\nAdvantage: If numbers are only in 213-740-xxxx and 323-821-xxxx, we allocate 10^3 + 2×10^3 + 2×10^4 entries", "notes_text": null, "keywords": ["three-level page tables", "hierarchical indexing", "extreme space savings", "sparse address spaces"], "images": [{"type": "diagram", "description": "Three-level hierarchy: 1st level by area code, 2nd level by first 3 digits, 3rd level by last 4 digits. Shows only allocated paths for 213-740 and 323-821"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "hierarchical_diagram"}, "section": "Multilevel Page Tables", "metadata": {"course": "CS356", "unit": 11, "topics": ["three-level page tables", "hierarchical page tables"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 20, "title": "3-Level Page Table", "summary": "Shows actual implementation: VPN is split into 3 indices (Idx1, Idx2, Idx3). PTEs of intermediate levels contain addresses of next-level tables. Only the final level PTEs contain actual PFNs", "main_text": "• PTEs of intermediate levels have address of next table\n• PTEs of last level have PFNs\n\nExample with VPN split into:\n- Idx1: 6 bits (bits 31-26) → Page Directory\n- Idx2: 7 bits (bits 25-19) → Level 2 table\n- Idx3: 7 bits (bits 18-12) → Level 3 table\n- Offset: 12 bits (bits 11-0)\n\nPTBR/CR3 points to Page Directory start address", "notes_text": null, "keywords": ["three-level page tables", "page directory", "intermediate tables", "hierarchical lookup", "PFN", "PTBR"], "images": [{"type": "technical_diagram", "description": "Shows 3-level page table walk: PTBR points to Page Directory, which has entry PD[0x3f] pointing to PT2, which has entry PT2[0x40] pointing to PT3, which has entry PT3[0x35] containing the final PFN. Only allocated subtrees shown"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "technical_architecture"}, "section": "Multilevel Page Tables", "metadata": {"course": "CS356", "unit": 11, "topics": ["page table walk", "hierarchical translation", "page directory"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 21, "title": "Example: Intel i7", "summary": "Real-world example: Intel i7 uses 4 levels of page tables for 48-bit virtual addresses (out of 64-bit architecture), with 9 bits per level and 12-bit page offset for 4KB pages", "main_text": "• 64-bit architecture but addresses use only 48 bits\n• 4 levels of page tables\n\nVirtual Address (48 bits):\n- Index 1: 9 bits → First Level\n- Index 2: 9 bits → Second Level  \n- Index 3: 9 bits → Third Level\n- Index 4: 9 bits → Fourth Level\n- Offset: 12 bits → Page (4 kB)\n\nEach level has 2^9 = 512 PTEs", "notes_text": null, "keywords": ["Intel i7", "4-level page tables", "48-bit addressing", "x86-64", "CR3", "page walk"], "images": [{"type": "technical_diagram", "description": "Intel i7 page table structure showing CR3 register pointing to 4-level hierarchy, each level with 512 entries, culminating in 4KB page with referenced byte"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "real_world_example"}, "section": "Multilevel Page Tables", "metadata": {"course": "CS356", "unit": 11, "topics": ["Intel architecture", "x86-64 paging", "4-level page tables"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 22, "title": "Saving Time with TLBs", "summary": "Section divider introducing Translation Lookaside Buffers (TLBs) as a solution to speed up address translation", "main_text": "Saving Time with TLBs", "notes_text": null, "keywords": ["TLB", "Translation Lookaside Buffer", "performance", "caching"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "TLBs", "metadata": {"course": "CS356", "unit": 11, "topics": ["TLB"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 23, "title": "Cost of a Memory Access", "summary": "Identifies the performance problem: with k-level page tables, every memory access requires k+1 memory accesses (k for page table walk, 1 for actual data). Need caching to speed this up", "main_text": "With k-level page tables, the MMU has to access k PTEs, one in each page table level...\n\n• To perform 1 memory access, we need k + 1 accesses (page tables + actual data with physical address)\n• Some of the PTEs (and the data itself) may be in the cache\n\nHow can we speed up the translation process?", "notes_text": null, "keywords": ["performance", "memory access cost", "page table walk", "latency", "multiple accesses"], "images": [{"type": "diagram", "description": "Flow diagram showing Processor → MMU → multiple Page Table accesses → Cache/memory → Data"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "problem_statement"}, "section": "TLBs", "metadata": {"course": "CS356", "unit": 11, "topics": ["address translation performance", "page table access overhead"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 24, "title": "Translation Lookaside Buffer", "summary": "Introduces TLB as a cache of VPN→PFN translations. On hit, get PFN immediately. On miss, perform full page table walk, then add entry to TLB using LRU eviction if full", "main_text": "TLB Solution: A cache of VPN ⇒ PFN translations! We use the VPN to search...\n\n• On hit, we have the PFN of the physical address\n\n• On miss:\n  ○ We access all page table levels in memory (some may be hits in cache)\n  ○ Once we have the PFN, we add a PTE entry 'VPN ⇒ PFN' to the TLB\n  ○ If the TLB is full, we evict the LRU entry (to make translation of addresses within frequently accessed pages faster)", "notes_text": null, "keywords": ["TLB", "Translation Lookaside Buffer", "VPN to PFN", "cache", "LRU eviction", "translation cache"], "images": [{"type": "system_diagram", "description": "Two diagrams showing TLB hit path (Processor → Translation → PA → Cache/memory → Data) and TLB miss path (with additional Page Table access then adding entry to TLB)"}], "layout": {"num_text_boxes": 2, "num_images": 2, "dominant_visual_type": "solution_diagram"}, "section": "TLBs", "metadata": {"course": "CS356", "unit": 11, "topics": ["TLB", "address translation caching", "TLB hit", "TLB miss"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 25, "title": "TLB Organization", "summary": "TLBs are organized like caches (fully associative, set-associative, or direct mapped) but with no block offset since they cache individual PTEs. VPN used for tag and set index", "main_text": "Similar organization to caches\n\n• Fully associative, set-associative, direct mapped\n  ○ VPN used for tag and set index\n\n• But no block offset: we read an entire PTE (a line of the TLB)\n  ○ The PTE gives us the PFN and permission bits\n\nExample: Virtual Address 7ffe1 6d8\n- VPN = 0x7ffe1 → used for TLB lookup\n- Page Offset = 0x6d8 → copied to physical address\n\nOn TLB hit: PTE contains PFN = 0x308ac\nPhysical Address = 308ac 6d8", "notes_text": null, "keywords": ["TLB organization", "fully associative", "set associative", "VPN", "tag", "PTE", "no block offset"], "images": [{"type": "technical_diagram", "description": "Shows virtual address split into VPN and offset, fully associative TLB with tag comparisons, and resulting physical address formation"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "technical_detail"}, "section": "TLBs", "metadata": {"course": "CS356", "unit": 11, "topics": ["TLB structure", "associativity", "TLB lookup"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 26, "title": "Set Associative TLB", "summary": "Example of 4-way set-associative TLB with 64 entries (16 sets, 4 ways). VPN split into tag and set index. Shows concrete example with VPN=0x7ffe1, set=1, tag=0x7ffe matching in way 2", "main_text": "Example:\nN = 64 entries\nK = 4 ways\nS = N/4 = 16 sets\n\nVPN split into:\n- Tag (upper bits): 0x7ffe\n- Set index (lower bits): 1\n\nLookup searches all 4 ways in set 1 for matching tag\nHit in Way 2 returns PFN 0x308ac", "notes_text": null, "keywords": ["set associative TLB", "4-way", "sets", "ways", "tag comparison", "parallel lookup"], "images": [{"type": "technical_diagram", "description": "Detailed 4-way set-associative TLB diagram showing 16 sets, with VPN breakdown into tag and set index, parallel tag comparisons in all 4 ways, with hit indicated in way 2"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "detailed_example"}, "section": "TLBs", "metadata": {"course": "CS356", "unit": 11, "topics": ["set-associative TLB", "TLB lookup process"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 27, "title": "TLB Exercise", "summary": "Practice exercise: given 2-way set-associative TLB with 4 sets, 256-byte pages, 16-bit addresses, translate virtual address 0x7E85 to physical address, and reverse translation for 0x3020", "main_text": "• 2-way set associative TLB (K = 2)\n• S = 4 sets\n• Page size is 256 bytes\n• 16-bit VAs and PAs\n\nWhat is the physical address of virtual address 0x7E85?\nAnswer: 0x9585\n\nWhat is the virtual address of physical address 0x3020?\nAnswer: 0xA920", "notes_text": null, "keywords": ["TLB exercise", "address translation", "set associative", "practice problem"], "images": [{"type": "table", "description": "TLB table showing Index (0-3), V bit, Tag, and PPFN for 2-way set-associative TLB with 8 total entries"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "exercise"}, "section": "TLBs", "metadata": {"course": "CS356", "unit": 11, "topics": ["TLB practice", "bidirectional translation"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 28, "title": "Context Switch & TLB", "summary": "Each process needs different TLB entries since they have different page tables. Two solutions: (1) Flush TLB on context switch, or (2) Tag TLB entries with ASID (Address Space ID / Process ID) so multiple processes can coexist in TLB", "main_text": "Each process has a different VAS and a different page table (mapping its VAS to portions of the PAS)...\n\n• Since the TLB caches entries of PT, each process must use different TLB entries!\n\n• Two alternative solutions:\n  ○ Flush/empty the TLB at each context switch (new entries will be added after misses)\n  ○ Store the process ID in the TLB: for a hit, we need both the tag and process ID to match (processes compete for TLB entries)\n\nASID = Address Space ID (aka PID = Process ID) is UNIQUE per process.", "notes_text": null, "keywords": ["context switch", "TLB flush", "ASID", "process ID", "TLB coherence", "process isolation"], "images": [{"type": "diagram", "description": "TLB structure with ASID field showing how multiple processes can have entries in TLB simultaneously, with ASID used in tag comparison"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "technical_solution"}, "section": "TLBs", "metadata": {"course": "CS356", "unit": 11, "topics": ["TLB and context switching", "ASID", "TLB tagging"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 29, "title": "Advanced Uses of VM Mappings", "summary": "Section divider introducing advanced virtual memory techniques", "main_text": "Advanced Uses of VM Mappings", "notes_text": null, "keywords": ["virtual memory", "advanced techniques", "memory mapping"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "Advanced VM Uses", "metadata": {"course": "CS356", "unit": 11, "topics": ["advanced virtual memory"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 30, "title": "Sharing Memory Between Processes", "summary": "OS can map virtual pages of different processes to the same physical frame to share memory. Used for shared libraries, cooperating processes, and fork() with copy-on-write", "main_text": "Using page tables, OS can map virtual pages of different processes to the same physical frame!\n\nUsed to share memory among processes (threads always share the entire VAS):\n• Load shared library code only once\n• Cooperating processes, e.g., Python's multiprocessing.shared_memory\n• fork(): duplicate PT & copy-on-write", "notes_text": null, "keywords": ["shared memory", "memory sharing", "shared libraries", "fork", "copy-on-write", "IPC", "interprocess communication"], "images": [{"type": "diagram", "description": "Shows Process A and Process B virtual address spaces with different virtual pages mapping to the same physical frame 6, demonstrating code sharing (both have 1-Code mapped to frame 6)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "application_diagram"}, "section": "Advanced VM Uses", "metadata": {"course": "CS356", "unit": 11, "topics": ["memory sharing", "shared libraries", "copy-on-write"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 31, "title": "Memory Mapped Files", "summary": "OS can map virtual pages directly to file blocks. Pages loaded on demand from file instead of swap space. Enables file caching in memory, private copy-on-write mappings, and zero-demand allocation", "main_text": "Using page tables, OS can map virtual pages to contiguous 4 kB blocks of a file!\n\n• As soon as the process tries to access a page mapped to FileX, OS loads the page into physical memory\n• File pages cached in memory/caches\n• Private mapping for copy-on-write\n• Zero-demand mapping: allocate pages of 0's on demand", "notes_text": null, "keywords": ["memory mapped files", "mmap", "file caching", "copy-on-write", "zero-demand", "page cache"], "images": [{"type": "diagram", "description": "Shows process virtual address space with pages 3 and 4 mapped to File X on disk, with pages cached in physical memory frames 0 and 3. File X also shown in swap file area"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "application_diagram"}, "section": "Advanced VM Uses", "metadata": {"course": "CS356", "unit": 11, "topics": ["memory mapped files", "file I/O", "demand paging"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 32, "title": "Example: ./a.out", "summary": "When starting a program, .text and .data are mapped as private (copy-on-write) from executable file, shared library code/data mapped as shared, and heap/stack are demand-zero regions", "main_text": "When we start a program with ./a.out\n\n• .text (binary code) and .data (global vars) parts of executable file mapped as 'private'\n  ○ If the process makes changes, a copy of the affected pages is made (and used in the VAS of the process)\n  ○ Otherwise, all processes started by ./a.out share the same pages\n\n• Heap and stack are demand-zero\n\n• .text and .data of library files (e.g., libc.so) are 'shared' (changes visible to other proc.)", "notes_text": null, "keywords": ["program loading", "executable", ".text", ".data", "shared libraries", "demand-zero", "copy-on-write", "libc"], "images": [{"type": "memory_layout", "description": "Process memory layout showing regions from low to high addresses: Code (.text), Initialized data (.data), Uninitialized data (.bss), Runtime heap, Memory-mapped shared libraries, User stack, with annotations for private/shared and demand-zero/file-backed"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "real_world_application"}, "section": "Advanced VM Uses", "metadata": {"course": "CS356", "unit": 11, "topics": ["program loading", "executable format", "memory layout"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 33, "title": "Virtual Memory & CPU Caches", "summary": "Section divider introducing the interaction between virtual memory and CPU caches", "main_text": "Virtual Memory & CPU Caches", "notes_text": null, "keywords": ["virtual memory", "CPU caches", "cache coherence", "address translation"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["virtual memory and caching"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 34, "title": "Cache are Physically Addressed", "summary": "CPU caches typically sit between MMU and memory, using physical addresses. Must translate VA to PA before cache lookup. Page offset unchanged allows VIPT optimization", "main_text": "CPU caches (L1, L2, L3) usually sit between MMU and memory\n\nThe CPU needs to translate the VA to a PA (and check permission bits) before searching into the cache...\n\nProcess:\n1. Virtual Address with VPN and Page Offset\n2. TLB lookup or page table walk to get PFN\n3. Form Physical Address = PFN + Page Offset\n4. Search L1/L2/L3 caches with Physical Address\n5. On cache miss, access main memory", "notes_text": null, "keywords": ["physically addressed cache", "cache lookup", "MMU", "physical address", "cache access"], "images": [{"type": "flow_diagram", "description": "Shows address translation flow: VA → MMU (with TLB and page table) → PA → Caches → Memory. Includes bit-level breakdown of virtual and physical addresses"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "system_flow"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["cache addressing", "physical caches", "memory hierarchy"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 35, "title": "Optimization: VIPT Caches", "summary": "Virtually Indexed Physically Tagged caches allow parallel cache set selection and address translation. Page offset determines cache set index, but cache tag still uses physical address. Works when page offset bits ≥ cache set index + block offset bits", "main_text": "Virtually Indexed: Since the page offset doesn't change, we know the cache set idx already from the VA when:\n(Page Offset bits) ≥ (Cache Set Idx bits) + (Block Offset Bits)\n\nPhysically Tagged: The cache tag to search still depends on the PFN, so we have to wait for MMU translation to search the cache...\n\n⇒ In parallel (thus faster):\n• Activation of cache set\n• MMU translation", "notes_text": null, "keywords": ["VIPT", "virtually indexed physically tagged", "parallel lookup", "cache optimization", "performance"], "images": [{"type": "technical_diagram", "description": "Shows how page offset bits from VA can be used for cache set selection while MMU translates VPN to PFN in parallel. Highlights the overlap region allowing simultaneous operations"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "optimization_technique"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["VIPT caches", "cache performance", "parallel operations"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 36, "title": "VIPT: Limitations", "summary": "VIPT requires page size ≥ (# cache sets) × (block size). For direct-mapped cache, size ≤ page size. For k-way set-associative, size ≤ k × page size. Limits cache size (typically 4KB to 32KB for L1)", "main_text": "(Page Offset bits) ≥ (Cache Set Idx bits) + (Block Offset Bits)\nimplies\n2^(Page Offset bits) ≥ 2^(Cache Set Idx bits + Block Offset Bits)\ni.e.,\n(Page Size) ≥ (# Cache Sets) * (Block Size)\n\n⇒ The size of a direct-mapped VIPT (1 block per set) cache cannot be greater than the size of a page (usually 4 kB)\n\n⇒ The size of a 2-way set-associative VIPT (2 blocks per set) cache cannot be greater than twice size of a page, and so on...\n\n(But having more than ≈ 8 ways is usually not desirable)", "notes_text": null, "keywords": ["VIPT limitations", "cache size", "page size", "set associativity", "design constraints"], "images": [{"type": "diagram", "description": "Bit-level diagram showing physical address with PFN and page offset, and how cache set index must fit within page offset for VIPT to work"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "constraint_analysis"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["VIPT constraints", "cache sizing"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 37, "title": "VIPT: Aliasing Problem", "summary": "VIPT has aliasing problem when page offset bits < cache set index + block offset bits. Different virtual addresses mapping to same physical address can select different cache sets, creating multiple inconsistent copies", "main_text": "VIPT has an aliasing problem when\n(Page Offset bits) < (Cache Set Idx bits) + (Block Offset Bits)\n\n• Different virtual addresses, e.g., 7ffe16d8 and 8dda26d8, can map to the same physical addr. 308ac6d8 (e.g., when processes share memory)\n\n• But using the VA for the cache index (16 or 26 instead of c6) selects different cache sets\n  ○ Multiple copies (alias) of data from same physical address\n  ○ One copy could be dirty & the other out-of-date!", "notes_text": null, "keywords": ["aliasing problem", "VIPT", "cache coherence", "shared memory", "multiple copies", "inconsistency"], "images": [{"type": "technical_diagram", "description": "Shows two different virtual addresses with different VPN low bits (16 vs 26) but mapping to same PFN (308ac), demonstrating how they would select different cache sets (set 16 vs set 26)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "problem_illustration"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["cache aliasing", "VIPT problems", "cache coherence issues"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 38, "title": "VIPT: Page Coloring", "summary": "Solution to aliasing: OS ensures that when different virtual pages map to the same physical frame, the VPN bits used by cache set index are identical. Called page coloring", "main_text": "The aliasing problem can be resolved by making sure that:\n\n• When different virtual pages are mapped to the same memory frame...\n• ...the VPN bits used by the cache set index are the same\n\nPrevious Example: VIPT uses cache set indices 16 for 7ffe16d8 and 26 for 8dda26d8...\n\nThe OS can avoid this by assigning a different VA to the region in the VAS of the second process, e.g., 8dda16d8 (now the same set idx 16 is used)", "notes_text": null, "keywords": ["page coloring", "aliasing solution", "cache set selection", "OS memory management", "VIPT fix"], "images": [{"type": "diagram", "description": "Shows how OS assigns virtual addresses such that the low-order VPN bits match when pages map to same physical frame, ensuring consistent cache set selection"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "solution_technique"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["page coloring", "cache management", "aliasing prevention"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 39, "title": "Core i7 Address Translation", "summary": "Complete diagram showing Intel Core i7 address translation flow from 48-bit VA through 4-level page tables and L1 TLB (16 sets, 4 entries/set) to physical address, then through L1 cache (64 sets, 8 lines/set) to L2/L3/memory", "main_text": "Complete Intel Core i7 memory access flow:\n1. 48-bit Virtual Address split into VPN1-VPN4 (9 bits each) and VPO (12 bits)\n2. L1 TLB lookup (16 sets, 4-way) using TLBT and TLBI from VPN\n3. On TLB hit: get PPN directly\n4. On TLB miss: walk 4-level page tables (CR3 → PTE → PTE → PTE → PTE → PPN)\n5. Form physical address from PPN + PPO\n6. L1 cache lookup (64 sets, 8-way) using CT and CI from PA\n7. On L1 miss: access L2, L3, and main memory", "notes_text": null, "keywords": ["Intel i7", "Core i7", "complete translation", "TLB", "cache hierarchy", "page table walk", "x86-64"], "images": [{"type": "comprehensive_diagram", "description": "Complex system diagram showing complete address translation and memory hierarchy: VA breakdown, TLB structure, page table hierarchy, PA formation, L1 cache structure, and connection to L2/L3/memory"}], "layout": {"num_text_boxes": 1, "num_images": 1, "dominant_visual_type": "comprehensive_system_diagram"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["complete translation flow", "Intel architecture", "memory hierarchy"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 40, "title": "Addresses of Page Tables", "summary": "Page tables themselves are accessed using physical addresses (PTBR and intermediate PTE pointers) to avoid chicken-and-egg problem. PTEs can be cached like any other data, causing cache hits/misses during page table walks", "main_text": "Physical addresses are used in\n• PTBR register (address of page directory)\n• PTEs of multilevel page tables (addresses of next-level PT)\n\nsince page tables are necessary for VA translation (otherwise, a 'Catch-22').\n\nPTEs are cached like any other data from memory... we can have cache hits/misses during a page table walk.", "notes_text": null, "keywords": ["page table addresses", "physical addressing", "PTBR", "PTE caching", "page table walk", "cache hits"], "images": [{"type": "technical_diagram", "description": "Shows multilevel page table walk with PTBR containing physical address, intermediate PTEs containing physical addresses of next level, with annotations showing where cache hits/misses can occur"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "technical_detail"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["page table storage", "physical addressing", "PTE caching"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 41, "title": "Page Faults", "summary": "Section divider introducing page fault handling", "main_text": "Page Faults", "notes_text": null, "keywords": ["page fault", "exception handling", "disk I/O"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "Page Faults", "metadata": {"course": "CS356", "unit": 11, "topics": ["page faults"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 42, "title": "Page Fault Scenarios", "summary": "CPU raises page fault when: (1) PTE valid bit = 0 (page not allocated or not in memory), or (2) incompatible permission bits. OS handler kills process if invalid access, otherwise allocates new page or loads from disk", "main_text": "When the MMU finds...\n• a PTE with valid bit = 0 (page not allocated or currently not in memory)\n• a PTE with valid bit = 1 but incompatible permission bits (e.g., PTE has 'rx' but instruction writes)\n\n...the CPU (HW) raises a 'page fault.' The OS page fault handler (SW) is invoked to:\n1. Kill the process if the page was not mapped in the VAS (e.g., with mmap)\n2. Kill the process if op was not allowed\n3. Allocate a new page on demand, or load the existing page from disk", "notes_text": null, "keywords": ["page fault", "invalid page", "protection exception", "segmentation fault", "permission bits", "OS handler"], "images": [{"type": "diagram", "description": "Process virtual memory diagram showing three scenarios: (1) Segmentation fault - accessing nonexistent page, (2) Protection exception - writing to read-only page, (3) Normal page fault - accessing valid but uncached page"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "scenario_analysis"}, "section": "Page Faults", "metadata": {"course": "CS356", "unit": 11, "topics": ["page fault types", "exception handling", "memory protection"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 43, "title": "Loading a Page from Disk", "summary": "Page fault handling process: (1-2) TLB miss and invalid PTE trigger page fault, (3) If memory full, evict LRU page with writeback, invalidate its TLB entry, (4) Load needed page from disk, update PTE, (5) Restart instruction, (6) Load new PTE into TLB", "main_text": "• Page fault: TLB miss (1) & invalid PTE in PT (2)\n• If there is no empty frame: select a victim (LRU), write back (if dirty or never swapped), set its PTE to invalid, invalidate its TLB entry if present (3)\n• Load the desired page into the memory frame (overwriting cached data of old page), set its PTE with valid=1 and VPN of memory frame (4)\n• Restart the instruction (5), which will load PTE into TLB (6)", "notes_text": null, "keywords": ["page fault handling", "page replacement", "LRU", "writeback", "disk I/O", "TLB invalidation"], "images": [{"type": "flow_diagram", "description": "Detailed system diagram showing: CPU → MMU → TLB miss (1) → Page Table with invalid PTE (2) → OS Page Fault Handler → Evict victim page (3) → Disk Driver loading new page (4) → Memory updated → Restart instruction (5) → TLB updated (6)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "process_flow"}, "section": "Page Faults", "metadata": {"course": "CS356", "unit": 11, "topics": ["page fault handling", "page replacement", "disk I/O"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 44, "title": "Page Eviction Bookkeeping", "summary": "Critical coherence requirement: when evicting a page, must invalidate its TLB entry and remove/overwrite its cached data. OS provides software coherence by invalidating TLB entries after modifying PTEs (invlpg instruction)", "main_text": "Copies (TLB entry & cached data) must be removed when the actual data change (PTE & page frame data)!\n\n• If we didn't invalidate the TLB entry of the evicted page, the MMU would find it at the old page frame (where the new page was saved)\n\n• If we didn't remove the data of the evicted page from the cache (or overwrite it when loading the new page), we would retrieve it when accessing the new page\n\n'Software coherence of TLB':\n• After modifying a PTE, the OS invalidates its TLB entry: invlpg 0x123\n• If TLB entries are not tagged with process ID, the OS invalidates all the TLB entries on context switch\n• TLB is usually write-through: changes to the modified bit are propagated to page table", "notes_text": null, "keywords": ["cache coherence", "TLB coherence", "TLB invalidation", "invlpg", "software coherence", "bookkeeping"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "technical_explanation"}, "section": "Page Faults", "metadata": {"course": "CS356", "unit": 11, "topics": ["TLB coherence", "cache coherence", "OS responsibilities"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 45, "title": "Page Hits in TLB, PT, CPU Cache", "summary": "Analysis of possible hit/miss combinations: TLB hit + PT invalid = impossible (incoherent), TLB miss + PT invalid + cache hit = impossible (data not in memory), other combinations explained", "main_text": "During a memory access (to a specific page) can we observe the following combinations of hit/miss?\n\nTLB Hit + PT Hit (valid) + Cache Hit/Miss → YES (page in memory, PTE in TLB, data may/may not be in cache)\n\nTLB Hit + PT Miss (invalid) + Cache Hit/Miss → NO (TLB cannot have valid entry if PT doesn't)\n\nTLB Miss + PT Hit (valid) + Cache Hit/Miss → YES (page in memory, PTE will be loaded into TLB)\n\nTLB Miss + PT Miss (invalid) + Cache Hit → NO (cannot have cache hit if data not in memory)\n\nTLB Miss + PT Miss (invalid) + Cache Miss → YES (page fault will load page into memory)", "notes_text": null, "keywords": ["TLB hit", "cache hit", "page table", "consistency", "coherence", "hit combinations"], "images": [{"type": "table", "description": "Table analyzing all combinations of TLB hit/miss, Page Table hit/miss (valid/invalid), and CPU Cache hit/miss, with explanations of which are possible and why"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "analysis_table"}, "section": "Page Faults", "metadata": {"course": "CS356", "unit": 11, "topics": ["cache coherence", "TLB consistency", "memory hierarchy"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 46, "title": "Remarks on Memory Hierarchy", "summary": "Section divider introducing discussion of memory hierarchy design considerations", "main_text": "Remarks on Memory Hierarchy", "notes_text": null, "keywords": ["memory hierarchy", "design", "performance"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "Memory Hierarchy", "metadata": {"course": "CS356", "unit": 11, "topics": ["memory hierarchy"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 47, "title": "Memory Hierarchy", "summary": "Complete memory hierarchy overview: Registers (fastest) → L1 Cache (≈1ns, 32KB) → L2 Cache (≈4ns, 256KB) → L3 Cache (≈10-20ns, 8MB) → Main Memory (≈100ns, 16GB) → Secondary Storage (≈2ms HDD / 16μs SSD, 2TB). Unit of transfer increases at each level", "main_text": "Memory Hierarchy from top to bottom:\n\nRegisters (fastest, smallest, most expensive)\nL1 Cache (32 kB, ≈ 1 ns)\nL2 Cache (256 KB, ≈ 4 ns)\nL3 Cache (8 MB, ≈ 10-20 ns)\nMain Memory (16 GB, ≈ 100 ns)\nSecondary Storage (2 TB, ≈ 2 ms HDD seek / ≈ 16 μs SSD random read)\n\nUnit of Transfer:\n• Registers to Cache: Word or Byte (1-8 bytes)\n• Cache to Memory: Cache Block/Line (64 bytes) - spatial locality\n• Memory to Disk: Page (4 kB) - spatial locality\n\nReference: Network RTT: 0.5 ms same datacenter, 60 ms CA to NY, 150 ms CA to EU", "notes_text": null, "keywords": ["memory hierarchy", "latency", "capacity", "cache block", "page", "spatial locality"], "images": [{"type": "hierarchy_diagram", "description": "Pyramid diagram showing memory hierarchy with size, latency, and cost increasing in opposite directions. Shows physical components at each level"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "hierarchy_diagram"}, "section": "Memory Hierarchy", "metadata": {"course": "CS356", "unit": 11, "topics": ["memory hierarchy", "latency", "capacity", "transfer units"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 48, "title": "Virtual Memory vs Other Caches", "summary": "Disk latency is drastically worse (20,000x HDD or 160x SSD vs L3-RAM 10x), driving design decisions: larger page size (4KB vs 64B), fully associative, SW fault handling, pseudo-LRU replacement", "main_text": "Disk latency is much higher than memory: 20,000x (HDD) or 160x (SSD)...\n• Drastically worse than L3 vs RAM (10x)\n\nThis determines many design decisions:\n• Page size larger than cache block size (e.g., 4 kB vs 64 B) to amortize latency\n• VM uses physical memory as a fully associative (lower miss rate) cache of pages, while caches/TLBs are set-associative\n• VM handles page faults in SW, while caches/TLBs handle misses in HW\n• VM implements pseudo-LRU, caches/TLB use full LRU for low associativity, or random", "notes_text": null, "keywords": ["disk latency", "page size", "fully associative", "miss handling", "LRU", "design tradeoffs"], "images": [{"type": "diagram", "description": "Hard disk drive cross-section showing read/write head, platters, and mechanical components illustrating why disk access is slow"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "comparison"}, "section": "Memory Hierarchy", "metadata": {"course": "CS356", "unit": 11, "topics": ["VM design", "latency impact", "cache vs VM"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 49, "title": "Disk Cache", "summary": "OS maintains disk cache (page cache in Linux) to minimize disk I/O. File pages cached in memory frames. Multiple processes reading same file hit cache. Writes go to memory (write back) with periodic sync. Cache size reduced when memory scarce", "main_text": "OS implements a disk cache ('page cache' in Linux) to minimize disk IO:\n\n• Files are also split into pages\n• When a process reads from a file, the corresponding pages are saved to memory frames\n  ○ If another process reads the same portions of the file, we only access physical memory!\n• If any process writes to the file, we only make changes in memory (write back) and sync periodically\n\nThe amount of disk cache is reduced when physical memory is scarce", "notes_text": null, "keywords": ["disk cache", "page cache", "file I/O", "write back", "buffer cache", "memory pressure"], "images": [{"type": "diagram", "description": "Shows file X pages (0-FileX, 1-FileX) both on disk and cached in physical memory frames, with multiple process virtual address spaces mapping to the same cached pages"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "system_optimization"}, "section": "Memory Hierarchy", "metadata": {"course": "CS356", "unit": 11, "topics": ["disk caching", "file system cache", "I/O optimization"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 50, "title": "Monitoring VM Usage: Linux", "summary": "Explains Linux 'free' and 'lsblk' command outputs showing total/used/free physical memory, shared memory, buffer/cache memory (reclaimable), available memory, and swap partition on disk", "main_text": "Linux memory monitoring:\n\ntotal: Total physical memory (62Gi)\nused: Physical memory used by processes (392Mi)\nfree: Physical memory not used at all (46Gi)\nshared: Physical memory shared between processes (13Mi)\nbuff/cache: Physical memory used by disk caches - can be reclaimed if processes need more memory (16Gi)\navailable: Physical memory available after reclaiming caches (61Gi)\n\nSwap: Total swap area on disk (29Gi)\nlsblk shows swap partition: /dev/sda5 (29.8G, type SWAP)", "notes_text": null, "keywords": ["Linux", "free command", "memory monitoring", "swap", "buffer cache", "available memory"], "images": [{"type": "screenshot", "description": "Terminal output showing 'free -h' command with memory statistics and 'lsblk' showing disk partitions including swap partition"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "practical_example"}, "section": "Memory Hierarchy", "metadata": {"course": "CS356", "unit": 11, "topics": ["Linux memory", "monitoring tools", "swap space"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 51, "title": "Monitoring VM Usage: macOS", "summary": "macOS Activity Monitor showing memory usage: Physical Memory 16GB, Memory Used 13.05GB, Cached Files 2.27GB, Swap Used 538.9MB", "main_text": "macOS Activity Monitor Memory tab shows:\n\nPhysical Memory: 16.00 GB\nMemory Used: 13.05 GB\nCached Files: 2.27 GB\nSwap Used: 538.9 MB\nApp Memory: 8.97 GB\nWired Memory: 1.66 GB\nCompressed: 1.57 GB\n\nLists processes sorted by memory usage with columns for Process Name, Memory, Threads, Ports, PID, User", "notes_text": null, "keywords": ["macOS", "Activity Monitor", "memory pressure", "swap", "compressed memory", "wired memory"], "images": [{"type": "screenshot", "description": "macOS Activity Monitor window showing Memory tab with process list and memory statistics at bottom"}], "layout": {"num_text_boxes": 1, "num_images": 1, "dominant_visual_type": "practical_example"}, "section": "Memory Hierarchy", "metadata": {"course": "CS356", "unit": 11, "topics": ["macOS memory", "monitoring tools", "memory compression"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 52, "title": "Monitoring VM Usage: Windows", "summary": "Windows Task Manager Performance tab memory metrics explained: In use (physical memory used), Current committed (virtual memory allocated), Max committed (max allocatable virtual memory), Cached (disk cache), Paged/non-paged pools (kernel memory)", "main_text": "Windows Task Manager memory metrics:\n\n• In use: physical memory used (total-available on Linux)\n• Current committed: virtual memory allocated by processes (used on Linux)\n• Max committed: virtual memory that can be allocated by processes (used+available on Linux)\n• Cached: used for disk caches\n• Paged/non-paged pools: kernel and driver memory that can/cannot be swapped to disk\n\nExample shown: 2.8 GB in use (124 MB compressed), 1.1 GB available, 5.1/7.8 GB committed, 1.1 GB cached", "notes_text": null, "keywords": ["Windows", "Task Manager", "memory usage", "committed memory", "paged pool", "non-paged pool"], "images": [{"type": "screenshot", "description": "Windows Task Manager Performance tab showing Memory panel with usage graph and detailed memory statistics"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "practical_example"}, "section": "Memory Hierarchy", "metadata": {"course": "CS356", "unit": 11, "topics": ["Windows memory", "monitoring tools", "committed memory"]}}