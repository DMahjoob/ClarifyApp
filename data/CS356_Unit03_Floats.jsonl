{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":1,"chunk_index":0,"title":"Unit 3 — Floating Point: IEEE 754 Representation","summary":"Introduces Unit 3: floating point and IEEE 754 representation; sets the topic and scope for the lecture slides that follow.","main_text":"Unit 3: Floating Point — IEEE 754 Representation. This slide serves as the unit title page announcing that the module covers floating point representation and the IEEE 754 standard. It identifies the unit (Unit 3) and the broad topic (Floating Point / IEEE 754 Representation), preparing students for detailed discussion of floating vs fixed point, formats, special values, rounding, and programmer implications in the following slides.","notes_text":"Title/cover slide — no technical details beyond unit identification. Use as citation anchor for the whole unit.","keywords":["Floating Point","IEEE 754","Unit 3","Floating vs Fixed","Representation"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Overview / IEEE 754","importance_score":6,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:0]{index=0}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":2,"chunk_index":0,"title":"Floating vs Fixed Point (overview)","summary":"High-level contrast between floating-point and fixed-point number systems, motivating floating point for very large and very small values.","main_text":"Floating point is used to represent both very small fractional numbers and very large numbers (examples: Avogadro's number ~6.022×10^23, Boltzmann's constant ~1.38×10^-23). 32- or 64-bit integers cannot represent such wide ranges, so floating-point formats (e.g., float and double in C) allocate bits differently to provide larger dynamic range. The slide emphasizes that with the same number of total bit combinations, float must space representable values non-uniformly to achieve range advantage over integers.","notes_text":"Motivation slide: show why floating point is necessary (range) and mention that floats trade off uniform density for range. Good to reference later when explaining exponent and significand fields.","keywords":["range","precision","float","double","dynamic range","integer limitations"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Motivation / Floating vs Fixed","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:1]{index=1}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":3,"chunk_index":0,"title":"Floating Point Intuition: spacing across magnitudes","summary":"Illustrates how floating point redistributes representable values to cover both very small and very large magnitudes with examples and a small numeric diagram.","main_text":"Same number of combinations (bits) can be used to represent values with different spacing: floats place denser representable values near zero (small magnitudes) and sparser spacing at large magnitudes. The slide uses small decimal values (e.g., 0.0, 0.1, 0.2, 0.3, -0.1, -0.2, -0.3) and large numbers to emphasize that a float must 'space values differently' to extend range beyond what an integer can cover. Visual axis or number-line style representation demonstrates non-uniform spacing of representable numbers.","notes_text":"Conceptual figure: useful when later explaining mantissa + exponent — the exponent controls scale while mantissa controls density within that scale.","keywords":["value spacing","density","dynamic range","mantissa","exponent"],"images":[{"description":"Number-line style illustration showing dense representable values near zero and sparser spacing at large magnitudes (visual aid for float spacing).","labels":["0.0","0.1","0.2","0.3","-0.1","-0.2","-0.3"],"position":{"x":0.05,"y":0.15,"width":0.9,"height":0.6}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":3,"topic":"Floating Point Intuition","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:2]{index=2}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":4,"chunk_index":0,"title":"Fixed Point (Base 10) — examples and ranges","summary":"Explains fixed-point decimal representations with examples (unsigned integers, fixed with 1 or 3 decimals) and their ranges and rounding errors.","main_text":"Fixed point in base 10 with a fixed number of digits (example: 6 digits total) can be interpreted in multiple ways: unsigned integers (000000–999999) give range [0, 10^6 - 1] with absolute rounding error ≤1/2; fixed-point with one decimal (00000.0 … 99999.9) gives range [0, 10^5 - 0.1] with abs. error ≤0.1/2; fixed-point with three decimals (000.000 … 999.999) gives range [0, 10^3 - 0.001] with abs. error ≤0.001/2. The slide notes that representation error (rounding) exists—add/sub of fixed point is exact (except overflow), while mul/div cause additional error.","notes_text":"Sets up contrast to floating point: fixed-point has uniform spacing, predictable absolute error, but limited range. Useful when explaining why floating-point uses exponent to vary scale.","keywords":["fixed-point","range","absolute error","decimal digits","rounding","overflow"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Fixed Point Examples","importance_score":7,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:3]{index=3}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":5,"chunk_index":0,"title":"Floating Point (Base 10) — exponent idea and biased exponent","summary":"Demonstrates base-10 floating point by moving the decimal point with an exponent, and introduces biased exponent storage to represent negative and positive exponents.","main_text":"Floating-point uses an exponent to move the decimal point and thus trade precision for range. Examples show how a mantissa with an exponent covers different ranges and precisions (e.g., 1.2345×10^5 vs 1.2345×10^-1). The slide explains biased exponent encoding for a single decimal-digit example (BIAS=4): stored digit = exponent + BIAS, so stored digits 0..9 map to exponents -4..+5. It also mentions normalized notation: one nonzero digit before the point in scientific notation (i.e., normal form).","notes_text":"This slide connects decimal intuition with the binary floating-point representation that follows; biased exponent concept will be generalized to Excess-127 / Excess-1023 in IEEE formats.","keywords":["exponent","biased exponent","normalization","mantissa","range","scientific notation"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Exponent & Bias","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:4]{index=4}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":6,"chunk_index":0,"title":"Perils of Floating Point — precision loss example","summary":"Shows a practical example where adding a small value to a very large floating-point number has no effect due to limited significant digits.","main_text":"Perils of floating point: finite number of significant digits causes loss of small increments when added to large values. Example: 123450 + 0.10000 should be 123450.1, but encoding with limited significant digits can represent 123450 and 123450.1 identically, causing the 0.1 to be lost. The slide emphasizes that floating point extends range at the cost of reduced density around large magnitudes and demonstrates how small additions can be swallowed by large values.","notes_text":"Illustrative caution for programmers: order of operations and grouping matters; adding small values into big ones can be lost. This motivates later advice on associativity and numerical stability.","keywords":["precision loss","rounding","significant digits","catastrophic cancellation","numerical stability"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Precision Loss","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:5]{index=5}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":7,"chunk_index":0,"title":"Reality of Floats — C example showing non-associativity","summary":"Presents a C program demonstrating that floating-point addition is not associative and some common surprising behaviors when adding small numbers to large floats.","main_text":"C code example demonstrates surprising float behavior: with float x = 1000000.0f, expressions like (x + 0.01f) == x evaluate to true because adding 0.01f does not change x due to limited precision. The program prints boolean results showing pitfalls: -x + (x + 0.01f) == 0.0 may hold (bad), while (-x + x) + 0.01f == 0.01f (better) — showing different parenthesizations matter. Key point: floats have finite significant digits; operations are not associative, so adding numbers of similar magnitude first is recommended.","notes_text":"Concrete demonstration useful when discussing associativity and compiler optimizations. Reference for later slides on programming implications.","keywords":["C example","associativity","precision","float quirks","programming advice"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"FP behavior in code","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:6]{index=6}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":9,"chunk_index":0,"title":"Floating Point in Base 2","summary":"Introduces binary floating point, its analogy to decimal scientific notation, and the three fields: sign, exponent, and fraction.","main_text":"Binary floating point mirrors decimal scientific notation. Decimal scientific notation has the form ±D.DDD × 10^exp; binary floating point instead uses ±b.bbbb × 2^exp. IEEE formats implement three fields: a sign bit (0 for positive, 1 for negative), an exponent field (encoded with a bias), and a fraction field (mantissa/significand) that stores bits after an implicit leading 1 in normalized numbers. This slide introduces the structural components that define all IEEE 754 floating-point values.","notes_text":"Foundation slide: prepares students for normalization rules and IEEE single/double specifications on subsequent slides.","keywords":["binary","scientific notation","mantissa","sign bit","exponent","fraction"],"images":[{"description":"Diagram showing three fields labeled S, Exp., and Fraction, representing components of binary floating point.","labels":["S","Exp.","Fraction"],"position":{"x":0.1,"y":0.3,"width":0.8,"height":0.4}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":3,"topic":"Binary Floating Point","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:0]{index=0}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":10,"chunk_index":0,"title":"Normalized Floating Point","summary":"Defines normalization in binary floating point and explains why the leading 1 in normalized values is implicit and not stored.","main_text":"Correct normalized scientific notation in decimal requires exactly one nonzero digit before the decimal point (e.g., 7.54×10^14 instead of 0.754×10^15). Binary normalization follows the same rule: normalized values always have the form ±1.bbbbbb × 2^exp. Because the leading bit is always 1 for any nonzero normalized binary number, IEEE floating point omits this bit from storage, effectively giving an extra bit of precision. Hardware must normalize values before storage, e.g., a computed intermediate result 0.001101 × 2^5 is normalized to 1.101000 × 2^2.","notes_text":"The implicit leading-1 rule is critical when discussing precision, subnormals/denormals, and the width of the fraction field.","keywords":["normalization","implicit bit","leading one","binary scientific notation","precision"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Normalization","importance_score":10,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:1]{index=1}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":11,"chunk_index":0,"title":"IEEE 754 Introduction","summary":"Introduces the IEEE 754 floating-point standard used for representing real numbers in computers.","main_text":"This slide marks the transition from conceptual binary floating-point notation to the standardized IEEE 754 format, which defines how hardware encodes sign, exponent, fraction (including normalization, biases, special values, rounding behavior, and binary formats). It sets the stage for upcoming details specific to single-precision (32-bit) and double-precision (64-bit) layouts.","notes_text":"High-level transition slide; no numerical details but important structurally as an anchor for the section.","keywords":["IEEE 754","standard","floating point","format","specification"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"IEEE 754 Overview","importance_score":7,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:2]{index=2}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":12,"chunk_index":0,"title":"IEEE 754 Floating Point Formats — Single and Double Precision","summary":"Details the IEEE 754 single-precision and double-precision formats, including field sizes, exponent biases, and approximate decimal ranges.","main_text":"IEEE 754 single precision (float in C) uses 32 bits: 1 sign bit, 8 exponent bits (Excess-127), and 23 fraction bits. Its approximate decimal precision is about 7 significant digits across a range of roughly 10±38. Double precision (double in C) uses 64 bits: 1 sign, 11 exponent bits (Excess-1023), and 52 fraction bits, giving about 16 significant decimal digits and a range of approximately 10±308. For both formats, the fraction bits represent the mantissa after the implicit leading 1 in normalized numbers.","notes_text":"Useful slide for memorization: bit widths, biases, and approximate decimal equivalence are commonly tested.","keywords":["single precision","double precision","exponent bias","fraction bits","float","double","range"],"images":[{"description":"Bitfield diagrams showing 1 sign bit, exponent field, and fraction field for single and double precision.","labels":["S","Exp.","Fraction","1","8","23","11","52"],"position":{"x":0.05,"y":0.2,"width":0.9,"height":0.5}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":3,"topic":"IEEE Formats","importance_score":10,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:3]{index=3}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":13,"chunk_index":0,"title":"Single-Precision Examples","summary":"Works through examples of encoding and decoding IEEE 754 single-precision values using sign, exponent, and fraction fields.","main_text":"Examples illustrate how to interpret 32-bit IEEE floating-point bit patterns. For instance, a bitstring beginning with sign=1, exponent=10000010₂ (130 decimal), and a given fraction represents a negative normalized binary value −1.1100110 × 2^3, which may be converted to decimal (e.g., −14.375). Additional examples show how to encode decimal fractions (e.g., 0.6875) into normalized binary scientific notation and corresponding IEEE fields. The slide demonstrates converting between raw bit patterns (e.g., hex like 000003F3) and the underlying numeric value.","notes_text":"Very useful for problem sets requiring manual IEEE 754 conversion. Students should practice extracting exponent (subtract bias), reconstructing significand, and renormalizing when necessary.","keywords":["IEEE 754","single precision","conversion","exponent decoding","fraction","hex encoding"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":3,"topic":"IEEE Examples","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:4]{index=4}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":14,"chunk_index":0,"title":"Excess-N Exponent Encoding","summary":"Explains Excess-N (bias) exponent representation and illustrates how adding a bias allows signed exponents to be stored as unsigned values.","main_text":"With k exponent bits, the unsigned range is 0…2^k−1. Excess-N representation defines stored_value = true_exponent + N. For single precision, the bias is 127; for double, 1023. Examples: an exponent of +1 becomes stored exponent 1+127 = 128 (10000000₂); an exponent of −2 in double becomes stored exponent −2 + 1023 = 1021 (binary 01111111101₂). Excess-N gives monotonic ordering: larger exponents correspond to larger stored unsigned integers, simplifying comparison. The slide also shows how storing unsigned integers and subtracting a central offset maps them to symmetric signed exponent ranges.","notes_text":"Critical for understanding exponent fields and special values (all 0s and all 1s) in IEEE formats.","keywords":["Excess-127","Excess-1023","biased exponent","signed exponent","storage","IEEE 754"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Exponent Bias","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:5]{index=5}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":15,"chunk_index":0,"title":"Why Not Two’s Complement for Exponents?","summary":"Shows why IEEE 754 uses biased exponents instead of two’s complement by comparing ordering behavior and numeric comparison logic.","main_text":"Two’s complement makes sign bit the most significant bit, causing incorrect ordering of exponents when comparing floating-point values. Because floating-point numbers are ordered first by exponent and then by fraction, exponents must be comparable using unsigned integer comparison. Examples show two’s-complement encoding producing misleading orderings (e.g., negative exponents appearing numerically larger). Excess-127 preserves correct ordering: larger exponents always have larger stored values. The slide lists example mappings between two’s complement, stored value, and excess-127 interpretations.","notes_text":"This conceptual slide justifies the storage design of IEEE exponents and supports students in reasoning about sorting and comparing FP numbers.","keywords":["two’s complement","biased exponent","ordering","comparison","Excess-127","IEEE design"],"images":[{"description":"Table comparing two’s complement vs stored exponent vs Excess-127 exponent interpretations.","labels":["2’s comp.","Stored Value","Excess-127"],"position":{"x":0.1,"y":0.3,"width":0.8,"height":0.5}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"comparison"},"metadata":{"course":"CS356","unit":3,"topic":"Exponent Ordering","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:6]{index=6}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":16,"chunk_index":0,"title":"Reserved Exponent Values in IEEE 754","summary":"Explains that exponent fields of all 1s or all 0s are reserved for special meanings and lists the valid exponent range for single precision.","main_text":"In IEEE 754 formats, exponent values of all 1s (binary 111…111) and all 0s (000…000) are reserved for special-number encodings: infinities, NaNs, zeros, and subnormal numbers. For single precision (8-bit exponent), the valid exponent range after bias removal is −126 to +127. The slide includes a table mapping stored exponent values 0–255 to excess-127 interpretations and indicates which entries are reserved.","notes_text":"This slide is foundational for the upcoming explanation of special values including ±0, ±∞, denormals, and NaNs.","keywords":["reserved exponent","special values","range","bias","subnormal"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Exponent Range","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:7]{index=7}"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":17,"chunk_index":0,"title":"IEEE Exponent Special Values","summary":"Explains how exponent patterns of all zeros or all ones encode zeros, denormals, infinities, and NaNs depending on the fraction field.","main_text":"IEEE 754 uses exponent values 000…0 and 111…1 for special categories. When the exponent is all zeros and the fraction field is also zero, the number is ±0 depending on the sign bit. When the exponent is zero and the fraction is nonzero, the number is denormalized (a subnormal), interpreted as ±0.bbbbbb × 2^−126 in single precision. When the exponent is all ones and the fraction is zero, the number represents ±∞. When the exponent is all ones and the fraction is nonzero, the encoding represents NaN (Not-a-Number), used for undefined operations such as 0/0 or √(negative). The slide displays a table mapping exponent patterns and fraction patterns to meanings.","notes_text":"This table is essential for understanding corner cases in floating-point arithmetic and behavior in exceptional operations.","keywords":["special values","zero","infinity","NaN","denormal","subnormal","IEEE 754"],"images":[{"description":"Field table showing how exponent and fraction patterns encode zero, denormalized numbers, infinity, and NaN.","labels":["Exp.","Fraction","±0","Denormalized","±∞","NaN"],"position":{"x":0.1,"y":0.25,"width":0.8,"height":0.5}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"table"},"metadata":{"course":"CS356","unit":3,"topic":"Special Exponent Patterns","importance_score":10,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":18,"chunk_index":0,"title":"Special Values: Zero and Infinity (C demonstration)","summary":"Shows bit-level encodings for +0, −0, +∞, −∞, max and min floats, and how division by zero produces infinities.","main_text":"A C program prints raw IEEE 754 encodings of special floating-point values using a union that reinterprets float bits as integers. The slide demonstrates that +0.0 and −0.0 differ only in the sign bit (00000000 vs 80000000). +∞ has encoding 0x7F800000; −∞ is 0xFF800000. FLT_MAX (largest normalized float) encodes as 0x7F7FFFFF, and FLT_TRUE_MIN (smallest positive subnormal) encodes as 0x00000001. The slide shows that dividing 1.0 by ±0.0 yields ±∞ and that large results overflow to infinity, while tiny results underflow to zero. It also shows that +0.0 == −0.0 evaluates to true in comparisons, despite different bit patterns.","notes_text":"Highlights how floating point treats signed zero, infinity, and overflow/underflow; useful when debugging numeric edge cases.","keywords":["signed zero","infinity","encoding","overflow","underflow","C example"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Zeros and Infinities","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":19,"chunk_index":0,"title":"Special Values: NaN (Not-a-Number)","summary":"Demonstrates “sticky” NaN behavior, bit patterns, and non-reflexive comparisons using C code.","main_text":"NaNs occur when the exponent is all ones and the fraction field is nonzero. A C program example shows that 0/0 produces a NaN with encoding such as 0xFFC00000, where the fraction holds an ‘error code’. The NaN payload propagates across operations: multiplying the NaN by 42 still yields the same NaN bit pattern, demonstrating stickiness. Additionally, NaN comparisons are always false, even comparing a NaN to itself. Expressions like (nan >= 0.0), (nan < 0.0), and (nan == nan) all evaluate to false. This captures IEEE 754’s definition that NaNs represent invalid numeric results.","notes_text":"Important for reasoning about floating-point comparisons and why ‘isnan()’ checks are necessary.","keywords":["NaN","Not-a-Number","sticky behavior","comparison false","0/0","exception"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"NaN Behavior","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":20,"chunk_index":0,"title":"Transition to Denormalized (Subnormal) Numbers","summary":"Explains how denormalized numbers allow a smooth transition from normalized values to zero by using an implicit 0.x significand.","main_text":"When the exponent field is all zeros and the fraction is nonzero, IEEE interprets the significand as 0.bbbbbb rather than the normalized 1.bbbbbb. The effective exponent is fixed at −126 (for single precision), giving the smallest representable magnitudes. Examples show: 0 00000001 000…0 corresponds to normalized 1.0 × 2^−126; 0 00000000 100…0 corresponds to 0.1₂ × 2^−126 = 2^−127; and 0 00000000 010…0 corresponds to 2^−128. This mechanism fills the gap between the smallest normal number (~2^−126) and zero with gradually smaller subnormals.","notes_text":"Understanding the smooth transition helps students grasp underflow behavior and the significance of gradual underflow in IEEE arithmetic.","keywords":["denormal","subnormal","implicit zero","underflow","fraction field","IEEE 754"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Subnormals","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":21,"chunk_index":0,"title":"Floating Point vs Fixed Point — Range vs Precision","summary":"Compares floating point and fixed point by emphasizing floating point’s wider range but lower precision for large magnitudes.","main_text":"Single-precision floating point provides roughly 7 significant decimal digits across an enormous exponent range (10^±38), whereas a 32-bit signed integer represents only about ±2 billion. Floating point achieves this larger range because its 23 fraction bits are distributed across exponentially growing intervals controlled by the exponent. However, it sacrifices precision: not all numbers in the representable range can be encoded exactly, and spacing grows with magnitude. Double precision expands this to around 16 significant digits and range up to 10^±308. The slide reiterates the central tradeoff: range vs. precision.","notes_text":"Important conceptual contrast: fixed point has uniform spacing, while floating point has nonuniform spacing tied to exponent.","keywords":["range","precision","floating point","fixed point","dynamic range","significant digits"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Range vs Precision","importance_score":7,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":22,"chunk_index":0,"title":"12-bit “IEEE Short” Format (Class-Specific)","summary":"Defines a simplified 12-bit floating-point format used for instructional purposes, including sign, exponent, and fraction bit allocations.","main_text":"The slide introduces a fictional 12-bit floating-point format used solely for class exercises. It contains: 1 sign bit; 5 exponent bits using Excess-15 representation (stored exponent = true exponent + 15); and 6 fraction bits representing the significand bits after the implicit leading 1. Reserved exponent patterns follow the same conventions as IEEE formats. The slide provides a labeled diagram highlighting the fields and how normalization works for this toy format.","notes_text":"Useful for homework problems requiring manual conversions; simpler than 32-bit IEEE format but conceptually identical.","keywords":["12-bit float","toy format","Excess-15","fraction bits","sign bit","normalization"],"images":[{"description":"Bitfield diagram of the 12-bit floating-point format showing sign, exponent, and fraction fields.","labels":["S","Exp.","Fraction","1","5 bits","6 bits"],"position":{"x":0.1,"y":0.3,"width":0.8,"height":0.45}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":3,"topic":"Instructional Float Format","importance_score":6,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":23,"chunk_index":0,"title":"12-bit Format Examples","summary":"Works through example encodings and decodings of the class’s 12-bit floating-point format, including renormalization steps.","main_text":"Examples include converting binary patterns like 1 10100 101101 into numeric values. For instance, with sign=1, exponent=10100₂ (20 decimal), the true exponent is 20−15=5, giving −1.101101 × 2^5 = −54.5. Another example converts 0 10011 010111: exponent=19 decimal → true exponent=4, giving +1.010111 × 2^4 = +21.75. Additional examples demonstrate negative exponents, such as 1 01101 100000: exponent=13 decimal → true exponent=−2, producing −0.375. These examples illustrate normalization, exponent decoding, and binary-to-decimal conversion steps.","notes_text":"Good practice examples for understanding conversion process identical to IEEE 754 but using smaller fields.","keywords":["12-bit","conversion","examples","binary","exponent decoding","significand"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":3,"topic":"12-bit Examples","importance_score":7,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":24,"chunk_index":0,"title":"Rounding (Section Introduction)","summary":"Introduces the concept of rounding in floating-point arithmetic and prepares for detailed rounding-mode explanations.","main_text":"This slide introduces the need for rounding in floating-point arithmetic. Because floating-point formats have limited precision, many operations (integer-to-float conversion, floating add/sub, multiplication, and division) produce more bits than can fit in the available fraction field, requiring rounding to keep the result representable. The slide sets up the detailed explanation of rounding modes and guard/round/sticky bits in the following slides.","notes_text":"Only a section heading; no detailed rules yet. Acts as a transition into rounding methods.","keywords":["rounding","precision limit","floating point operations","overflow of fraction","rounding introduction"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Rounding Overview","importance_score":5,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":25,"chunk_index":0,"title":"The Need to Round","summary":"Shows why rounding is required in floating-point arithmetic by illustrating operations that generate more bits than can be stored.","main_text":"Rounding is essential because floating-point formats cannot store all bits that arise from arithmetic operations. Integer-to-float conversion may produce a binary representation with more fraction bits than allowed. Floating add/sub may require alignment of exponents, causing low-order bits to be shifted out. Floating multiply/divide typically creates products or quotients with more bits than the fraction field can accommodate. The slide shows examples: converting +725 into normalized binary yields 1.011010101 × 2^9, which has more fraction bits than the format supports; adding numbers alike in exponent can produce numerous bits; multiplying significands generates a product with many fractional bits, requiring rounding to fit into the target format.","notes_text":"Key conceptual point: overflow of fraction bits is normal; rounding is required and unavoidable in IEEE arithmetic.","keywords":["rounding","fraction bits","normalization","overflow of precision","add/sub","mul/div"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Why Rounding Is Needed","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":26,"chunk_index":0,"title":"Rounding Methods","summary":"Describes the four primary IEEE 754 rounding modes, including round-to-nearest-even, toward zero, toward +∞, and toward −∞.","main_text":"IEEE 754 defines multiple rounding modes. (1) Round to Nearest, Ties to Even: chooses the representable value closest to the exact result; if exactly halfway, choose the one whose least significant bit is even. This reduces long-term statistical bias. (2) Round Toward Zero (Chopping): truncates bits beyond the representable range, moving the result toward zero. (3) Round Toward +∞ (Round Up): rounds to the smallest representable value greater than or equal to the exact value. (4) Round Toward −∞ (Round Down): rounds to the greatest representable value less than or equal to the exact value. The slide concisely illustrates each rule conceptually.","notes_text":"Students must recognize when each mode is used and how ties are resolved; round-to-nearest-even is the IEEE default.","keywords":["rounding modes","nearest even","round toward zero","round up","round down","IEEE 754"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Rounding Modes","importance_score":10,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":27,"chunk_index":0,"title":"Number Line View of Rounding Methods","summary":"Uses number-line diagrams to visualize how each rounding mode chooses between adjacent representable values.","main_text":"The slide shows four number-line illustrations. In each, representable floating values are marked as discrete points; green regions represent exact results lying between them. For round-to-nearest, the exact value is mapped to the closest representable number; halfway cases are resolved using the even rule. For round-to-zero, values are truncated toward zero: negative exact values round up toward zero, positives round down. For round-to-+∞, values round upward; for round-to-−∞, values round downward. The diagrams demonstrate which direction the green intervals collapse to under each rounding mode.","notes_text":"Useful visual reference for students who think in terms of geometric intuition rather than bit patterns.","keywords":["rounding","number line","visualization","nearest","toward zero","toward infinity"],"images":[{"description":"Four number-line diagrams illustrating round-to-nearest, round-to-zero, round-to-+infinity, and round-to-−infinity.","labels":["0","+∞","−∞","round to nearest","round to zero","round to +∞","round to −∞"],"position":{"x":0.07,"y":0.18,"width":0.86,"height":0.65}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":3,"topic":"Rounding Visualization","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":28,"chunk_index":0,"title":"Many More Rounding Details","summary":"Placeholder slide indicating that additional rounding-related mechanisms exist beyond the major rounding modes.","main_text":"This slide indicates that floating-point arithmetic contains many more nuanced rounding behaviors and edge cases that go beyond the major modes previously discussed. While no specific content is listed, the slide implies additional subtleties exist in rounding intermediates, extended precision, fused operations, and architecture-dependent behaviors.","notes_text":"Section divider; no technical content to extract, but signals upcoming rounding examples.","keywords":["rounding","IEEE 754","details","precision","arithmetic"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Additional Rounding Notes","importance_score":3,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":29,"chunk_index":0,"title":"Rounding to Nearest in Base 10","summary":"Explains decimal rounding rules analogous to IEEE’s round-to-nearest-even mode using representative decimal examples.","main_text":"Using decimal analogies, the slide shows how round-to-nearest-even works. Values between 1.2351 and 1.2399 round up to 1.24; values between 1.2301 and 1.2349 round down to 1.23. For exact halfway cases like 1.2350, the tie is resolved by rounding to the option with an even digit in the last place—here, 1.24. For 1.2450, the even choice is also 1.24. This even-tie-breaking rule reduces systematic upward or downward drift over repeated operations by making the probability of rounding up or down roughly balanced.","notes_text":"Establishes intuition for tie-to-even in familiar decimal settings before translating to binary versions.","keywords":["decimal rounding","nearest even","tie-breaking","rounding bias","IEEE analogy"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Decimal Rounding Analogy","importance_score":6,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":30,"chunk_index":0,"title":"Rounding to Nearest in Base 2 (GRS Bits)","summary":"Shows how binary halfway cases are detected and resolved using guard, round, and sticky bits.","main_text":"In binary rounding, the exact halfway point corresponds to the bit pattern 10…0 in the extra bits beyond the fraction field. Hardware keeps additional bits—Guard (G), Round (R), and Sticky (S)—to determine whether the truncated portion is exactly half, more than half, or less than half. If GRS = 100…0 (i.e., only the guard bit is 1 and all others are zero), the value is exactly halfway and is rounded to the even representable value. If the additional bits indicate more than halfway (1x…x), the result rounds up. If less than halfway (0x…x), the result rounds down. Examples use values like 10.10000₂ for exactly half (round to 2), 10.10010₂ for more than half (round to 3), and 10.00010₂ for less than half (round to 2).","notes_text":"The GRS mechanism is central to all practical floating-point rounding implementations.","keywords":["GRS bits","guard","round","sticky","binary rounding","halfway cases"],"images":[{"description":"Diagram showing a binary significand with extra Guard, Round, and Sticky bits used to determine rounding direction.","labels":["GRS","bit field"],"position":{"x":0.08,"y":0.25,"width":0.84,"height":0.5}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":3,"topic":"Binary Rounding Mechanics","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":31,"chunk_index":0,"title":"Rounding to Nearest in Base 2 — Examples","summary":"Provides examples showing how additional bits guide rounding actions, including renormalization when rounding causes overflow of the fraction.","main_text":"Several rounding cases are shown. If additional bits are 110, the value is more than halfway, so the fraction increments (round up). In some cases, adding 1 to the fraction overflows, requiring renormalization (e.g., 1.111111 × 2^4 + a small increment becomes 1.000000 × 2^5). If additional bits are 001 (less than halfway), the fraction remains unchanged. The slide walks through three concrete examples, illustrating when rounding causes fraction overflow and subsequent exponent adjustment.","notes_text":"This slide concretizes how rounding interacts with normalization rules and exponent adjustments.","keywords":["rounding examples","renormalization","fraction overflow","GRS bits","nearest even"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":3,"topic":"Rounding Examples Binary","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":32,"chunk_index":0,"title":"Round-to-Nearest: Halfway Case Examples","summary":"Shows detailed halfway rounding cases and demonstrates which candidate representable value has an even least significant bit.","main_text":"Examples illustrate three halfway cases: (1) 1.001100100 × 2^4 has two rounding options (1.001100 or 1.001101) and rounds down because the even option is selected. (2) A case like 1.111111100 × 2^4 rounds up, causing renormalization to 1.000000 × 2^5. (3) Another example 1.001101100 × 2^4 yields two candidates (1.001101 or 1.001110) and rounds up to the one with even LSB. These examples emphasize the tie-to-even rule and show that renormalization may occur even in halfway cases.","notes_text":"Important because halfway cases are subtle; students frequently misapply the even rule.","keywords":["halfway case","round-to-even","tie-breaking","renormalization","binary rounding"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Halfway Rounding","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":33,"chunk_index":0,"title":"Round to Zero (Chopping)","summary":"Shows how the round-toward-zero mode simply discards extra bits, producing the truncated significand.","main_text":"In round-toward-zero mode, also known as chopping, the Guard, Round, and Sticky bits are discarded, and the fraction is left unchanged. Three examples are shown: 1.001100001×2⁴, 1.001101101×2⁴, and 1.001100111×2⁴. In each case, even though the GRS bits vary, no rounding direction is applied—extra bits are dropped and the representable prefix becomes the final stored significand. This mode always moves the result toward zero (reducing magnitude), regardless of sign.","notes_text":"This rounding mode is commonly used when converting floats to ints; it is simple but can produce bias.","keywords":["round toward zero","chopping","GRS bits","truncation","rounding mode"],"images":[{"description":"Three examples illustrating truncated significands after dropping G, R, and S bits.","labels":["GRS"],"position":{"x":0.12,"y":0.30,"width":0.78,"height":0.40}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":3,"topic":"Rounding Toward Zero","importance_score":6,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":34,"chunk_index":0,"title":"Rounding Implementation (GRS and Sticky Bit)","summary":"Explains how hardware uses guard, round, and sticky bits to implement rounding efficiently.","main_text":"To implement rounding without storing an unbounded number of extra bits, hardware keeps only a limited set of bits beyond the fraction: Guard bits (several), a Round bit, and a Sticky bit. The Sticky bit is the OR of all bits that follow the Guard and Round bits, indicating whether any trailing bits were nonzero. Example: 1.01001010010₂×2⁴ becomes 1.010010101₂×2⁴ after rounding using GRS bits. With GRS, hardware determines whether to round up, round down, or tie-to-even using only three bits, making rounding efficient and precise enough for IEEE 754 rules.","notes_text":"This slide is key for understanding hardware-level rounding operations and the use of extended precision pipelines.","keywords":["guard bit","round bit","sticky bit","rounding hardware","GRS"],"images":[{"description":"Diagram showing significand with Guard, Round, and Sticky bits after the stored fraction field.","labels":["GRS"],"position":{"x":0.08,"y":0.30,"width":0.84,"height":0.45}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":3,"topic":"Hardware Rounding","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":35,"chunk_index":0,"title":"Implications for Programmers","summary":"Highlights why programmers must account for rounding, non-associativity, and subtle float behaviors when writing numerical code.","main_text":"This introductory slide notes that floating-point rounding and representation have real consequences for software. It prepares for subsequent slides that show examples of association errors, catastrophic cancellation, and pitfalls in arithmetic expressions. Programmers must be aware of precision limits, rounding modes, and ordering of operations to produce stable and predictable numeric results.","notes_text":"Section divider leading into practical programming concerns, not yet containing equations.","keywords":["programming","numerical stability","rounding effects","non-associativity","precision"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Programming Implications","importance_score":5,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":36,"chunk_index":0,"title":"Floating Point Addition and Subtraction","summary":"Shows that FP add/sub are not associative and demonstrates catastrophic cancellation with examples.","main_text":"Floating-point addition and subtraction are not associative: (a+b)+c ≠ a+(b+c). Examples are provided: (0.0001 + 98475) − 98474 ≠ 0.0001 + (98475 − 98474). Due to rounding, 98475 − 98474 = 1, but 0.0001 + 98475 rounds to 98475.0 in many formats, making the left side reduce to 1, while the right side yields 1.0001. Another example involving extremely large numbers (1 + 1.11…1 × 2¹²⁷ − 1.11…1 × 2¹²⁷) shows how huge magnitudes swallow small values entirely. Catastrophic cancellation also appears when subtracting nearly equal numbers, e.g., 9.999 − 9.998 = 0.001, producing one meaningful digit from four.","notes_text":"Students should reorder computations to add numbers of similar magnitude first to reduce error.","keywords":["non-associativity","cancellation","floating point add","floating point sub","numerical error"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Addition/Subtraction Issues","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":37,"chunk_index":0,"title":"Floating Point Multiplication and Division","summary":"Explains non-associativity in FP multiply/divide and shows how overflow and underflow occur depending on evaluation order.","main_text":"Floating-point multiplication and division are not associative and also violate distributivity over addition. Examples: (big1 × big2) / (big3 × big4) may overflow during the first multiplication; computing 1/big3 × 1/big4 × big1 × big2 may underflow early; computing (big1/big3) × (big2/big4) often avoids overflow/underflow. Also, a*(b+c) ≠ a*b + a*c because rounding intervenes. A cautionary note on integer operations in C: expressions like F = (9/5)*C + 32 behave differently than F = (9*C)/5 + 32 due to integer truncation.","notes_text":"Highlights that operation ordering matters, and integer vs float operations must be considered separately.","keywords":["multiplication","division","non-associative","overflow","underflow","C integer division"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Multiplication/Division Behavior","importance_score":7,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":38,"chunk_index":0,"title":"Floating Point Comparisons (== and <)","summary":"Warns against direct equality or loop-counter comparisons with floats and suggests using tolerances.","main_text":"Equality comparisons on floats are unreliable because many decimal values (e.g., 0.1, 0.2, 0.3) are not exactly representable in binary. Example: (0.1f + 0.2f == 0.3f) evaluates to false. Floats also should not be used as loop counters because increments accumulate rounding error; e.g., incrementing t by 0.1 may never reach exactly 1.0. A safer approach is to test whether two floats differ by less than a small epsilon. Python’s isclose(x,y) is referenced as an example of a principled approximate comparison method.","notes_text":"Classic pitfall for students transitioning from pure math to numerical computing.","keywords":["floating point compare","equality","epsilon","loop counter","precision error"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Comparison Pitfalls","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":39,"chunk_index":0,"title":"Floating Point & Compiler Optimizations","summary":"Shows how compiler transformations can change numerical results because FP operations are not associative.","main_text":"Given expressions x = a + b + c and y = b + c + d, a compiler might apply common-subexpression elimination to compute temp = b + c and reuse it. However, in floating point, changing the grouping of additions can yield different results due to rounding. The slide references a discussion by Linus Torvalds regarding the use of -ffast-math, which allows compilers to violate strict IEEE 754 associativity rules to gain performance at the cost of accuracy or predictability.","notes_text":"Key idea: optimization is constrained because mathematically equivalent expressions are not numerically equivalent in FP.","keywords":["compiler optimization","ffast-math","associativity","numerical stability","common subexpression"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Compiler Effects","importance_score":6,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":40,"chunk_index":0,"title":"Floating Point & Casting","summary":"Describes how overflow and rounding occur when converting between ints, floats, and doubles.","main_text":"Casts between numeric types behave differently depending on size and precision. int → float: no overflow but rounding may occur because float has only 24 bits of precision. int → double: no rounding for 32-bit ints because double has 53 bits of precision. float → double: safe (no rounding). double → float: overflow possible if exponent is too large; rounding occurs because float has fewer fraction bits. float/double → int: overflow possible; fractional part is truncated (round toward zero). The slide also poses a question about casting from long, hinting at platform-specific integer widths.","notes_text":"Crucial slide for students writing C programs involving mixed numeric types.","keywords":["casting","overflow","rounding","int to float","double to float","truncate"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Casting Behavior","importance_score":7,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":41,"chunk_index":0,"title":"References for Floating Point","summary":"Lists recommended external resources explaining floating-point arithmetic and common pitfalls.","main_text":"The slide references several authoritative resources: The Floating-Point Guide (floating-point-gui.de), Goldberg’s classic paper 'What Every Computer Scientist Should Know About Floating-Point Arithmetic', and 'Losing My Precision: Tips for Handling Tricky Floating Point Arithmetic'. These resources cover binary representation, pitfalls, precision issues, rounding, and best practices for working with floating point in software.","notes_text":"No technical content beyond listing external references.","keywords":["reference","floating point guide","Goldberg paper","precision","numerical analysis"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"References","importance_score":2,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit11_VirtualMemory","slide_number":42,"chunk_index":0,"title":"Hints for DataLab","summary":"Provides C-oriented guidance for converting floating-point bit patterns to integers, including handling of exponent extremes.","main_text":"Hints include: To compute absolute value, manipulate the sign bit. To compare floats without '==', evaluate whether |x−y| < epsilon. For float-to-int conversion: if the exponent is too big, return 0x80000000 (overflow); if too small, return 0; otherwise, extract and shift the integer portion of the significand (e.g., converting a bit pattern like 10100100.101…). The slide repeats the full Excess-127 exponent table for reference, showing special cases such as +∞, −∞, and NaN for exponent 255.","notes_text":"Directly relevant for assignments in bit-manipulation labs requiring manual IEEE-754 decoding.","keywords":["DataLab","float to int","absolute value","comparison","exponent table","overflow"],"images":[{"description":"Exponent table showing stored values, excess-127 mapping, and special encodings such as INF and NaN.","labels":["Excess-127","255","NaN","inf"],"position":{"x":0.1,"y":0.25,"width":0.8,"height":0.45}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"table"},"metadata":{"course":"CS356","unit":3,"topic":"DataLab Hints","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
