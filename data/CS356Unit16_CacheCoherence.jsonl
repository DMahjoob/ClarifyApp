{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":1,"chunk_index":0,"title":"Unit 16: Cache Coherence","summary":"Introduces Unit 16 and states the core goal: keeping private multicore caches consistent with shared memory.","main_text":"Unit 16: Cache Coherence. Keeping multicore caches in sync.","notes_text":"","keywords":["cache coherence","multicore","sync","unit 16","shared memory"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":16,"topic":"Cache Coherence Overview","importance_score":5,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":2,"chunk_index":0,"title":"Parallelism Landscape","summary":"Contrasts instruction-level and thread-level parallelism and motivates multicore growth.","main_text":"Instruction-Level Parallelism (Pipeline, Dynamic Scheduling). Diminishing returns in 2000–2005: silicon and energy costs growing faster than performance, inefficient after a point. Thread-Level Parallelism: “Replicate CPU cores with same design, use multiple threads.” Works well with data parallelism (scientific computing), or with request-level parallelism (multiuser OS, web server, database). Parallelism.","notes_text":"","keywords":["parallelism","ILP","thread-level parallelism","multicore","diminishing returns"],"images":[{"description":"Visual showing a multi-core CPU with replicated cores.", "labels":["Core (w/registers)","Multi-core CPU"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":4,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":16,"topic":"Parallelism Motivation","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":3,"chunk_index":0,"title":"Thread Pools and SMP","summary":"Uses a web-server thread pool to illustrate symmetric multiprocessing with shared memory.","main_text":"Thread Pool of Apache Web Server. Get page /index.html. SMP: Symmetric Multi-Processing. Most multicore systems are SMP. Share a single memory. Using a single address space. Uniform memory access (UMA) latency from all cores. Only up to approx. 32 cores. What about AWS x1.32xlarge with 64 cores? Multi-socket! It uses 4 × (16-core CPU). Memory/address space is still shared, but latency is not uniform (NUMA, see next slide). Shared-memory: each core can access/address the entire memory; Symmetric: uniform access time.","notes_text":"","keywords":["SMP","UMA","NUMA","thread pool","shared memory","multi-socket"],"images":[{"description":"Diagram contrasting thread pool requests with an SMP multicore layout.", "labels":["Thread Pool","SMP","UMA"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":5,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":16,"topic":"SMP Basics","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":4,"chunk_index":0,"title":"SMP Cache Hierarchy","summary":"Shows private L1/L2 caches per core, shared L3, and a single shared memory address space.","main_text":"Core (w/registers), L1 Cache, L2 Cache for each core. L3 Cache. Main Memory. Addresses 0x0000 to 0xFFFF. Multi-core CPU. on-chip interconnect.","notes_text":"","keywords":["L1","L2","L3","shared memory","address space","on-chip interconnect"],"images":[{"description":"Cache hierarchy diagram: per-core L1/L2 private caches, shared L3, shared main memory.", "labels":["L1 Cache","L2 Cache","L3 Cache","Main Memory"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"Multicore Cache Organization","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":5,"chunk_index":0,"title":"DSM: Distributed Shared Memory","summary":"Introduces distributed shared memory for scalability and bandwidth, while keeping a shared address space.","main_text":"DSM: Distributed Shared-Memory. Shared-memory: each core can address the entire memory; Distributed: more bandwidth; memory faster if local. Multi-core CPU with multiple on-chip interconnects and a CPU interconnect (ring, mesh, e.g., Intel Xeon UPI or AMD Epyc Infinity Fabric) on motherboard. Main Memory Addresses 0x00000 to 0x0FFFF and 0x10000 to 0x1FFFF.","notes_text":"","keywords":["DSM","distributed shared memory","bandwidth","local memory","UPI","Infinity Fabric"],"images":[{"description":"Diagram showing DSM across multiple sockets with shared address space but non-uniform latency.", "labels":["DSM","CPU interconnect","Main Memory"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":4,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"DSM / NUMA","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":6,"chunk_index":0,"title":"Distributed Systems vs Shared Memory","summary":"Contrasts distributed systems with DSM/SMP by noting memory is not shared and communication uses networking/RPC.","main_text":"Distributed System. Memory is not shared! Nodes have different address spaces; use RPCs to exchange data. I/O subsystem and network card. TCP/IP over Ethernet.","notes_text":"","keywords":["distributed system","no shared memory","RPC","TCP/IP","address spaces"],"images":[{"description":"Diagram showing two multicore nodes connected by Ethernet, emphasizing separate address spaces.", "labels":["TCP/IP over Ethernet","Memory is not shared"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"Distributed Systems Contrast","importance_score":5,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":7,"chunk_index":0,"title":"Challenges of Parallelism: Amdahl’s Law","summary":"Explains speedup limits from finite parallel fractions using Amdahl’s law and a 64-core example.","main_text":"Challenges of Parallelism. Not enough parallelism in programs. Amdahl’s law (you can only speedup the parallel fraction): T(n) = runtime with n cores = T(1) [(parallel fraction)/n + 1 − (parallel fraction)]. Example: if 99% of the program can run in parallel, using 64 cores is only T(1)/T(n) = 1/(0.99/64 + 0.01) = 39 times faster. Solution: Find algorithms that can be parallelized better!","notes_text":"","keywords":["Amdahl’s law","speedup","parallel fraction","64 cores","runtime"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":16,"topic":"Parallel Speedup Limits","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":8,"chunk_index":0,"title":"Challenges of Parallelism: Communication Latency","summary":"Quantifies core-to-core latency effects and motivates caching of shared data.","main_text":"Challenges of Parallelism. Slow communication between cores. 35–50 clock cycles to communicate with cores in same CPU. 100–300 to communicate with cores on separate CPU chips. Example: 4 GHz, base CPI of 0.5, 100 ns delay for remote references, 0.2% of remote references; effective CPI is base CPI + penalty for remote references = 0.5 + 0.002 × 100 ns / (0.25 ns) = 1.3. Solution: Restructure programs or cache shared data.","notes_text":"","keywords":["communication latency","remote references","CPI","NUMA cost","cache shared data"],"images":[{"description":"Diagram showing cores and caches connected by on-chip interconnect, illustrating remote latency.", "labels":["on-chip interconnect"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":16,"topic":"Communication Costs","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":9,"chunk_index":0,"title":"Coherence Problem in SMP Caches","summary":"Defines the coherence problem arising from private caches on different cores holding the same block.","main_text":"Coherence of SMP Caches. Coherence Problem: Threads of the same program can share memory while running on different cores (or switch core). What if two cores have the same block (same memory address) in their L1/L2 private caches? For read-only, it’s fine. What if one core writes to block 0 in its L1/L2 cache? The L1/L2 cache of the other core has stale data! Write-back policy alone isn’t safe.","notes_text":"","keywords":["coherence problem","private caches","stale data","write-back","SMP"],"images":[{"description":"Diagram of two cores with private caches and shared L3/memory, highlighting duplicated blocks.", "labels":["Core 1","Core 2","L1","L2","L3"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"Cache Coherence Motivation","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":10,"chunk_index":0,"title":"Example of Coherence Issues","summary":"Walks through a read/read/write sequence that leads to stale reads and multiple inconsistent versions of a block.","main_text":"Example of Coherence Issues. a) Core1 reads blocks 0,1,2,3. b) Core2 reads blocks 0,1,4,5. c) Core1 writes to block 0 in its L1 cache (write-back policy). d) Core2 reads block 0 in its L1 cache: data read by Core2 is stale/wrong! e) Core2 writes block 0 in its L1 cache: now we have two versions of block 0! We must propagate changes made to the local cache of one core to local caches of other cores (or invalidate them).","notes_text":"","keywords":["stale read","two versions","invalidate","propagate writes","example"],"images":[{"description":"Multi-step cache state diagram showing reads, write-back, stale data, and divergence.", "labels":["blocks 0–5","stale","invalidate"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"Coherence Failure Example","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":11,"chunk_index":0,"title":"Coherence vs. Consistency","summary":"Distinguishes what value may be read (coherence) from when it becomes visible (consistency).","main_text":"Coherence vs Consistency. Goal: “Any read of data returns the most recently written value.” Coherence = what values can be read after writes to X. Concurrent writes are serialized (e.g., during bus access) so that all cores see them in same order. Consistency = when new values of X can be read. Takes time to propagate writes to all cores. We assume to have waited for propagation.","notes_text":"","keywords":["coherence","consistency","serialization","propagation delay","visibility"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":16,"topic":"Coherence vs Consistency","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":12,"chunk_index":0,"title":"Cache Line States","summary":"Introduces the basic coherence states tracked per cache line.","main_text":"Goal: Track state of each cache line, in the cache of each core. Possible states: Invalid: This core doesn’t have a copy. Shared: This core has a copy that other cores may have as well (without changes). Modified: Only this core has copy and has made changes to it.","notes_text":"","keywords":["cache line states","Invalid","Shared","Modified","directory"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":16,"topic":"Coherence States (MSI Family)","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":13,"chunk_index":0,"title":"Directory Snapshot Example","summary":"Shows a directory view of blocks per core after a write, illustrating state assignments.","main_text":"Directory example after Core1 writes block 0. In L1 cache of Core 1: Block 4 invalid (not present), Block 1 shared (Core 2 has it too), Block 0 modified (Core 2 has a copy, but it shouldn’t!). Directory: Core 1 B0 modified, B1–3 shared, B4 invalid. Core 2 B0–1 shared, B2–3 invalid, B4–5 shared.","notes_text":"","keywords":["directory","modified","shared","invalid","block states"],"images":[{"description":"Diagram linking per-core caches to a directory state table at L3.", "labels":["Directory","B0 modified","B1 shared"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":16,"topic":"Directory Tracking","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":14,"chunk_index":0,"title":"Approaches: Directory vs Snooping","summary":"Compares the two main families of coherence protocols and where they are used.","main_text":"Approaches. Directory-Based (common in SMPs like i7 and DSMs like Xeon): Reads/writes update state of each block in a directory (usually at L3). The directory knows who has the block, propagates changes. Single directory for all cores in SMPs, distributed directory in DSMs. Snooping (common in SMPs, also some DSMs like Opteron): L1/L2 cache controllers of each core “snoop” read/write events, broadcast by others, track block states locally, invalidate caches.","notes_text":"","keywords":["directory-based","snooping","L3 directory","broadcast","invalidate"],"images":[{"description":"Side-by-side diagrams of directory-based and snooping coherence.", "labels":["Directory-Based","Snooping"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"comparison"},"metadata":{"course":"CS356","unit":16,"topic":"Coherence Approaches","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":15,"chunk_index":0,"title":"How Snooping Works (Example)","summary":"Step example where cores broadcast misses and invalidations, and a modified core supplies dirty data.","main_text":"Example of Snooping. a) Core1 reads blocks 0,1,2,3; read miss is broadcast by Core1. b) Core2 reads blocks 0,1,4,5; read miss is broadcast by Core2. c) To write to block 0 in L1, Core1 broadcasts an invalidate message; Core2 picks it up and invalidates; then Core1 can modify block 0. d.1) Core2 broadcasts a read miss of 0; Core1 snoops it and sends dirty block. d.2) L3 cache and Core2 receive the modified version. Key points: Core1 tells Core2 to invalidate its copy; Core1 snoops a miss and gives dirty block data to L3/Core2.","notes_text":"","keywords":["snooping","invalidate","read miss","dirty block","broadcast"],"images":[{"description":"Annotated multi-step snooping coherence diagram with misses and invalidates.", "labels":["miss","invalidate","dirty block"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":4,"num_images":1,"dominant_visual_type":"flowchart"},"metadata":{"course":"CS356","unit":16,"topic":"Snooping Example","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":16,"chunk_index":0,"title":"MSI Snooping Protocol Overview","summary":"Defines MSI states and describes read/write miss handling under snooping.","main_text":"MSI Snooping Protocol. Possible block states at the local L1/L2 cache of each core: Invalid (not cached at this core), Shared (cached, same value as L3; other cores can have it too), Modified (cached, different from L3, invalid at other cores). Only one core can have a cache block in the modified state. Protocol: When writing to a shared block, ask other cores to invalidate (and wait). Local state becomes modified; others become invalid. Further writes without notifying. When an invalid block is needed: Read miss asks if another core has it modified; if so, modified value broadcast to L3 and this core (state becomes S). Write miss: core with modified state must writeback & invalidate; state becomes M at this core.","notes_text":"","keywords":["MSI","Invalid","Shared","Modified","read miss","write miss"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":16,"topic":"MSI Protocol","importance_score":9,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":17,"chunk_index":0,"title":"MSI State Machine","summary":"Presents the MSI transition diagram and how to read local vs remote events.","main_text":"MSI Snooping Protocol state transitions among Invalid, Shared, Modified. Local events: local read miss broadcasts miss and gets from L3/bus; local write hit broadcasts invalidate then writes; local write miss broadcasts write miss and gets from L3/bus. Remote events: remote read miss may trigger write back/broadcast of value; remote write miss or invalidate forces transitions to Invalid. HOW TO READ: transition initiated by local event vs remote event with actions taken. Valid states with 2 cores: both invalid; one invalid one shared; one invalid one modified; both shared.","notes_text":"","keywords":["state machine","transitions","local event","remote event","invalidate","writeback"],"images":[{"description":"MSI finite-state diagram for cache-line coherence under snooping.", "labels":["Invalid","Shared","Modified"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"MSI State Transitions","importance_score":9,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":18,"chunk_index":0,"title":"Remote Read of Shared Block","summary":"Shows MSI behavior when a core reads a block that is Shared elsewhere.","main_text":"Remote Read of Shared Block. Core1 has an unmodified copy of L3 data, Core2 wants to read that. Core2 broadcasts read miss, reads from L3, new block state is Shared. Block state at Core1: Shared. Block state at Core2: Shared.","notes_text":"","keywords":["remote read","shared block","read miss","MSI"],"images":[{"description":"Two-core MSI example for remote read while block is Shared.", "labels":["Shared","read miss"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"MSI Examples","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":19,"chunk_index":0,"title":"Remote Read of Modified Block","summary":"Explains MSI behavior when another core reads a block in Modified state.","main_text":"Remote Read of Modified Block. Core1 has a modified value and Core2 broadcasts a miss. Core1 writes back/broadcasts the value; block goes to Shared for both cores. Block state at Core1: Shared. Block state at Core2: Shared.","notes_text":"","keywords":["remote read","modified block","write back","broadcast value","Shared"],"images":[{"description":"Two-core MSI example for remote read miss serviced by Modified owner.", "labels":["Modified","Shared","write back"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"MSI Examples","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":20,"chunk_index":0,"title":"Local Write of Shared Block","summary":"Shows that a writer must invalidate other Shared copies before writing.","main_text":"Local Write of Shared Block. Both cores have an unmodified copy of L3 data. To write, Core1 broadcasts invalidate, Core2 invalidates, and Core1 state becomes Modified.","notes_text":"","keywords":["local write","invalidate","Shared to Modified","MSI"],"images":[{"description":"MSI example of local write to Shared block requiring invalidation.", "labels":["invalidate","Modified"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"MSI Examples","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":21,"chunk_index":0,"title":"Remote Write of Modified Block","summary":"Explains the MSI role reversal when another core writes a block currently Modified elsewhere.","main_text":"Remote Write of Modified. Core1 has a Modified block and Core2 wants to write to it. Core2 broadcasts write miss; Core1 writes back/broadcasts modified value to L3/Core2; block states invert, and Core2 becomes Modified owner.","notes_text":"","keywords":["remote write","write miss","state inversion","Modified owner"],"images":[{"description":"MSI example of remote write miss causing writeback and ownership transfer.", "labels":["write miss","write back","Modified"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"MSI Examples","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":22,"chunk_index":0,"title":"Variants of MSI","summary":"Lists common MSI extensions that add new stable states to reduce traffic.","main_text":"Variants of MSI. MESI adds an Exclusive state: only this core has block (like Modified) but clean (same as L3). No need to send invalidates to modify; just mark as Modified. Exclusive changes to Shared on remote read miss. MESIF (Intel i7) adds Forward state, a specialized Shared state to designate the core that should respond to data requests. MOESI (AMD Opteron) adds Owned state to indicate block is owned by core cache and stale in L3; avoids writebacks upon remote read misses of a locally modified block (core broadcasts value).","notes_text":"","keywords":["MESI","Exclusive","MESIF","Forward","MOESI","Owned"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":16,"topic":"MSI Variants","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":23,"chunk_index":0,"title":"Coherence vs Atomicity","summary":"Warns that coherence actions aren’t fully atomic and can lead to deadlocks or non-atomic instruction behavior.","main_text":"We assumed all coherence operations to be atomic: a write/read miss is broadcast and detected, acquires bus, receives up-to-date value as a single atomic action. In practice, these are distinct operations; non-atomic actions introduce possibility of deadlocks. Coherence protocols are more complex to avoid deadlocks. Even then, coherence does not make assembly instructions atomic! Two threads executing addl $1,0x124 on different cores can both read x, compute x+1, and each write back, resulting in increment by 1 not 2.","notes_text":"","keywords":["atomicity","deadlocks","non-atomic coherence","addl race","bus acquisition"],"images":[{"description":"Timeline/diagram showing non-atomic add across two cores despite coherence.", "labels":["invalidate","miss","x+1"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":16,"topic":"Atomicity Pitfalls","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":24,"chunk_index":0,"title":"Locks Ensure Atomicity","summary":"Shows x86 lock-prefixed instructions and a spinlock using cmpxchg to enforce atomic updates.","main_text":"Locks Ensure Atomicity. Locking individual x86 instructions: lock addl $1,0x124 holds the cache line exclusively (M) by the core during execution; for other cores, it is invalid. Locking multiple instruction with lock cmpxchgl %edx,0x456: “if (*0x456 == %eax) then (*0x456 = %edx) else (%eax = *0x456)”. Spinlock example: get_lock loop reads lock variable, then uses lock cmpxchgl to set it to 1 atomically; execute critical section; release with movl $0,lock_address.","notes_text":"","keywords":["locks","lock prefix","cmpxchg","spinlock","atomic"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":16,"topic":"Atomic Operations","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":25,"chunk_index":0,"title":"Directory-Based Coherence Motivation","summary":"Motivates directories for DSM because snooping broadcasts don’t scale.","main_text":"Directory-Based Coherence. DSM distributes memory among cores. Each CPU has its own memory and DDR4 channels (34 GB/s). We can write more data in parallel (more bandwidth). But snooping broadcasts are not scalable with many CPUs/cores. Solution: Directory-Based Coherence Protocols. Each CPU has a directory with state of blocks of its memory. It knows which local/remote cores have copies of blocks. It forwards invalidate/data-fetch requests to those cores only. Easy to implement at L3 cache; used also for SMPs (e.g., Intel i7).","notes_text":"","keywords":["directory-based coherence","DSM scalability","no broadcast","L3 directory","invalidate forwarding"],"images":[{"description":"Diagram showing per-CPU directories controlling coherence for their memory blocks.", "labels":["Directory","DSM"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"Directory Coherence","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":26,"chunk_index":0,"title":"Directory-Based Example","summary":"Step example showing misses go to directory, directory sends invalidates, and owners write back on demand.","main_text":"Directory-Based Example. a) Core1 reads blocks 0,1,2,3; read miss to directory; data received. b) Core2 reads blocks 0,1,4,5; read miss to directory; data received. c) To write block 0 in L1, Core1 asks directory to send invalidate to nodes with the block; Core2 invalidates; then Core1 modifies block 0. d.1) Core2 sends read miss to directory, which asks Core1 to writeback to L3. d.2) Directory sends modified version to Core2. The directory forwards invalidate/data requests only to cores with the specific block (better scalability, more latency).","notes_text":"","keywords":["directory example","invalidate","owner writeback","read miss","scalability"],"images":[{"description":"Multi-step directory coherence diagram with directory-mediated invalidates and fetches.", "labels":["miss","invalidate","writeback"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"flowchart"},"metadata":{"course":"CS356","unit":16,"topic":"Directory Protocol Example","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":27,"chunk_index":0,"title":"Protocol at Cores (Directory-Based)","summary":"Gives the per-core state machine for directory coherence, paralleling MSI but via directory messages.","main_text":"Protocol at Cores. States: Uncached (no core has block), Shared (many clean copies), Modified (cached at owner core). Local read miss: send miss to directory. Local write hit: send invalidate to directory, wait, then write. Local write miss: send miss (new tag) to directory, receive. Remote directory commands include invalidate and fetch-invalidate (owner writes back). Similar to snooping but invalidation/fetch interacts with directory, and commands received only for blocks managed by this core.","notes_text":"","keywords":["core protocol","Uncached","Shared","Modified","fetch-invalidate","directory miss"],"images":[{"description":"Directory-based per-core state transition diagram.", "labels":["Uncached","Shared","Modified"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"Directory Core FSM","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":28,"chunk_index":0,"title":"Protocol at L3 Directory","summary":"Provides the directory’s own state transitions for remote read/write misses and writebacks.","main_text":"Protocol at L3 Directory. States: Uncached (no core has block), Shared (copies at core set S), Modified (cached at owner core). Remote read miss at i: if Uncached/Shared, update S and send data. Remote write miss at i: send invalidate to S or fetch-invalidate to owner, set S={i}, send data. Remote writeback clears ownership and may set S={} . Transitions initiated by remote events from core i; directory sends invalidate or fetch commands accordingly.","notes_text":"","keywords":["L3 directory protocol","S set","fetch-invalidate","remote miss","writeback"],"images":[{"description":"Directory state machine describing S-set updates and owner interactions.", "labels":["Uncached","Shared","Modified","S={i}"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"Directory FSM","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":29,"chunk_index":0,"title":"Coherence Pitfalls: False Sharing","summary":"Explains false sharing and why coherence traffic can devastate performance even without logical data sharing.","main_text":"Coherence Pitfalls for Programmers. False Sharing: When independent data of two threads (i.e., not shared) ends up on a cache line. Performance penalty: each thread is writing only part of the cache line, but coherence forces invalidations when threads run on separate cores. Example code toggling x and y in separate threads causes repeated invalidations because X and Y share a cache line. One solution: 64-byte alignment puts y on different line: int y __attribute__((aligned (64))) = 0.","notes_text":"","keywords":["false sharing","cache line","invalidations","alignment","performance pitfall"],"images":[{"description":"Diagram showing X and Y in one cache line leading to M/I ping-pong; contrasted with aligned separation.", "labels":["Cache Line","X","Y","M","I"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":16,"topic":"Programming Pitfalls","importance_score":7,"file_hash":"<sha256 placeholder>"}}
