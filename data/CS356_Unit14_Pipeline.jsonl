{"deck_name":"CS356_Unit14_Pipeline","slide_number":1,"chunk_index":0,"title":"Unit 14: Pipelining and Processor Architecture","summary":"Opens Unit 14 by introducing CPU pipelining as an “assembly line” approach to improving processor throughput.","main_text":"This title slide introduces Unit 14, focused on pipelining and processor architecture. The central framing is that pipelining works like an assembly line inside the CPU: different parts of instruction execution are overlapped so multiple instructions can be in progress at once. The deck will build from hardware basics up through processor datapaths and pipeline control, explaining how modern CPUs increase performance via parallelism of instruction stages.","notes_text":"","keywords":["Unit 14","pipelining","processor architecture","CPU assembly line","instruction throughput"],"images":[{"description":"Cover visual representing pipelining/CPU assembly line concept.","labels":[],"position":{"x":0.1,"y":0.15,"width":0.8,"height":0.7}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":14,"topic":"Pipelining Overview","importance_score":6,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":2,"chunk_index":0,"title":"Basic Hardware (HW) Review","summary":"Briefly signals a hardware fundamentals review needed before discussing pipelines.","main_text":"This slide marks the start of a quick review of basic hardware concepts that are prerequisites for understanding pipelining. The unit assumes familiarity with core digital logic, registers, and timing, which will be used to reason about datapaths and instruction stage overlap in later slides.","notes_text":"","keywords":["hardware review","digital logic","registers","timing","pipeline prerequisites"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Hardware Foundations","importance_score":5,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":3,"chunk_index":0,"title":"Logic Circuits: Combinational vs Sequential","summary":"Defines combinational and sequential logic and explains why both are essential CPU building blocks.","main_text":"The slide distinguishes two major classes of logic circuits. **Combinational logic** performs a specific function mapping inputs to outputs, has no internal state, and for any fixed input produces a deterministic output after a propagation delay. **Sequential logic** includes storage elements (registers) that hold state across time. Registers are described as fundamental building blocks: they remember bits for later use, behaving like variables in software. This distinction is critical because CPU datapaths use combinational blocks to compute results and sequential blocks to store intermediate values between clock cycles.","notes_text":"","keywords":["logic circuits","combinational logic","sequential logic","registers","state","propagation delay"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Logic Basics","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":4,"chunk_index":0,"title":"Combinational Logic Gates","summary":"Introduces basic logic gates and truth-table behavior used to build larger CPU components.","main_text":"This slide reviews fundamental combinational logic gates such as AND, OR, and NOT. It emphasizes that circuits composed of these gates implement Boolean functions. Truth-table examples illustrate how different input combinations map to outputs. These gates serve as the primitive units from which more complex hardware structures (like adders, multiplexers, and ALUs) are constructed.","notes_text":"","keywords":["logic gates","AND","OR","NOT","truth table","Boolean functions","combinational circuits"],"images":[{"description":"Diagram of AND/OR/NOT gates with truth-table style input/output labels.","labels":["OR gate","AND gate","NOT gate"],"position":{"x":0.1,"y":0.25,"width":0.8,"height":0.6}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Logic Gates","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":5,"chunk_index":0,"title":"Propagation Delay","summary":"Explains that real logic circuits take time to settle, motivating clocked designs.","main_text":"The slide defines **propagation delay**: digital logic does not produce outputs instantly. After inputs change, outputs stabilize only after some gate delay. The slide highlights that circuit delay accumulates across multiple gates, so deeper combinational logic increases total compute time. This matters for CPU timing because the clock period must be long enough for all combinational paths between registers to finish before the next clock edge.","notes_text":"","keywords":["propagation delay","gate delay","timing","critical path","clock period","combinational depth"],"images":[{"description":"Timing/logic example illustrating input changes and delayed output stabilization.","labels":[],"position":{"x":0.1,"y":0.25,"width":0.8,"height":0.6}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":14,"topic":"Timing and Delay","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":6,"chunk_index":0,"title":"ALUs (Arithmetic Logic Units)","summary":"Shows how logic gates combine into an ALU that performs arithmetic and Boolean operations.","main_text":"This slide introduces the **ALU**, built from combinational logic gates. The ALU performs arithmetic (e.g., addition, subtraction) and logical operations (AND, OR, XOR, compare) on binary inputs. The slide notes that inputs are multi-bit words (e.g., 32-bit), and the ALU operates bit-wise in parallel across those words. ALUs are central to instruction execution because most operations in the datapath flow through an ALU stage.","notes_text":"","keywords":["ALU","arithmetic logic unit","binary arithmetic","logic operations","datapath","32-bit words"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"ALU Basics","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":7,"chunk_index":0,"title":"Sequential Devices: Registers","summary":"Describes registers as clocked storage that capture values between combinational stages.","main_text":"The slide introduces registers as **sequential devices** that store state. A register captures its D input on a clock edge and holds that value until the next edge. Registers are used to separate combinational logic into stages: the ALU computes a result, which is then stored in a register so it can be used in later cycles. This clocked storage model is essential for pipelining, where each pipeline stage ends with registers to hold intermediate results.","notes_text":"","keywords":["registers","sequential logic","clock edge","state storage","pipeline stages","D input"],"images":[{"description":"Diagram showing ALU output latched into a register on a clock pulse; may include example assembly flow into %rax.","labels":["clock","register","ALU"],"position":{"x":0.08,"y":0.25,"width":0.84,"height":0.6}}],"layout":{"num_text_boxes":3,"num_images":2,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":14,"topic":"Registers","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":8,"chunk_index":0,"title":"Clock Signal and Cycle Time","summary":"Defines the clock waveform and relates cycle time to processor frequency.","main_text":"This slide explains the **clock signal** as an alternating high/low voltage pulse that coordinates sequential updates. The **cycle time** (clock period) is the time between clock edges; frequency is its inverse. The slide gives a numeric example connecting GHz to nanoseconds per cycle. The key idea is that all combinational logic between registers must complete within one cycle for correct operation.","notes_text":"","keywords":["clock signal","cycle time","frequency","GHz","clock period","timing constraint"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Clocking","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":9,"chunk_index":0,"title":"From x86 to RISC","summary":"Transitions from complex x86/CISC ideas toward RISC principles used in pipeline design.","main_text":"This slide marks the conceptual shift from x86 (historically CISC-style) toward RISC design thinking. The rest of the unit will rely on RISC-like uniform instruction stages, which are easier to pipeline and reason about than highly variable-latency CISC instructions.","notes_text":"","keywords":["x86","RISC transition","CISC vs RISC","pipeline friendliness","uniform stages"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"RISC Motivation","importance_score":6,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":10,"chunk_index":0,"title":"From CISC to RISC","summary":"Compares CISC and RISC philosophies and why RISC aligns with pipelining.","main_text":"The slide contrasts **CISC** and **RISC** architectures. CISC machines use complex instructions that may do more work per instruction but take varying time to execute, reducing the number of instructions needed per task. RISC machines favor simpler instructions that execute in a more uniform way and often in the same number of steps, enabling faster instruction throughput even if more instructions are required overall. This uniformity is a key reason RISC-style pipelines are cleaner and more efficient to design.","notes_text":"","keywords":["CISC","RISC","instruction complexity","uniform execution","pipeline design","throughput tradeoff"],"images":[{"description":"Comparison visual summarizing CISC vs RISC execution characteristics.","labels":["CISC","RISC"],"position":{"x":0.12,"y":0.28,"width":0.76,"height":0.55}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"comparison"},"metadata":{"course":"CS356","unit":14,"topic":"CISC vs RISC","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":11,"chunk_index":0,"title":"A RISC Subset of x86","summary":"Defines a simplified RISC-style subset of x86 by restricting memory access and register operations.","main_text":"This slide explains how to view a RISC-style subset of x86 by imposing constraints that make instruction execution more uniform. First, any mov that accesses memory is split into explicit load and store instructions: **ld** (load/read from memory) and **st** (store/write to memory). Second, ld/st are limited to at most indirect addressing with a displacement, so complex addressing modes like `ld 0x04(%rdi,%rsi,4), %rax` are disallowed as “too much work.” Allowed forms look like `ld 0x40(%rdi), %rax` or `st %rax, 0x40(%rdi)`. Third, arithmetic/logic instructions may only operate on register values. Memory-operand ALU ops are forbidden (e.g., `add (%rsp), %rax`), so the RISC equivalent requires an ld to bring the memory value into a register, an add between registers, then possibly an st back. The point is to standardize instruction behavior into clean stages suitable for pipelining.","notes_text":"","keywords":["RISC subset","x86 restrictions","ld","st","no complex addressing","register-only ALU","pipeline-friendly ISA"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"RISC Constraints for Pipelining","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":12,"chunk_index":0,"title":"Developing a Processor Organization","summary":"Lists core hardware components per instruction type to motivate a unified datapath.","main_text":"This slide inventories the major hardware blocks needed to execute different instruction classes in the simplified ISA. For **ALU-type** ops like `add %rax, %rbx`, the processor uses the **PC (rip)** to fetch, the **Register File** to read operands, and the **ALU** to compute results and update condition codes (flags). For **LD** instructions such as `ld 8(%rax), %rbx`, the datapath needs PC/I-Cache fetch, Register File read for the base register, the ALU to compute an effective address (`%rax + 8`), the **D-Cache** to read memory, and then writeback to a destination register. For **ST** like `st %rbx, 8(%rax)`, it similarly computes an address in the ALU and writes data to D-Cache. For conditional jumps (**JE**), the datapath fetches and decodes the instruction, checks condition codes, and if the condition is true updates `PC = PC + disp`. The slide’s takeaway is that a small repeated set of components supports all instruction types, enabling a consistent staged pipeline design.","notes_text":"","keywords":["processor organization","PC","register file","ALU","I-Cache","D-Cache","LD/ST/JE datapath"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":14,"topic":"Datapath Components","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":13,"chunk_index":0,"title":"Processor Block Diagram (Overall)","summary":"Shows a five-stage processor block diagram and stage timing that determines cycle time.","main_text":"This slide presents the overall processor block diagram organized into classic five stages: **Fetch**, **Decode**, **Execute**, **Mem**, and **WB (Writeback)**. The datapath includes an **I-Cache** for instruction fetch, a **Register File** for operand access, an **ALU** for effective address and arithmetic/logic work, a **D-Cache** for data reads/writes, and condition-code outputs (ZF, OF, CF, SF). Control signals derived during decode steer ALU operations and cache read/write behavior. The slide indicates each stage’s delay (about 10 ns per stage) and notes that the **clock cycle time equals the worst-case path delay**, summing to roughly **50 ns** through the full non-pipelined datapath. This diagram is the baseline that later slides specialize per instruction type and then pipeline by inserting registers between stages.","notes_text":"","keywords":["processor block diagram","5-stage datapath","Fetch","Decode","Execute","Mem","WB","cycle time","critical path"],"images":[{"description":"Overall 5-stage datapath diagram with I-Cache, RegFile, ALU, D-Cache, control signals, and condition codes; stage delays labeled ~10 ns each.","labels":["Fetch","Decode","Exec.","Mem","WB","I-Cache","D-Cache","ALU","RegFile","Control Signals","ZF","OF","CF","SF"],"position":{"x":0.05,"y":0.12,"width":0.9,"height":0.76}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Baseline Datapath","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":14,"chunk_index":0,"title":"Processor Block Diagram (add)","summary":"Specializes the datapath for an ALU instruction and shows where work happens by stage.","main_text":"This slide overlays the overall processor block diagram with the specific flow for an ALU-type add, e.g., `add %rax, %rdx` with machine code `[48 01 c2]`. In **Fetch**, the instruction is read from I-Cache using PC. In **Decode**, the register file reads the source and destination registers (%rax and %rdx) and the control unit identifies this as an ALU operation. In **Execute**, the ALU computes `%rax + %rdx`. There is no data-memory access in the **Mem** stage for add, so it is effectively idle. In **WB**, the computed result is written back to the destination register `%rdx`. The slide emphasizes the clean per-stage division that makes pipelining possible.","notes_text":"","keywords":["add datapath","ALU instruction","stage behavior","Fetch/Decode/Execute/Mem/WB","writeback"],"images":[{"description":"Block diagram annotated for add instruction path; highlights RegFile read, ALU add, and WB to destination register.","labels":["add %rax,%rdx","ALU op","WB result"],"position":{"x":0.06,"y":0.12,"width":0.88,"height":0.76}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Add Instruction Datapath","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":15,"chunk_index":0,"title":"Processor Block Diagram (ld)","summary":"Shows the datapath stages for a load instruction including effective-address calc and D-Cache read.","main_text":"This slide specializes the datapath for a load instruction, using an example whose machine code is `[48 8b 43 40]` (load from memory at base+0x40 into a register). In **Fetch**, the ld instruction is obtained from I-Cache. In **Decode**, the register file reads the base register (e.g., %rbx) and control identifies a load. In **Execute**, the ALU computes the effective address `base + displacement` (e.g., `%rbx + 0x40`). In **Mem**, the D-Cache is read at that address to obtain data. In **WB**, the loaded data is written into the destination register (e.g., `%rax = data`). The slide highlights that unlike add, ld uses the memory stage, which impacts pipeline hazards later.","notes_text":"","keywords":["ld datapath","load instruction","effective address","D-Cache read","writeback","displacement addressing"],"images":[{"description":"Block diagram annotated for ld instruction; ALU computes address, D-Cache supplies data, WB writes destination register.","labels":["ld","addr","data","0x40"],"position":{"x":0.06,"y":0.12,"width":0.88,"height":0.76}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Load Instruction Datapath","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":16,"chunk_index":0,"title":"Processor Block Diagram (st)","summary":"Shows the datapath stages for a store instruction including address calc and D-Cache write.","main_text":"This slide specializes the datapath for a store instruction, with example machine code `[48 89 43 40]` (store a register value to memory at base+0x40). In **Fetch**, the st instruction is read from I-Cache. In **Decode**, the register file supplies both the base register for addressing (e.g., %rbx) and the data register to store (e.g., %rax). In **Execute**, the ALU computes the effective address `%rbx + 0x40`. In **Mem**, the D-Cache performs a **write** of the source data to that address. There is no meaningful register writeback in **WB** for st. The slide reinforces the stage-by-stage decomposition shared with ld, differing mainly by memory read vs write and lack of WB.","notes_text":"","keywords":["st datapath","store instruction","effective address","D-Cache write","no WB","displacement"],"images":[{"description":"Block diagram annotated for st instruction; ALU computes address, D-Cache writes data from a register.","labels":["st","addr","data","0x40"],"position":{"x":0.06,"y":0.12,"width":0.88,"height":0.76}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Store Instruction Datapath","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":17,"chunk_index":0,"title":"Processor Block Diagram (je)","summary":"Specializes the datapath for a conditional jump based on condition codes and PC update.","main_text":"This slide specializes the datapath for a conditional jump, example `je` with displacement (e.g., `disp = 0x08`) and machine code `[74 08]`. In **Fetch**, the instruction is obtained from I-Cache using PC. In **Decode**, control logic recognizes a conditional branch while condition codes (such as ZF) are available from previous ALU results. In **Execute**, the ALU (or branch logic) evaluates the branch condition; if true, it computes the target `PC + disp`. The **Mem** stage is not used for je. In **WB**, there is no register writeback; the key effect is the potential PC update. The slide makes clear why branches are special in pipelines: the next PC may depend on a condition resolved only after decode/execute.","notes_text":"","keywords":["je datapath","conditional branch","PC update","displacement","condition codes","ZF"],"images":[{"description":"Block diagram annotated for je path; condition check leads to PC+disp target update.","labels":["je","PC + disp","disp=0x08","74 08"],"position":{"x":0.06,"y":0.12,"width":0.88,"height":0.76}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Branch Instruction Datapath","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":18,"chunk_index":0,"title":"Pipelining","summary":"Section header introducing pipelining as the next major topic.","main_text":"This slide is a section divider titled “Pipelining.” It marks the transition from baseline datapaths and per-instruction stage behavior to the idea of overlapping those stages across multiple instructions. The following slides will explain why pipelining improves throughput and how stage registers enable overlap.","notes_text":"","keywords":["pipelining","section header","throughput","stage overlap"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Pipelining Intro","importance_score":5,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":19,"chunk_index":0,"title":"Example (Non-Pipelined Timing)","summary":"Uses a loop example to show total time when stages are not overlapped.","main_text":"This slide presents a simple loop example: `for(i=0; i < 100; i++) C[i] = (A[i] + B[i]) / 4;`. It assumes a non-pipelined organization where each full instruction sequence for one iteration takes a fixed time per input set. The slide states an example timing like **10 ns per input set**, leading to **1000 ns total** for 100 iterations. A memory diagram labels arrays A[i], B[i], and C[i], along with loop index i and counter state. The purpose is to establish the baseline cost before pipelining so later slides can show how overlapping stages reduces total execution time.","notes_text":"","keywords":["timing example","non-pipelined cost","loop","A[i]","B[i]","C[i]","baseline throughput"],"images":[{"description":"Memory/loop schematic labeling A[i], B[i], C[i], and loop counter i to visualize per-iteration work.","labels":["A[i]","B[i]","C[i]","i","Cntr"],"position":{"x":0.12,"y":0.32,"width":0.76,"height":0.5}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":14,"topic":"Baseline Timing Example","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":20,"chunk_index":0,"title":"Pipelining Example","summary":"Introduces pipelining as inserting registers to split combinational logic into overlappable stages.","main_text":"Continuing the loop example `C[i] = (A[i] + B[i]) / 4`, this slide introduces pipelining at a high level. It depicts at least two stages (Stage 1 and Stage 2) and explains that **pipelining inserts registers** between pieces of combinational logic so execution is divided into multiple stages. Once split, different instructions can occupy different stages simultaneously, overlapping in time like an **assembly line**. The key takeaway is that pipelining improves **throughput** by allowing multiple iterations or instructions to be in flight at once, even though each individual instruction still passes through the same stages.","notes_text":"","keywords":["pipelining example","stage registers","overlap","assembly line","throughput improvement"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Pipelining Concept","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":21,"chunk_index":0,"title":"Need for Registers","summary":"Explains why registers are necessary in pipelined hardware to stabilize values while combinational logic operates.","main_text":"This slide introduces the motivation for pipeline registers in a processor datapath. Combinational logic requires stable inputs to compute correctly. Without registers, values would continuously ripple through circuits, making correct, timed operation impossible. Registers provide separation between combinational stages by capturing outputs and holding them fixed for one clock cycle. This enables predictable timing, ensures correct data propagation, and forms the foundation of synchronous pipelining. By storing intermediate values, registers allow different pipeline stages to work concurrently while preventing interference from upstream changes.","notes_text":"Core concept: registers isolate logic and make clocked, staged execution feasible.","keywords":["pipeline registers","combinational logic","timing","stabilizing values","synchronous design","pipeline stages"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Pipeline Registers","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":22,"chunk_index":0,"title":"Processors & Pipelines","summary":"Introduces how processors overlap instruction execution across stages from multiple viewpoints.","main_text":"This slide emphasizes that processors use pipelining to overlap the execution of multiple instructions. It presents pipelining from two viewpoints: the **instruction view**, where each instruction passes through stages like Fetch, Decode, Execute, Mem, and Writeback; and the **stage view**, where at any cycle, different instructions occupy different stages. These perspectives reinforce the idea that pipelining increases throughput by keeping all functional units working simultaneously. Rather than completing one full instruction before starting the next, pipelining allows concurrent progression, much like an industrial assembly line.","notes_text":"","keywords":["pipelining","overlap","instruction stages","throughput","instruction view","stage view"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Pipelining Overview","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":23,"chunk_index":0,"title":"Balancing Pipeline Stages","summary":"Shows why pipeline stages must be balanced so the clock period matches the longest stage delay.","main_text":"This slide explains that the clock period of a pipeline must be at least as long as the slowest pipeline stage. Several diagrams (Fig. 1–Fig. 4) illustrate how splitting logic into stages affects performance. If one stage contains too much logic compared to others, the clock must slow down to accommodate it, limiting throughput. Proper balancing requires distributing logic as evenly as possible so no single stage becomes the bottleneck. The slide emphasizes that the longest-stage delay determines the achievable clock frequency, and pipeline design must account for optimizing stage delays.","notes_text":"Key design challenge: minimize the maximum stage delay.","keywords":["pipeline balancing","clock period","critical path","stage delay","throughput","pipeline design"],"images":[{"description":"Four small diagrams labeled Fig.1–Fig.4 showing different ways logic can be partitioned among pipeline stages.","labels":["Fig.1","Fig.2","Fig.3","Fig.4"],"position":{"x":0.05,"y":0.25,"width":0.9,"height":0.55}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Stage Balancing","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":24,"chunk_index":0,"title":"Balancing Pipeline Stages (Main Points)","summary":"Compares pipeline depths and shows latency vs throughput for 1-, 2-, 4-, and 8-stage pipelines.","main_text":"This slide expands on stage balancing by listing key observations: (1) The latency of any single instruction equals the sum of all pipeline stage delays; deeper pipelines do not reduce per-instruction latency. (2) Throughput improves as long as each stage's delay decreases enough to justify the additional pipeline overhead. The slide shows example comparisons: a 1-stage (non-pipelined) system with 20 ns latency; a 2-stage pipeline achieving 2× throughput; a 4-stage pipeline achieving 4× throughput; and an 8-stage pipeline reaching 8× throughput. These examples illustrate that pipelining primarily improves throughput, not latency, and requires careful control of stage delay growth.","notes_text":"Deeper pipelines suffer diminishing returns due to overhead.","keywords":["pipeline stages","throughput","latency","multi-stage pipeline","performance scaling","pipeline depth"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Pipeline Depth","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":25,"chunk_index":0,"title":"Throughput and Latency","summary":"Defines throughput and latency using a timing table showing clock period and instructions per second.","main_text":"This slide contains a table comparing pipeline depth (n), clock period in picoseconds, and throughput in GIPS (billion instructions per second). The formulas given are: **throughput = 1/clock**, and **latency = n × clock**. Increasing pipeline depth can increase throughput by reducing clock period, but latency still grows linearly with the number of stages. The slide reinforces that pipelining trades off latency for throughput: even though the clock gets faster, each instruction still must traverse n stages, each with its own register overhead.","notes_text":"","keywords":["throughput","latency","clock period","GIPS","pipeline depth","timing table"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Latency vs Throughput","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":26,"chunk_index":0,"title":"5-Stage Pipeline Overview","summary":"Defines the 5 pipeline stages and maps them to machine code, operands, and example instructions.","main_text":"This slide introduces the classical 5-stage RISC pipeline: **Fetch**, **Decode**, **Execute**, **Memory**, and **Writeback**. It shows how machine code instructions, operands, and control signals flow through these stages. The table lists common operations: **ADD** (add reg to reg), **LD** (load from memory), **ST** (store to memory), and **JE** (conditional jump). Each instruction is decomposed into how it uses registers, ALU operations, displacement fields, and memory accesses. This slide forms the structural reference for all subsequent pipeline timing diagrams.","notes_text":"","keywords":["5-stage pipeline","Fetch","Decode","Execute","Mem","WB","instruction breakdown","ISA mapping"],"images":[{"description":"Pipeline diagram with five labeled stages and arrows showing operand and control-flow movement through the datapath.","labels":["Fetch","Decode","Exec","Mem","WB"],"position":{"x":0.08,"y":0.22,"width":0.84,"height":0.65}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"5-Stage Pipeline","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":27,"chunk_index":0,"title":"Pipelining — Execution of a Sequence","summary":"Introduces the idea of visualizing instruction overlap in a pipeline through cycle-by-cycle diagrams.","main_text":"This slide sets up a multi-instruction pipeline trace by stating: “Let’s see how a sequence of instructions can be executed.” It signals a transition into examples that show how instructions progress through Fetch, Decode, Execute, Mem, and WB over time. These examples illustrate how pipelining increases throughput by keeping all stages busy once the pipeline is filled.","notes_text":"","keywords":["pipeline sequence","instruction overlap","cycle diagram","pipeline fill"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Pipeline Tracing","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":28,"chunk_index":0,"title":"Sample Sequence — 1","summary":"Shows the first cycle-by-cycle pipeline diagram for a sample LD instruction.","main_text":"This slide begins the detailed pipeline walk-through for a sample instruction sequence. It charts how an LD instruction proceeds through the stages Fetch, Decode, Exec, Mem, and WB. The diagram includes the datapath components used by the LD: D-Cache, ALU, Register File, PC, and condition code outputs (ZF, OF, CF, SF). This visualization provides the first concrete illustration of overlapping activities in the pipeline as subsequent slides add additional instructions.","notes_text":"","keywords":["pipeline example","load instruction","stage timing","D-Cache","ALU","RegFile"],"images":[{"description":"Pipeline timing chart showing LD instruction moving through stages with datapath components displayed.","labels":["LD","Fetch","Decode","Exec","Mem","WB","D-Cache","ALU","RegFile","PC"],"position":{"x":0.05,"y":0.18,"width":0.9,"height":0.7}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Pipeline Example 1","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":29,"chunk_index":0,"title":"Sample Sequence — 2","summary":"Adds an ADD instruction into the pipeline, showing overlapping execution with the preceding LD.","main_text":"This slide continues the pipeline trace by adding the next instruction—an ADD—that begins Fetch while the LD is still in later stages. The diagram shows both instructions occupying different pipeline stages simultaneously. The datapath blocks (I-Cache, D-Cache, ALU, Register File, PC) illustrate how hardware is reused each cycle by different instructions. This slide demonstrates the essential throughput gain: new instructions enter the pipeline every cycle once the pipeline is filled.","notes_text":"","keywords":["pipeline overlap","ADD instruction","multi-instruction trace","timing diagram","throughput"],"images":[{"description":"Pipeline timing chart showing LD followed by ADD occupying staggered stages.","labels":["LD","ADD","Fetch","Decode","Exec","Mem","WB"],"position":{"x":0.05,"y":0.2,"width":0.9,"height":0.68}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Pipeline Example 2","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":30,"chunk_index":0,"title":"Sample Sequence — 3","summary":"Extends the example with additional instructions, showing deeper pipeline fill patterns.","main_text":"This slide deepens the pipeline example by adding more instructions and showing their progression across the five stages. Instructions include an ADD operation (`ADD %rcx, %rdx`) and a load involving displacement (`0x40 / %rbx / READ`). The diagram shows how the ADD uses the ALU in Execute, while the LD performs address calculation and then memory access. Condition codes (ZF, OF, CF, SF) update as the ADD executes. As additional instructions enter the pipeline, the chart reveals the steady-state overlap where nearly all stages are busy each cycle.","notes_text":"","keywords":["pipeline timing","ADD","LD with displacement","stage progression","condition codes","steady-state pipeline"],"images":[{"description":"Pipeline chart showing ADD and LD with displacement across multiple stages with ALU and D-Cache involvement.","labels":["ADD","LD","0x40","%rbx","READ","ZF","OF","CF","SF"],"position":{"x":0.06,"y":0.18,"width":0.88,"height":0.7}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Pipeline Example 3","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":31,"chunk_index":0,"title":"Sample Sequence — 4","summary":"Adds additional overlapping instructions to deepen the pipeline example, showing multiple operations in flight.","main_text":"This slide continues the pipeline execution trace by adding a third and fourth instruction, further illustrating how the pipeline fills over time. The stages Fetch, Decode, Exec, Mem, and WB are shown across multiple cycles with instructions staggered in a diagonal pattern. The datapath blocks (I-Cache, D-Cache, ALU, Register File, PC) appear repeatedly to show resource reuse. The deeper pipeline view demonstrates that once full, the pipeline retires one instruction per cycle, even though each instruction individually still takes five cycles to complete.","notes_text":"","keywords":["pipeline fill","overlapping instructions","timing diagram","throughput","multi-instruction pipeline"],"images":[{"description":"Pipeline timing chart with four instructions staggered across stages, showing full pipeline operation.","labels":["Fetch","Decode","Exec","Mem","WB"],"position":{"x":0.05,"y":0.2,"width":0.9,"height":0.7}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Pipeline Example 4","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":32,"chunk_index":0,"title":"Register File Usage in Pipeline","summary":"Shows when pipeline stages read or write registers and how hazards arise from this timing.","main_text":"This slide highlights how instructions interact with the register file in a pipelined processor. The Decode stage reads register operands, while the Writeback stage writes results. Because these operations occur in different cycles, dependencies between instructions can cause data hazards. The slide visually maps read and write operations to their respective cycles in the pipeline timeline. It serves as a precursor to identifying Read-After-Write (RAW) hazards, demonstrating how the pipeline might attempt to read stale data before previous instructions have updated the register file.","notes_text":"","keywords":["register file","pipeline hazard","RAW hazard","decode stage","writeback stage"],"images":[{"description":"Diagram showing timing of register reads during Decode and writes during WB across multiple instructions.","labels":["RegFile","read","write","Decode","WB"],"position":{"x":0.08,"y":0.22,"width":0.84,"height":0.65}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Register File Timing","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":33,"chunk_index":0,"title":"ISA Constraints on Pipelining","summary":"Describes how the instruction set architecture must support safe pipelining behavior.","main_text":"This slide states: “The ISA must be defined to work correctly with pipelining.” Some architectures are easier to pipeline than others. For example, instructions must not overwrite registers before dependent instructions can safely read them; condition codes must update in an orderly and predictable manner; memory dependencies must be well-defined; and the execution semantics must allow overlapping without violating program order. Many RISC architectures were explicitly designed with these constraints in mind, whereas CISC architectures like x86 require more complex hardware mechanisms to preserve correct behavior when pipelined.","notes_text":"","keywords":["ISA constraints","pipeline design","instruction semantics","program order","architectural hazards"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"ISA Constraints","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":34,"chunk_index":0,"title":"Data Hazards Overview","summary":"Introduces hazards as pipeline conditions where parallel execution causes incorrect behavior.","main_text":"This slide introduces hazards broadly: situations where parallel execution in a pipeline could violate program semantics. It lists the three major categories: (1) **Data Hazards**, which occur when instructions depend on the results of prior ones; (2) **Control Hazards**, associated with branches and changes to the PC; and (3) **Structural Hazards**, which occur when hardware resources are insufficient to support all simultaneous operations. The slide sets the stage for focusing specifically on data hazards next, which occur when the pipeline reads operands before they have been produced.","notes_text":"","keywords":["hazards","data hazard","control hazard","structural hazard","pipeline correctness"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Hazards Overview","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":35,"chunk_index":0,"title":"Data Hazards","summary":"Explains RAW, WAR, and WAW hazards and notes which occur in classic 5-stage RISC pipelines.","main_text":"This slide enumerates the three types of data hazards:\n• **RAW (Read After Write)**: The most common, occurring when an instruction tries to read a register before a previous instruction writes it.\n• **WAR (Write After Read)**: Occurs when an instruction writes a register before a previous instruction reads it.\n• **WAW (Write After Write)**: Occurs when two instructions write the same register out of order.\nIn classic 5-stage RISC pipelines, only RAW hazards occur, because register reads happen early (Decode) and writes occur late (Writeback). WAR and WAW hazards are eliminated by design through in-order execution and fixed pipeline stage behavior.","notes_text":"","keywords":["data hazard","RAW hazard","WAR hazard","WAW hazard","pipeline hazards","in-order pipeline"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Data Hazards","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":36,"chunk_index":0,"title":"RAW Hazard Example","summary":"Demonstrates a RAW hazard where an instruction reads a register before a previous one writes it.","main_text":"This slide presents a concrete RAW hazard example using register operations r2 ← r1 + r3 followed by r4 ← r2 + r5. The consumer instruction needs the updated value of r2, but in the pipeline it may attempt to read r2 during Decode before the producing instruction reaches Writeback. The diagram illustrates that without hazard mitigation, the consumer instruction would retrieve a stale value. The slide sets up the need for forwarding or stalling as solutions.","notes_text":"","keywords":["RAW hazard","register dependency","decode stage","writeback stage","data dependency"],"images":[{"description":"Dependency diagram showing two instructions where the second uses r2 produced by the first.","labels":["r2","dependency","RAW"],"position":{"x":0.1,"y":0.25,"width":0.8,"height":0.55}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"RAW Hazards","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":37,"chunk_index":0,"title":"Pipeline Diagram: RAW Hazard","summary":"Shows timeline visualization of a RAW hazard without forwarding or stalling.","main_text":"In this slide, the pipeline timing chart illustrates the RAW hazard across cycles. The producer instruction writes its result in the WB stage, which occurs after the consumer instruction attempts to read the register in Decode. This mismatch in timing creates the hazard. The diagram highlights the specific cycles where the Decode stage attempts to read a not-yet-written value. This visual reinforces why naive pipelining cannot guarantee correctness when dependent instructions are close together.","notes_text":"","keywords":["pipeline timing","RAW hazard","cycle diagram","producer-consumer dependency"],"images":[{"description":"Pipeline timing chart showing Decode of consumer preceding Writeback of producer.","labels":["Decode","WB","hazard"],"position":{"x":0.06,"y":0.22,"width":0.88,"height":0.65}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"RAW Timing","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":38,"chunk_index":0,"title":"Fixing Data Hazards","summary":"Introduces forwarding and stalling as the two primary mechanisms to fix RAW hazards.","main_text":"This slide states: “Data hazards can be fixed with: Forwarding — use the result early. Stalling — wait for result to be ready.” Forwarding allows the pipeline to directly route results from later stages (e.g., Exec or Mem) to earlier stages (e.g., Decode or Exec) without waiting for Writeback. Stalling introduces pipeline bubbles when forwarding cannot solve the hazard, especially for load-use hazards. The slide sets up the detailed diagrams explaining both techniques in the next slides.","notes_text":"","keywords":["data hazard fix","forwarding","stalling","pipeline bubble","RAW hazard solution"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Hazard Solutions","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":39,"chunk_index":0,"title":"Forwarding","summary":"Shows how forwarding bypasses the register file to supply results to dependent instructions earlier.","main_text":"Forwarding (also called bypassing) solves many RAW hazards by routing ALU or memory results to earlier pipeline stages before Writeback completes. The slide shows datapath arrows from the ALU/Mem outputs back into the Execute stage of a following instruction. This avoids reading stale values from the register file during Decode. The key idea is that even though the register file is not yet updated, the correct result exists earlier in the pipeline and can be forwarded to dependent instructions.","notes_text":"","keywords":["forwarding","bypassing","RAW hazard fix","ALU bypass","data dependency"],"images":[{"description":"Datapath diagram showing forwarding paths from ALU and Mem stages back into pipeline input.","labels":["ALU","Mem","forward","bypass"],"position":{"x":0.08,"y":0.22,"width":0.84,"height":0.65}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Forwarding Mechanism","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":40,"chunk_index":0,"title":"Forwarding Examples","summary":"Gives concrete examples showing when forwarding succeeds and when it cannot fully resolve hazards.","main_text":"This slide gives multiple examples of forwarding paths for operations such as r2 ← r1 + r3 followed by r4 ← r2 + r5. It shows that ALU results can be forwarded directly to the next instruction’s Execute stage. Another example shows forwarding of memory-loaded values, but also highlights that load-use hazards cannot always be solved by forwarding alone because loaded data becomes available only after the Mem stage. In such cases, a one-cycle stall is required. The diagrams visualize arrows representing the forwarded values entering the consumer instruction’s Execute stage.","notes_text":"","keywords":["forwarding examples","ALU result forwarding","load-use hazard","stall requirement","pipeline dependencies"],"images":[{"description":"Example diagrams showing different forwarding paths and one load-use case requiring a stall.","labels":["forward","ALU","load-use","stall"],"position":{"x":0.06,"y":0.2,"width":0.88,"height":0.7}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Forwarding Examples","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":41,"chunk_index":0,"title":"Forwarding: Solution 2","summary":"Shows forwarding hardware that bypasses register writes to supply ALU results directly to dependent instructions.","main_text":"This slide introduces forwarding (also called bypassing) as a hardware solution to data hazards in pipelined processors. When an instruction in the Execute stage produces a value that a following instruction needs, the processor can route this value directly to the dependent instruction without waiting for it to reach the Writeback stage. The diagram shows instruction i+2 in Fetch, i+1 in Decode, and an ADD instruction in Execute receiving forwarded operands. Forwarding eliminates many RAW hazards and reduces the number of pipeline stalls required.","notes_text":"","keywords":["forwarding","bypassing","data hazard","RAW hazard","pipeline","ALU result","dependency"],"images":[{"description":"Diagram showing three instructions in different stages and a forwarding path sending an ALU output back to an earlier stage.","labels":["Fetch","Decode","Exec","ADD","forwarding path"],"position":{"x":0.1,"y":0.25,"width":0.8,"height":0.55}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Forwarding","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":42,"chunk_index":0,"title":"Solving Data Hazards","summary":"Explains that forwarding solves most RAW hazards and introduces the special case of the load-use hazard.","main_text":"This slide summarizes how forwarding reduces most data hazards by supplying results as soon as they are computed. However, forwarding alone cannot handle the load-use hazard, where an instruction immediately following a load needs the loaded value before it is available in the pipeline. Because the loaded data is only ready after the Memory stage, the next instruction cannot receive it in time through forwarding alone. Thus, while forwarding is powerful, additional hazard detection and stalling hardware are required for load-use situations.","notes_text":"","keywords":["data hazard","RAW hazard","load-use hazard","forwarding","stall","pipeline dependency"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Data Hazards","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":43,"chunk_index":0,"title":"Load-Use Hazard","summary":"Shows the classic load-use hazard where forwarding cannot supply the loaded value in time.","main_text":"The load-use hazard occurs when an instruction performs a load from memory and the very next instruction needs that loaded value. The pipeline diagram shows that the load's result is not ready until the Memory stage, while the dependent instruction requires it in the Execute stage one cycle earlier. Because forwarding cannot provide data that does not yet exist, the processor must insert a bubble (stall) to delay the dependent instruction. This slide emphasizes that load-use hazards are unavoidable without stalling.","notes_text":"","keywords":["load-use hazard","load instruction","stall","bubble","pipeline timing","forwarding limitation"],"images":[{"description":"Pipeline chart showing a load instruction followed by a dependent instruction, with a stall cycle inserted.","labels":["Load","Dependent instruction","stall","Mem","Exec"],"position":{"x":0.1,"y":0.22,"width":0.8,"height":0.6}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Load-Use Hazard","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":44,"chunk_index":0,"title":"Load-Use Hazard: Insert a Bubble","summary":"Demonstrates inserting one stall cycle to allow the load instruction to finish before the dependent instruction executes.","main_text":"To resolve the load-use hazard, the processor inserts one bubble (stall cycle) between the load instruction and the dependent instruction. The diagram shows that the dependent instruction is held in the Decode stage while the load proceeds to Memory. After the loaded value is available, forwarding can supply it to the dependent instruction in the Execute stage. This slide illustrates exactly how the single-cycle stall enables correct execution without violating the pipeline’s timing constraints.","notes_text":"","keywords":["bubble","stall","hazard resolution","pipeline stall","load-use dependence","Decode stage stall"],"images":[{"description":"Diagram showing Decode stage stall while the load proceeds, creating a bubble in the pipeline.","labels":["stall","bubble","Decode","Mem","Exec"],"position":{"x":0.08,"y":0.2,"width":0.84,"height":0.6}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Load-Use Stall","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":45,"chunk_index":0,"title":"Hazard Detection Unit","summary":"Introduces hardware that detects when an instruction depends on a load in the immediately preceding cycle.","main_text":"The hazard detection unit monitors the pipeline for load-use hazards. It identifies when the instruction in the Decode stage needs a register that is the destination of a load instruction in the Execute stage. If such a condition is detected, the hazard detection unit asserts control signals that stall the Decode stage, insert a bubble into the Execute stage, and prevent the Fetch stage from advancing. This makes the pipeline pause just long enough for the load value to become available.","notes_text":"","keywords":["hazard detection unit","stall logic","control signals","load dependency","pipeline control","Decode stall"],"images":[{"description":"Block diagram showing hazard detection logic comparing register IDs and triggering stall signals.","labels":["Hazard Detection","stall","register match"],"position":{"x":0.1,"y":0.25,"width":0.8,"height":0.55}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Hazard Detection","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":46,"chunk_index":0,"title":"Hazard Detection Logic (Detail)","summary":"Shows the exact condition that triggers a stall when a load-use hazard is detected.","main_text":"This slide details the boolean logic used by the hazard detection unit. A stall is required if the instruction in the Execute stage is a load, and the Decode stage instruction reads a register that matches the load's destination register. In this case, the pipeline must freeze the PC and IF/ID pipeline register, and insert a NOP into the ID/EX register. The slide typically includes the exact conditions comparing register numbers and load control signals.","notes_text":"","keywords":["hazard logic","stall condition","load-use detection","boolean logic","pipeline registers","NOP insertion"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Hazard Logic","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":47,"chunk_index":0,"title":"Control Hazards","summary":"Introduces hazards caused by branches and changes in control flow.","main_text":"Control hazards occur when the pipeline does not know the next instruction to fetch because of a branch instruction. Since branches determine the next PC value, the Fetch stage may continue fetching the wrong instructions until the branch resolves. This slide states that control hazards are a major performance limiter in deeper pipelines and motivates the need for techniques like stalling, flushing, branch prediction, and delayed branching.","notes_text":"","keywords":["control hazard","branch hazard","pipeline flush","branch resolution","PC update","wrong-path instructions"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Control Hazards","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":48,"chunk_index":0,"title":"Branch Hazard Timeline","summary":"Shows pipeline timing for branches and highlights when the branch condition is known.","main_text":"This slide shows the cycle-by-cycle pipeline behavior for a branch instruction. The branch instruction is fetched and decoded normally, but its condition and target are not known until a later stage—typically the Execute stage. Instructions fetched after the branch may be invalid if the branch is taken, requiring a flush of those pipeline stages. The timeline illustrates where these wrong-path instructions appear and why the pipeline must remove them.","notes_text":"","keywords":["branch hazard","timing diagram","pipeline flush","branch resolution","wrong-path fetch"],"images":[{"description":"Timeline chart for branch execution showing when the branch is resolved and where wrong-path instructions appear.","labels":["branch","flush","timeline"],"position":{"x":0.1,"y":0.2,"width":0.8,"height":0.6}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Branch Timing","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":49,"chunk_index":0,"title":"Flushing the Pipeline","summary":"Explains flushing wrong-path instructions after a branch is resolved.","main_text":"When a branch instruction resolves and its outcome differs from the predicted or default path, all instructions fetched after the branch but before its resolution must be flushed. Flushing replaces these instructions with NOPs to prevent them from modifying the machine state. The slide shows how the flush control signal clears the IF/ID and ID/EX pipeline registers. This ensures the pipeline restarts from the correct path without executing invalid instructions.","notes_text":"","keywords":["pipeline flush","branch misprediction","control hazard","NOP injection","pipeline registers"],"images":[{"description":"Diagram showing control signals clearing pipeline registers after a branch resolves.","labels":["flush","pipeline register","branch"],"position":{"x":0.1,"y":0.28,"width":0.8,"height":0.5}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Flushing","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":50,"chunk_index":0,"title":"Branch Prediction (Intro)","summary":"Introduces the idea of branch prediction to reduce control hazard penalties.","main_text":"Because waiting for branches to resolve causes pipeline stalls and flushes, processors use branch prediction to guess the outcome of branches before they execute. If the prediction is correct, the pipeline continues without interruption. If incorrect, the pipeline flushes the mispredicted instructions and resumes from the correct target. This slide motivates why prediction is essential for high performance in modern pipelined processors.","notes_text":"","keywords":["branch prediction","control hazard","pipeline performance","misprediction","prediction accuracy"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Branch Prediction","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":51,"chunk_index":0,"title":"A Look Ahead: Branch Prediction","summary":"Explains that the pipeline’s current handling of branches is too slow and motivates the need for more advanced prediction techniques.","main_text":"This slide points out that the pipeline currently assumes branches resolve late, which causes stalls and incorrect-path execution. Because many programs contain frequent branches, this naive approach wastes cycles and lowers throughput. The slide introduces the idea that real processors must predict the outcome of branches earlier, either through static heuristics or dynamic history-based mechanisms. It emphasizes that improving prediction accuracy is essential for reducing pipeline flushes and maintaining high instruction-per-cycle performance.","notes_text":"","keywords":["branch prediction","pipeline stalls","control hazards","static prediction","dynamic prediction","performance impact"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Branch Prediction Motivation","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":52,"chunk_index":0,"title":"Static Branch Prediction","summary":"Introduces fixed prediction strategies such as always-taken and always-not-taken and discusses their limitations.","main_text":"This slide describes static branch prediction, where the processor uses a predetermined rule to guess branch outcomes. Common strategies include always predicting not-taken, always predicting taken, or choosing based on branch direction (e.g., backward branches taken for loops). While simple and low-cost, static predictors cannot adapt to changing program behavior. As a result, their accuracy is limited, especially for branches whose behavior varies over time. The slide sets up the need for dynamic predictors that learn patterns at runtime.","notes_text":"","keywords":["static prediction","always taken","always not taken","branch behavior","pipeline control hazards","prediction limitations"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Static Branch Prediction","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":53,"chunk_index":0,"title":"Dynamic Branch Prediction (Overview)","summary":"Introduces hardware that learns branch behavior using runtime feedback rather than fixed rules.","main_text":"The slide explains that dynamic branch prediction improves performance by monitoring actual branch behavior over time. Hardware records recent outcomes and uses that history to make better guesses for future executions of the same branch. Unlike static strategies, dynamic predictors can adapt to patterns such as alternating behavior or phase changes in loops. This adaptability significantly increases prediction accuracy, reducing the number of pipeline flushes caused by mispredictions.","notes_text":"","keywords":["dynamic prediction","branch history","runtime learning","misprediction reduction","adaptive hardware","pipeline performance"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Dynamic Prediction Intro","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":54,"chunk_index":0,"title":"1-Bit Branch History Predictor","summary":"Describes the simplest dynamic predictor that tracks the last outcome of a branch and reuses it.","main_text":"This slide introduces the 1-bit branch predictor, which stores a single bit per branch indicating whether it was last taken or not taken. When the branch is encountered again, the predictor repeats the previous outcome. Although simple, this approach fails on loop-closing branches because a single misprediction causes the predictor to flip incorrectly, leading to two consecutive wrong guesses. The slide highlights the weakness of relying solely on the most recent outcome.","notes_text":"","keywords":["1-bit predictor","branch history bit","taken/not taken","loop branches","misprediction","dynamic prediction"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"1-Bit Predictors","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":55,"chunk_index":0,"title":"2-Bit Saturating Counter Predictor","summary":"Explains how using two bits reduces mispredictions caused by temporary behavior changes.","main_text":"This slide describes the widely used 2-bit saturating counter predictor. Instead of remembering only the most recent branch outcome, the predictor maintains a four-state machine ranging from strongly-not-taken to strongly-taken. The predictor changes states gradually, requiring two consecutive opposite outcomes to flip prediction direction. This added stability significantly improves accuracy for loop branches and reduces incorrect flips caused by one-time anomalies.","notes_text":"","keywords":["2-bit predictor","saturating counter","state machine","branch prediction","loop behavior","accuracy improvement"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"2-Bit Saturating Predictor","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":56,"chunk_index":0,"title":"Summary: Branch Prediction Importance","summary":"Reviews how prediction minimizes control hazards and enables high pipeline throughput.","main_text":"This slide concludes the branch prediction section by reiterating that control hazards severely limit pipeline efficiency if branches are resolved late. Accurate prediction—whether static or dynamic—allows the pipeline to continue fetching and decoding instructions without waiting for the branch result. Dynamic predictors, especially multi-bit and history-based mechanisms, dramatically reduce misprediction penalties. The slide emphasizes that effective branch prediction is essential for modern high-performance pipelined processors.","notes_text":"","keywords":["branch prediction summary","control hazards","pipeline throughput","misprediction cost","dynamic predictors","performance"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Branch Prediction Summary","importance_score":9,"file_hash":"sha256_placeholder"}}

