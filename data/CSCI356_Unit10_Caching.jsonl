{"deck_name": "CS356Unit10_Caches", "slide_number": 1, "title": "Unit 10: Caches", "summary": "Title slide introducing the cache memory unit and its role in improving memory access performance.", "main_text": "Unit 10: Caches\nImproving Memory Access Performance", "notes_text": null, "keywords": ["caches", "memory hierarchy", "CS356"], "images": [{"type": "logo", "description": "CS356 Trojan logo and university branding"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "title_slide"}, "section": "Introduction", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache overview"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 2, "title": "Why Caches?", "summary": "Introduces the motivation for caches by highlighting the growing CPU-memory performance gap.", "main_text": "CPU speed has increased much faster than main memory speed. Caches are used to reduce the average memory access time by storing frequently used data closer to the processor.", "notes_text": null, "keywords": ["cpu-memory gap", "performance", "latency", "cache motivation"], "images": [{"type": "diagram", "description": "Graph showing widening gap between CPU performance and memory speed"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "comparison"}, "section": "Cache Motivation", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache necessity"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 3, "title": "Memory Hierarchy", "summary": "Displays the layered structure of memory with tradeoffs between speed, size, and cost.", "main_text": "The memory hierarchy consists of registers, L1, L2, L3 caches, main memory, and disk storage, each varying in speed, cost, and capacity.", "notes_text": null, "keywords": ["memory hierarchy", "l1 cache", "l2 cache", "latency"], "images": [{"type": "diagram", "description": "Pyramid showing levels of memory hierarchy"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Cache Architecture", "metadata": {"course": "CS356", "unit": 10, "topics": ["memory structure"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 4, "title": "Principle of Locality", "summary": "Explains temporal and spatial locality as foundational reasons caches improve performance.", "main_text": "Temporal locality: recently accessed data is likely to be accessed again soon. Spatial locality: nearby data is likely to be accessed soon. Caches exploit these behaviors to reduce latency.", "notes_text": null, "keywords": ["locality", "temporal locality", "spatial locality"], "images": [{"type": "diagram", "description": "Memory access pattern illustrating clustered and repetitive usage"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Locality", "metadata": {"course": "CS356", "unit": 10, "topics": ["locality principle"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 5, "title": "Cache Terminology", "summary": "Introduces fundamental terms such as hit, miss, and eviction in cache operation.", "main_text": "A cache hit occurs when requested data is found in the cache. A miss occurs when it is not. When the cache is full, an existing block must be evicted.", "notes_text": null, "keywords": ["hit", "miss", "eviction", "cache block"], "images": [{"type": "flowchart", "description": "Flow of memory request through cache system"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Cache Basics", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache terminology"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 6, "title": "Cache Organization", "summary": "Introduces how caches are structured into blocks, lines, and sets.", "main_text": "The cache is divided into blocks (also called lines). Each block holds a fixed-size chunk of data from memory. These blocks are organized into sets depending on the cache mapping technique.", "notes_text": null, "keywords": ["cache organization", "blocks", "cache lines", "sets"], "images": [{"type": "diagram", "description": "Block-based structure of a cache with labeled lines"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Cache Architecture", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache structure"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 7, "title": "Direct-Mapped Cache", "summary": "Explains how each memory block maps to exactly one cache line.", "main_text": "In a direct-mapped cache, each block of main memory maps to exactly one location in the cache. While simple and fast, this can lead to conflict misses.", "notes_text": null, "keywords": ["direct mapped", "cache mapping", "conflict miss"], "images": [{"type": "diagram", "description": "Mapping from memory blocks to single cache lines"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Cache Mapping", "metadata": {"course": "CS356", "unit": 10, "topics": ["direct mapping"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 8, "title": "Fully Associative Cache", "summary": "Shows that any memory block can be placed in any cache line.", "main_text": "Fully associative caches allow any block of main memory to be stored in any cache line. This reduces conflict but increases lookup complexity.", "notes_text": null, "keywords": ["fully associative", "cache", "flexible mapping"], "images": [{"type": "diagram", "description": "Memory blocks mapping to any cache location"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Cache Mapping", "metadata": {"course": "CS356", "unit": 10, "topics": ["associative mapping"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 9, "title": "Set-Associative Cache", "summary": "Combines features of direct-mapped and fully associative caches.", "main_text": "In set-associative caches, each block maps to a specific set but can be stored in any line within that set, balancing speed and flexibility.", "notes_text": null, "keywords": ["set-associative", "mapping", "cache sets"], "images": [{"type": "diagram", "description": "Block mapping into cache sets"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Cache Mapping", "metadata": {"course": "CS356", "unit": 10, "topics": ["set associative"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 10, "title": "Replacement Policies", "summary": "Introduces policies for deciding which cache line to evict.", "main_text": "When a cache is full, replacement policies determine which line to remove. Common strategies include LRU, FIFO, and Random replacement.", "notes_text": null, "keywords": ["replacement policy", "LRU", "FIFO", "random eviction"], "images": [{"type": "chart", "description": "Comparison table of eviction strategies"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "comparison"}, "section": "Replacement", "metadata": {"course": "CS356", "unit": 10, "topics": ["replacement policies"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 11, "title": "Write Policies", "summary": "Explains how writes are handled in cache systems.", "main_text": "Write-through immediately updates main memory, while write-back delays updates until eviction. Each approach has performance and consistency tradeoffs.", "notes_text": null, "keywords": ["write policy", "write-back", "write-through"], "images": [{"type": "diagram", "description": "Write operation flow through cache"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Write Strategies", "metadata": {"course": "CS356", "unit": 10, "topics": ["write policies"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 12, "title": "Cache Performance Metrics", "summary": "Defines how cache effectiveness is measured.", "main_text": "Cache performance is measured using hit rate, miss rate, miss penalty, and average memory access time (AMAT). These metrics evaluate efficiency.", "notes_text": null, "keywords": ["AMAT", "hit rate", "miss rate", "performance"], "images": [{"type": "formula", "description": "AMAT calculation equation"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "text-heavy"}, "section": "Performance", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache performance"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 13, "title": "Cache Optimization Techniques", "summary": "Introduces methods for improving cache efficiency and reducing misses.", "main_text": "Techniques such as blocking, prefetching, and loop transformation improve locality and reduce cache miss rates.", "notes_text": null, "keywords": ["optimization", "prefetching", "blocking", "loop tiling"], "images": [{"type": "diagram", "description": "Illustration of optimized loop access patterns"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Optimization", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache optimization"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 14, "title": "Policies & Metadata", "summary": "Section divider introducing cache policies and metadata management.", "main_text": "Policies & Metadata", "notes_text": null, "keywords": ["policies", "metadata", "section divider"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_header"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache policies"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 15, "title": "Designing a Cache", "summary": "Overview of cache design considerations including write policy, eviction policy, and organization.", "main_text": "Caching is everywhere: L1/L2/L3 caches, paging between MM and disk, file I/O, browser pages/images, DNS. Design issues include: Write Policy (whether to modify data just in cache or persist changes), Eviction Policy (deciding what to remove when full), Organization (keeping track of cache content, selecting block size for spatial locality).", "notes_text": null, "keywords": ["cache design", "write policy", "eviction policy", "organization", "caching applications"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache design principles"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 16, "title": "Terminology: Hit, Miss, Eviction", "summary": "Defines fundamental cache operations: hit, miss, and eviction with visual flow.", "main_text": "When the processor generates a memory read or write, it first checks cache: Yes means hit (get data quickly from cache), No means miss (must load a block and store in cache for future accesses). If cache is full, must evict some other block. Blocks are larger than requested data to exploit spatial locality and start on addresses that are multiples of block size (e.g., 64 B).", "notes_text": null, "keywords": ["hit", "miss", "eviction", "cache operation", "block alignment"], "images": [{"type": "flowchart", "description": "Flow showing request at 0x400028, cache miss, loading cache line 400020-40003f, memory response, cache forwarding, and subsequent cached access"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache terminology"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 17, "title": "Write Policy", "summary": "Introduces write-through and write-back policies for handling write hits.", "main_text": "In case of a write hit, what should we do? Write Through Policy: Update data in the cache and in the following level. Write Back Policy: Update data only in current level, keep track of which blocks have modified data, when a modified block is evicted or cache is flushed, write it back to the next level to persist changes.", "notes_text": null, "keywords": ["write policy", "write-through", "write-back", "write hit"], "images": [{"type": "diagram", "description": "CPU, Registers, Cache, Main Memory hierarchy"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["write policies"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 18, "title": "Write Through", "summary": "Details write-through policy characteristics and performance implications.", "main_text": "We immediately update the block in the current level and the next one. Keeps both versions in synch at all times. Can write just the modified portion. Poor performance when: next level of hierarchy is slow (e.g., L3 to MM, or MM to disk), there are many repeated writes to the same block.", "notes_text": null, "keywords": ["write-through", "synchronization", "performance", "memory hierarchy"], "images": [{"type": "diagram", "description": "CPU, Registers, Cache, Main Memory with immediate update flow"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["write-through policy"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 19, "title": "Write Back", "summary": "Explains write-back policy with delayed persistence and dirty bit tracking.", "main_text": "Update the block only in the current level and write it to the next level only when it is evicted (write only the final version, not intermediate ones). Next level can be out-of-date. Need to track modified blocks and write back entire blocks (instead of just modified portions, which would be too difficult to track). Writing to current level is faster! Write back is definitely faster when the next level is much slower (e.g., in L3 or for memory paging); in practice, used.", "notes_text": null, "keywords": ["write-back", "dirty bit", "delayed write", "eviction"], "images": [{"type": "flowchart", "description": "4-step process: 1) Update only block in cache, 2) Make other updates in cache, 3) When cache full and need to evict modified block, 4) write back block and then evict"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["write-back policy"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 20, "title": "Eviction Policy", "summary": "Compares LRU, FIFO, and Random eviction policies.", "main_text": "What block to evict when the cache is full? LRU: The least recently used block - usually the best policy because it exploits temporal locality, hard to implement, often just approximated with easier policies. FIFO: The oldest block - not always good, the block may be old but we may have accessed it several times, even recently. Random: Pick a block randomly - performs surprisingly well.", "notes_text": null, "keywords": ["eviction policy", "LRU", "FIFO", "random", "temporal locality"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["eviction policies"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 21, "title": "Block Metadata", "summary": "Introduces the metadata needed for each cache block.", "main_text": "Data of cache blocks doesn't tell us: 1) Where in memory (range of addresses) a block comes from - we need to know to check whether it's a hit or miss, 2) Whether the block was modified - if modified, we have to write it back to memory before evicting, 3) When the block was last accessed - to implement LRU or pseudo-LRU.", "notes_text": null, "keywords": ["metadata", "cache block", "address tag", "modified bit", "LRU tracking"], "images": [{"type": "diagram", "description": "CPU with registers, ALU, cache, and main memory showing metadata requirements"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache metadata"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 22, "title": "Block Metadata: Example", "summary": "Concrete example showing valid bit, modified bit, and address range for cache blocks.", "main_text": "Valid Bit: Whether we already have data on this block of cache (valid=0 if line is empty and we ignore remaining metadata). Modified Bit: Whether we have modified this block (i.e., only the cached copy has been updated, we need to write back in case of eviction/flush). Address Range: Where the data is coming from, e.g., memory addresses 0x7c0 to 0x7cf. Do we really need to store the entire address range? No, just the tag 0x7c.", "notes_text": null, "keywords": ["valid bit", "modified bit", "dirty bit", "tag", "address range"], "images": [{"type": "diagram", "description": "Cache showing four blocks with metadata: 0x7c0-7cf (Valid, Modified), 0x470-47f (Valid, Unmodified), two empty blocks"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["metadata example"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 23, "title": "Splitting an Address", "summary": "Explains how memory addresses are split into block number and byte offset.", "main_text": "Given a memory address of m = n + k bits (e.g., m = 12 bits, split into n = 8 plus k = 4): There are 2^m addresses, each pointing to a different byte in memory (e.g., 0x000 to 0xFFF). The most significant n bits (e.g., 0xFD) identify a memory block of 2^k bytes. The block start address has all 0's for the least significant k bits (e.g., 0xFD0). The block end address has all 1's for the least significant k bits (e.g., 0xFDF). All addresses in the block have the same initial n bits, which tell us where the block is in memory.", "notes_text": null, "keywords": ["address splitting", "block number", "byte offset", "address fields"], "images": [{"type": "diagram", "description": "Address breakdown showing A[11:4] as Block # and A[3:0] as Byte # in block, with memory blocks from 0x000 to 0xFF0"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["address structure"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 24, "title": "Different Split", "summary": "Shows address splitting with different bit allocations (7+5 instead of 8+4).", "main_text": "What if we split an address of m = 12 bits into n = 7 plus k = 5 bits? Still 2^12 addresses, 0x000 to 0xFFF. The block is identified by the most significant 7 bits (e.g., 1111 110). The block start address has all 0's for the least significant k bits: 1111 110 0 0000 = 0xFC0. The block end address has all 1's for the least significant k bits: 1111 110 1 1111 = 0xFDF. Blocks include 2^5 = 32 bytes. Note how we convert hex to binary, since the split is not at a multiple of 4 bits.", "notes_text": null, "keywords": ["address splitting", "binary conversion", "block size", "bit allocation"], "images": [{"type": "diagram", "description": "Address showing A[11:5] as Block # and A[4:0] as Byte # in block, with example 0xFD4 = 1111 110 1 0100"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["address splitting variations"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 25, "title": "Analogy: Hotel Rooms", "summary": "Uses hotel room numbering as analogy for address block identification.", "main_text": "To refer to the range of rooms on the second floor, left aisle we would just say rooms 20x. 1st Digit = Floor, 2nd Digit = Aisle, 3rd Digit = Room within aisle. For 4 word (16-byte) blocks: Addr Range 000-00f (Binary: 0000 0000 0000..1111), 010-01f (Binary: 0000 0001 0000..1111). For 8 word (32-byte) blocks: Addr Range 000-01f (Binary: 0000 000 00000..11111), 020-03f (Binary: 0000 001 00000..11111).", "notes_text": null, "keywords": ["analogy", "hotel rooms", "address ranges", "block identification"], "images": [{"type": "diagram", "description": "Hotel layout showing 1st and 2nd floors with rooms numbered 100-109, 120-129, 200-209, 220-229"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["address analogy"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 26, "title": "Revised Metadata", "summary": "Shows complete metadata structure with valid bit, dirty bit, and tag.", "main_text": "Only valid bit, dirty bit, tag (for now, we ignore metadata for eviction policy). Examples shown with cache blocks containing tags like T=0111 1100 (V=1 D=0) for address 0x7c0-7cf, and T=0100 0111 (V=1 D=1) for 0x470-47f. Address splitting shown as A[11:4] for Tag and A[3:0] for Byte offset. Example addresses: 0x7c4 = 01000111 1100 (Byte 4 within block 7c0-7cf), 0xaf1 = 00011010 1111 (Byte 1 within block af0-aff).", "notes_text": null, "keywords": ["metadata structure", "valid bit", "dirty bit", "tag bits", "address fields"], "images": [{"type": "diagram", "description": "Cache and Memory diagram showing cache blocks with metadata (T, V, D bits) and corresponding memory blocks"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["metadata implementation"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 27, "title": "Organization & Lookup", "summary": "Section divider introducing fully-associative, set-associative, and direct mapping.", "main_text": "Organization & Lookup: Fully-Associative, Set-Associative, Direct", "notes_text": null, "keywords": ["organization", "lookup", "mapping schemes", "section divider"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_header"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache organization"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 28, "title": "Lookup: Fully Associative Cache", "summary": "Explains fully associative cache where blocks can be placed anywhere.", "main_text": "If we keep all cache blocks in a single set (fully associative cache), to find whether the cache has data of a given address, we need to compare the tag with those of all blocks. Everyday Example: You lost your keys - You think back to where you have been lately: library, class, gym. Where do you have to look to find your keys? If you had been home all day and discovered your keys were missing, where would you look? Key lesson: If something can be anywhere you have to search everywhere. By contrast, if we limit where things can be then our search need only look in those limited places.", "notes_text": null, "keywords": ["fully associative", "cache lookup", "search everywhere", "tag comparison"], "images": [{"type": "diagram", "description": "Cache with four blocks showing tags and valid/dirty bits"}], "layout": {"num_text_boxes": 3, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["fully associative cache"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 29, "title": "Parallel Tag Comparison", "summary": "Shows hardware implementation of parallel tag comparison for fully associative cache.", "main_text": "Requires additional hardware, expensive when caches have many blocks (> 16 or 32). Address = A[11:0] = 0x47c, Tag = A[11:4] = 0x47, Byte Offset A[3:0] = 0xc. When a block can be anywhere you have to search everywhere. Shows parallel comparators checking tag 0100 0111 against all cache block tags simultaneously, with valid bits ANDed, results ORed for HIT/MISS signal.", "notes_text": null, "keywords": ["parallel comparison", "hardware cost", "tag matching", "fully associative"], "images": [{"type": "circuit_diagram", "description": "Logic diagram showing parallel comparators, AND gates with valid bits, OR gate for hit/miss detection"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["parallel tag comparison"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 30, "title": "K-Way Set-Associative Cache", "summary": "Introduces set-associative cache as compromise between fully associative and direct mapped.", "main_text": "Idea: Given a cache of N blocks, split the cache into S = N/K sets, each with K blocks (ways). Split the address into 3 parts: tag | set index | block offset. Search for the tag only inside the set given by the index. On a miss, save inside the set given by the index (if full, evict a line in the set). Lookup is faster (up to K comparisons), but we have to evict from a specific set - other sets may have empty or less recently used lines.", "notes_text": null, "keywords": ["set-associative", "K-way", "sets", "ways", "tag index offset"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["set-associative cache"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 31, "title": "K-Way Set-Associative Cache", "summary": "Detailed example of 2-way set-associative cache with address mapping.", "main_text": "12-bit addresses, B=16 bytes/block, K=2 ways. Offset: B=16 bytes per block, log2(B) = 4 offset bits, determines byte/word within the block. Set: S=N/K=2 sets, log2(S) = 1 set bit, performs hash function (i mod S). Tag: Remaining bits, identifies blocks that map to the same bucket (block 0x00, 0x08, 0x0a, 0x0c). Example shows memory blocks mapping to Set 0 (even) or Set 1 (odd). Write 0x084 maps to Set 0, Block 0x080-0x08f can be placed anywhere in set 0.", "notes_text": null, "keywords": ["2-way set-associative", "address fields", "set mapping", "hash function"], "images": [{"type": "diagram", "description": "Cache with Set 0 and Set 1, showing blocks with tags, memory blocks 08-0c, address field breakdown"}], "layout": {"num_text_boxes": 3, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["set-associative example"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 32, "title": "K-Way Set-Associative Cache", "summary": "Shows cache operation after placing first block in set 0.", "main_text": "Continuing previous example: We picked the first line in set 0 for block 0x080-0x08f with T=0000 100, V=1, D=1. Shows the cache state after write to 0x084 has been processed and stored in Cache Block 0 of Set 0.", "notes_text": null, "keywords": ["cache placement", "set selection", "write operation"], "images": [{"type": "diagram", "description": "Cache showing Set 0 with populated block 0x080-0x08f, Set 1 empty"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache operation sequence"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 33, "title": "K-Way Set-Associative Cache", "summary": "Shows multiple operations filling both sets.", "main_text": "Operations: Write 0x084 (Set 0), Read 0x0b0 (Set 1), Read 0x0c8 (Set 0). Then we picked the first line in set 1 and the second in set 0. Cache now contains: Set 0 with blocks 0x080-0x08f (T=0000 100) and 0x0c0-0x0cf (T=0000 110), Set 1 with block 0x0b0-0x0bf (T=0000 101).", "notes_text": null, "keywords": ["multiple operations", "set filling", "cache state"], "images": [{"type": "diagram", "description": "Cache showing both sets with multiple blocks populated"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache filling sequence"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 34, "title": "K-Way Set-Associative Cache", "summary": "Demonstrates cache conflict requiring eviction.", "main_text": "Operations include Read 0x0a4 which maps to Set 0. Set 0 is full, we can only store this line there (set idx is 0) - must evict! Shows the cache conflict situation where both ways in Set 0 are occupied but a new block needs to be placed there.", "notes_text": null, "keywords": ["cache conflict", "eviction needed", "set full"], "images": [{"type": "diagram", "description": "Cache showing Set 0 full with two blocks, new block 0x0a0-0x0af needs placement"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache conflict"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 35, "title": "K-Way Set-Associative Cache", "summary": "Shows eviction and write-back of dirty block.", "main_text": "We evict the LRU block (first line), but first we write it back (it's dirty). Block 0x080-0x08f is written back to memory before being replaced with 0x0a0-0x0af. Final cache state shows Set 0 with blocks 0x0a0-0x0af and 0x0c0-0x0cf, Set 1 with block 0x0b0-0x0bf.", "notes_text": null, "keywords": ["LRU eviction", "write-back", "dirty block", "replacement"], "images": [{"type": "diagram", "description": "Cache after eviction, showing write-back arrow from cache to memory"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["eviction process"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 36, "title": "Fully Associative Cache", "summary": "Defines fully associative as special case of set-associative.", "main_text": "Fully associative caches are a special case of set-associative caches where: S=1 (a single set), K=N (all lines in that set). Since s = log2(S) = log2(1) = 0, we don't need set index bits - we just split into tag and block offset: b = log2(B) bits for block offset, t = m - b bits for the tag.", "notes_text": null, "keywords": ["fully associative", "single set", "no index bits", "tag and offset"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["fully associative definition"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 37, "title": "Fully Associative Cache", "summary": "Example of fully associative cache operation with first write.", "main_text": "Block 0 can go in any empty cache block, but let's just pick cache block 2. Write 0x004 places block 0x000-0x00f in Cache Block 2 with tag T=0000 0000, V=1, D=1. Address split into Tag (8 bits) and Offset (4 bits), no set index needed.", "notes_text": null, "keywords": ["fully associative example", "flexible placement", "no set constraint"], "images": [{"type": "diagram", "description": "Cache with 4 blocks, memory blocks shown, block 0x000-0x00f placed in cache block 2"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["fully associative example"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 38, "title": "Fully Associative Cache", "summary": "Shows multiple operations filling empty blocks freely.", "main_text": "Blocks can go anywhere so the next 3 accesses will prefer to fill in empty blocks. Operations: Write 0x004, Read 0x018, Read 0xfe0, Read 0xffc. Cache now contains blocks: 0x000-0x00f (T=0000 0000), 0x010-0x01f (T=0000 0001), 0xfe0-0xfef (T=1111 1110), 0xff0-0xfff (T=1111 1111).", "notes_text": null, "keywords": ["multiple operations", "free placement", "filling cache"], "images": [{"type": "diagram", "description": "Cache with all 4 blocks populated from various memory locations"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["fully associative filling"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 39, "title": "Fully Associative Cache", "summary": "Demonstrates LRU eviction with write-back in fully associative cache.", "main_text": "Now cache is full so when we access a new block (0xfc0-0xfcf) we have to evict a block from cache. Let us pick the Least Recently Used (LRU). Since it is dirty/modified we must write 0x000-0x00f back to MM. Operations show Read 0xfc4 causing eviction of block 0x000-0x00f and replacement with 0xfc0-0xfcf (T=1111 1100).", "notes_text": null, "keywords": ["LRU eviction", "fully associative", "write-back", "cache full"], "images": [{"type": "diagram", "description": "Cache showing eviction with write-back arrow, new block 0xfc0-0xfcf replacing 0x000-0x00f"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["fully associative eviction"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 40, "title": "Direct Mapping", "summary": "Defines direct-mapped cache as special case of set-associative.", "main_text": "Direct-mapped caches are a special case of set-associative caches where: S=N (a set for each line), K=1 (obviously, one line per set). The set index is also called block index (since each block is a set). Address fields: b = log2(B) bits for block offset, s = log2(N) bits for block index, t = m - s - b bits for the tag.", "notes_text": null, "keywords": ["direct mapped", "one line per set", "block index", "fixed mapping"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["direct mapping definition"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 41, "title": "Direct Mapping", "summary": "Illustrates direct-mapped cache with modulo-based placement.", "main_text": "Each block from memory can only be put in one location. MM block i maps to cache block 'i mod N'. Each set has only 1 block. Examples show: MM Block 0 maps to Cache Block 0 (0 mod 4), MM Block 1 to Cache Block 1 (1 mod 4), MM Block 2 to Cache Block 2 (2 mod 4), MM Block 3 to Cache Block 3 (3 mod 4), MM Block 4 to Cache Block 0 (0 mod 4), MM Block 5 to Cache Block 1 (1 mod 4), MM Block 6 to Cache Block 2 (2 mod 4).", "notes_text": null, "keywords": ["direct mapped", "modulo mapping", "fixed location", "hash function"], "images": [{"type": "diagram", "description": "Cache with 4 blocks, memory blocks 0-6 with arrows showing mod 4 mapping"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["direct mapping example"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 42, "title": "Direct Mapping", "summary": "Example of placing block 0x080 in direct-mapped cache.", "main_text": "Address = 080, Offset: B=16 bytes per block, log2(B) = 4 offset bits, determines byte/word within the block. Block: N=4 blocks in the cache, log2(N) = 2 block bits, performs hash function (i mod N). Tag: Remaining bits, identifies blocks that map to the same bucket (block 0, 4, 8). Block 0x080-0x08f hashes/maps to cache block 0 and thus must be placed there! Write 0x084 with tag 0000 10, block 00.", "notes_text": null, "keywords": ["direct mapping", "address breakdown", "fixed placement", "block calculation"], "images": [{"type": "diagram", "description": "Cache blocks 0-3, memory blocks 08-0c, address field breakdown showing tag|block|offset"}], "layout": {"num_text_boxes": 3, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["direct mapping operation"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 43, "title": "Direct Mapping", "summary": "Shows multiple operations in direct-mapped cache.", "main_text": "Other blocks must be placed where they hash which is computed by simply using the block bits. Operations: Write 0x084 (block 00, tag 0000 10), Read 0x09c (block 01, tag 0000 10), Read 0x0b8 (block 11, tag 0000 10), Read 0x0c8 (block 00, tag 0000 11). Cache contains blocks at positions determined by block index: 0x080-0x08f at position 0, 0x090-0x09f at position 1, 0x0b0-0x0bf at position 3, 0x0c0-0x0cf at position 0.", "notes_text": null, "keywords": ["direct mapping operations", "block placement", "hash calculation"], "images": [{"type": "diagram", "description": "Cache showing blocks placed according to block index bits"}], "layout": {"num_text_boxes": 3, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["direct mapping sequence"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 44, "title": "Summary of Mapping Schemes", "summary": "Compares fully associative, direct-mapped, and set-associative caches.", "main_text": "Fully associative: Most flexible (fewer evictions), longest search time O(N), no hashing, can be placed anywhere in cache, must search N locations. Direct-mapped cache: Least flexible (more evictions), shortest search time O(1), h(a) = block field, only search 1 location. K-way Set Associative mapping: Compromise - 1-way set associative = Direct, N-way set associative = Fully Assoc., work to search is O(K), for small K search in parallel: O(1), h(a) = set field, only search k locations.", "notes_text": null, "keywords": ["mapping comparison", "flexibility tradeoffs", "search complexity", "cache organization"], "images": [{"type": "diagram", "description": "Three address breakdowns: Fully Associative (Tag|Offset), Direct Mapped (Tag|Block|Offset), Set Associative (Tag|Set|Offset)"}], "layout": {"num_text_boxes": 4, "num_images": 1, "dominant_visual_type": "comparison"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["mapping schemes summary"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 45, "title": "Practice Example", "summary": "Practice problem setup for cache address mapping.", "main_text": "16-bit addresses, 2 kB cache, 32 bytes/block. Find address mapping for: Fully Associative, Direct Mapping, 4-way Set Associative, 8-way Set Associative. Questions: B? N? S, K?", "notes_text": null, "keywords": ["practice problem", "cache parameters", "address mapping"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["practice problem"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 46, "title": "Solution", "summary": "Calculates cache parameters B, N, and S for practice problem.", "main_text": "First find parameters: B = Block size, N = Cache blocks, S = Sets for 4-way and 8-way. B is given as 32 bytes/block. N depends on cache size and block size: N = (2 kB) / (32 bytes/block) = (2^11 / 2^5) = 2^6 = 64 blocks in the cache. S for 4-way & 8-way: S_4-way = N/k = 64/4 = 16 sets, S_8-way = N/k = 64/8 = 8 sets.", "notes_text": null, "keywords": ["parameter calculation", "cache size", "block count", "set count"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["solution derivation"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 47, "title": "Solution: Fully Associative", "summary": "Address breakdown for fully associative cache.", "main_text": "log2(32) = 5 byte/offset bits (A4-A0). Tag = 11 upper bits (A15-A5). Parameters: B = 32, N = 64, S_4-way = 16, S_8-way = 8.", "notes_text": null, "keywords": ["fully associative solution", "address fields", "tag bits", "offset bits"], "images": [{"type": "diagram", "description": "16-bit address showing Tag (bits 15-5) and Offset (bits 4-0)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["fully associative solution"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 48, "title": "Solution: Direct Mapping", "summary": "Address breakdown for direct-mapped cache.", "main_text": "log2(32) = 5 word bits (A4-A0). log2(64) = 6 block bits (A10-A5). Tag = 5 upper bits (A15-A11). Parameters: B = 32, N = 64, S_4-way = 16, S_8-way = 8.", "notes_text": null, "keywords": ["direct mapping solution", "block index", "tag calculation"], "images": [{"type": "diagram", "description": "16-bit address showing Tag (15-11), Block (10-5), Offset (4-0)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["direct mapping solution"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 49, "title": "Solution: Set Associative, K=4", "summary": "Address breakdown for 4-way set-associative cache.", "main_text": "log2(32) = 5 word bits (A4-A0). log2(16) = 4 set bits (A8-A5). Tag = 7 upper bits (A15-A9). Parameters: B = 32, N = 64, S_4-way = 16, S_8-way = 8.", "notes_text": null, "keywords": ["4-way set-associative", "set index", "address solution"], "images": [{"type": "diagram", "description": "16-bit address showing Tag (15-9), Set (8-5), Offset (4-0)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["4-way solution"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 50, "title": "Solution: Set Associative, K=8", "summary": "Address breakdown for 8-way set-associative cache.", "main_text": "log2(32) = 5 word bits (A4-A0). log2(8) = 3 set bits (A7-A5). Tag = 8 upper bits (A15-A8). Parameters: B = 32, N = 64, S_4-way = 16, S_8-way = 8.", "notes_text": null, "keywords": ["8-way set-associative", "set bits", "tag length"], "images": [{"type": "diagram", "description": "16-bit address showing Tag (15-8), Set (7-5), Offset (4-0)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["8-way solution"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 51, "title": "Practice Example", "summary": "Additional practice problem slide.", "main_text": "Practice Example", "notes_text": null, "keywords": ["practice", "additional problem"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_header"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["practice"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 52, "title": "Access Time & Design Tradeoffs", "summary": "Section divider introducing cache performance analysis.", "main_text": "Access Time & Design Tradeoffs", "notes_text": null, "keywords": ["access time", "design tradeoffs", "performance", "section divider"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_header"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache performance"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 53, "title": "Average Access Time", "summary": "Derives formula for average memory access time (AMAT).", "main_text": "Average Access Time = (Hit Rate) × (Hit Time) + (Miss Rate) × [(Hit Time) + (Miss Penalty)] = [(Hit Rate) + (Miss Rate)] × (Hit Time) + (Miss Rate) × (Miss Penalty) = (Hit Time) + (Miss Rate) × (Miss Penalty). Definitions: Hit Rate = (# hits)/(# accesses), fraction of accesses that find data in the cache. Miss Rate = (# misses)/(# accesses) = 1 – (Hit Rate). Hit Time = Time to lookup and retrieve data from the cache. Miss Penalty = Additional time to load data into the cache after a miss. Intuition: In either case (hit or miss), we still need to lookup and retrieve data from the cache (hit time); in case of a miss, we also pay the miss penalty to load data into the cache.", "notes_text": null, "keywords": ["AMAT", "average access time", "hit rate", "miss rate", "miss penalty"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["AMAT formula"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 54, "title": "Average Access Time: Example", "summary": "Numerical example calculating AMAT.", "main_text": "Average Access Time = (Hit Time) + (Miss Rate) × (Miss Penalty) = (5 ns) + [1 - (Hit Rate)] × (200 ns) = (5 ns) + (0.1) × (200 ns) = 25 ns. Note: The (Miss Penalty) includes the time to obtain the data from the next level of the cache hierarchy. That may also be a hit or a miss.", "notes_text": null, "keywords": ["AMAT calculation", "numerical example", "performance metrics"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["AMAT example"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 55, "title": "Average Access Time: 2-Level Cache", "summary": "Extends AMAT formula to multi-level cache hierarchy.", "main_text": "Average Access Time = (Hit Time of L1) + (Miss Rate of L1) × (Miss Penalty of L1). (Miss Penalty of L1) = average time to access L2 = (Hit Time of L2) + (Miss Rate of L2) × (Miss Penalty of L2). (Miss Penalty of L2) = average time to access Main Memory. Shows hierarchical structure: L1 (hit or miss), L2 (hit or miss), Main Memory (always a hit).", "notes_text": null, "keywords": ["multi-level cache", "L1 L2", "hierarchical AMAT"], "images": [{"type": "flowchart", "description": "Three-level hierarchy showing L1, L2, and Main Memory with hit/miss paths"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["multi-level AMAT"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 56, "title": "Average Access Time: Example", "summary": "Numerical example for 2-level cache AMAT calculation.", "main_text": "Consider a 2-level cache L1/L2/Memory where: (Hit Time of L1) = 5 ns, (Hit Time of L2) = 20 ns, (Hit Rate of L1) = 0.9, (Hit Rate of L2) = 0.8, (Time to Transfer a Cache Line from Memory) = 200 ns. Average Access Time = (Hit Time of L1) + (Miss Rate of L1) × (Miss Penalty of L1) = (5 ns) + (0.1) × [(20 ns) + (0.2) × (200 ns)] = 11 ns.", "notes_text": null, "keywords": ["2-level AMAT", "L1 L2 calculation", "performance example"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["2-level AMAT example"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 57, "title": "Cache Design Tradeoffs", "summary": "Three strategies to reduce average access time.", "main_text": "Average Access Time = (Hit Time) + (Miss Rate) × (Miss Penalty). Three ways to reduce access time: Reduce Hit Time - usually done by starting cache access before the end of virtual address translation (next unit). Reduce Miss Penalty - multi-level caches (previous example), smaller blocks. Reduce Miss Rate - crucial at lower levels of memory hierarchy (high miss penalty).", "notes_text": null, "keywords": ["design tradeoffs", "hit time", "miss penalty", "miss rate", "optimization"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache optimization strategies"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 58, "title": "Miss Rate", "summary": "Categorizes types of cache misses: compulsory, capacity, and conflict.", "main_text": "What causes cache misses? Compulsory Misses: First access to some data will always result in a miss. Capacity Misses: Due to cache being too small to hold our working set of data or to exploit spatial locality. Conflict (or Collision) Misses: Due to constraints of mapping scheme (direct or set associative) forcing us to evict data that we later need. Question: Can the cache accommodate the working set of frequently accessed data?", "notes_text": null, "keywords": ["miss types", "compulsory miss", "capacity miss", "conflict miss", "working set"], "images": [{"type": "diagram", "description": "Graph showing accessed memory addresses over time, plus cache showing Set 0 full causing conflict"}], "layout": {"num_text_boxes": 2, "num_images": 2, "dominant_visual_type": "mixed"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["miss classification"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 59, "title": "Block Size", "summary": "Analyzes impact of block size on cache performance.", "main_text": "A larger block size: Increases miss penalty (we have to load larger blocks into the cache), Reduces compulsory misses (we load more nearby data and take better advantage of spatial locality), Increases capacity/conflict misses (for a fixed cache size, we will have fewer blocks if block size is larger - a sparse working set may require many blocks but we can hold fewer, each set has fewer blocks which may result in more conflicts). Graph shows miss rate vs block size with increasing trends after optimal point.", "notes_text": null, "keywords": ["block size", "spatial locality", "miss penalty tradeoff"], "images": [{"type": "graph", "description": "Miss rate vs block size curve from Patterson & Hennessy showing U-shaped relationship"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["block size tradeoffs"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 60, "title": "Cache Size", "summary": "Examines effects of cache size on performance and cost.", "main_text": "A larger cache size: Reduces capacity/conflict misses (we can store more data, and more blocks in each set), Increases hit time (we need to search tags through more blocks), Increases cost and power, Has diminishing returns after we can hold most of the working set. Cache size of a level should be greater than the cache size of previous level. Shows hierarchy: L1 (128kB data & instr), L2 (4 kB to 4 MB). P(Miss in L2, given a miss in L1) is higher due to lower locality given miss in L1. P(Miss in L1 & L2) similar to P(Miss) in single-level with same size as L2 (but latency is lower, L1 is faster!).", "notes_text": null, "keywords": ["cache size", "capacity tradeoff", "diminishing returns", "multi-level sizing"], "images": [{"type": "diagram", "description": "L1 and L2 cache hierarchy"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache size tradeoffs"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 61, "title": "Associativity (# Ways)", "summary": "Analyzes impact of associativity on cache performance.", "main_text": "Higher associativity = more ways (blocks per set): Reduces conflict misses (for a fixed cache size, we will have more blocks in each set, so we will be able to: use invalid blocks that would otherwise be in other sets, evict blocks used less recently - we evict in the set), Increases hit time (we need to search tags through more blocks), Increases HW cost of policies, Diminishing returns. Shows conflict example in Set 0 with 2-way associative cache.", "notes_text": null, "keywords": ["associativity", "ways", "conflict reduction", "search cost"], "images": [{"type": "diagram", "description": "Set-associative cache showing Set 0 full, requiring eviction"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["associativity tradeoffs"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 62, "title": "Other Strategies: Prefetching", "summary": "Introduces prefetching as cache optimization technique.", "main_text": "Prefetching: On miss of block i, fetch block i and i+1. Can be performed in: Software, with prefetch instructions inserted by the compiler. Hardware, when detecting a specific pattern (common).", "notes_text": null, "keywords": ["prefetching", "hardware prefetch", "software prefetch", "optimization"], "images": [{"type": "diagram", "description": "Main Memory blocks showing prefetch pattern"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["prefetching"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 63, "title": "Example: Intel i7 Haswell", "summary": "Real-world example of modern cache architecture.", "main_text": "Quad-core CPU: L1/L2 caches for each core (Separate L1 caches for data and program instruction), L3 cache shared by all cores, Same block size (64 B) on all levels, Levels with increasing cache size: 32 kB (L1), 256 kB (L2), 8 MB (L3), Higher associativity at L3: since miss penalty is larger, we want fewer conflict misses.", "notes_text": null, "keywords": ["Intel i7", "Haswell", "multi-core", "L1 L2 L3", "real architecture"], "images": [{"type": "diagram", "description": "Block diagram showing 4 cores, each with L1/L2, shared L3, and main memory"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["Intel i7 architecture"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 64, "title": "Experiment: Measuring Read Tput", "summary": "Introduces experimental measurement of cache read throughput with different stride patterns.", "main_text": "data is an array of long. Adding each item every stride items, over elems items in total. Access pattern examples: stride-1: d[0] d[1] d[2] d[3] d[4] ... d[15] using indices a0 a1 a2 a3 a0 a1 a2 a3 ... stride-2: every other element, stride-4: every fourth element. Demonstrates spatial locality effects on cache performance.", "notes_text": null, "keywords": ["cache measurement", "stride access", "read throughput", "spatial locality experiment"], "images": [{"type": "diagram", "description": "Array access patterns showing stride-1, stride-2, and stride-4 with corresponding index patterns"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache performance measurement"]}}