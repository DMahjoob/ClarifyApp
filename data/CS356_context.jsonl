{"deck_name":"CS356_Unit00_Intro","slide_number":1,"chunk_index":0,"title":"Unit 0: Introduction","summary":"Course introduction slide announcing Unit 0 and framing the course kickoff.","main_text":"Unit 0: Introduction. What this class is about!","notes_text":"Opening slide—course title and framing. No deep content; used for navigation and orientation.","keywords":["introduction","course","Unit 0","overview","CS356"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Course Introduction","importance_score":3,"file_hash":"<sha256 placeholder> :contentReference[oaicite:0]{index=0}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":2,"chunk_index":0,"title":"Logistics","summary":"Logistical slide listing general course logistics and expectations.","main_text":"Logistics. (Slide contains course logistics information — see class website and announcements for details.)","notes_text":"Short logistics placeholder slide; primary details are on subsequent slides.","keywords":["logistics","schedule","attendance","expectations","class"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Logistics","importance_score":3,"file_hash":"<sha256 placeholder> :contentReference[oaicite:1]{index=1}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":3,"chunk_index":0,"title":"Instructor — Mark Redekopp","summary":"Instructor contact and background information for Professor Mark Redekopp.","main_text":"Mark Redekopp — Professor of Electrical and Computer Engineering and Computer Science Practice. Undergrad and grad work at USC. Email: redekopp@usc.edu. Office Hours: See website (various times M-F).","notes_text":"Include instructor email and office hours link on course website. Useful for students needing contact info.","keywords":["instructor","Mark Redekopp","contact","office hours","USC"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Instructor","importance_score":4,"file_hash":"<sha256 placeholder> :contentReference[oaicite:2]{index=2}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":4,"chunk_index":0,"title":"Instructor — Marco Paolieri","summary":"Secondary instructor (or staff) contact and background for Marco Paolieri.","main_text":"Marco Paolieri. PhD, University of Florence, Italy (2015). Postdoc at USC, 2016–2018. Senior Research Associate (since 2018). Email: paolieri@usc.edu. Office Hours: 5-6pm Tue/Thu (see website).","notes_text":"TA / senior researcher contact info. Useful for lab questions and specialized assistance.","keywords":["instructor","Marco Paolieri","contact","office hours","PhD"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Instructor","importance_score":4,"file_hash":"<sha256 placeholder> :contentReference[oaicite:3]{index=3}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":5,"chunk_index":0,"title":"Class Organization","summary":"High-level schedule for lectures, discussions, exams, and assignments.","main_text":"Class Organization: Lectures on Tue/Thu (attendance highly recommended). Discussions on Fri (attend any session). Exams: MT1, MT2, Final (only in person) — 25 pts best MT, 15 worst MT, 40 Final. Assignments (20 pts total across labs: 3 + 3 + 3 + 5 + 6) including bitwise ops in C, reverse engineering in assembly, programming in C.","notes_text":"Gives grading breakdown and meeting cadence for the course.","keywords":["lectures","discussions","exams","assignments","grading"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Organization & Grading","importance_score":6,"file_hash":"<sha256 placeholder> :contentReference[oaicite:4]{index=4}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":6,"chunk_index":0,"title":"Class Website","summary":"Link to the course website for slides, assignments, and resources.","main_text":"Class Website: usc-cs356.github.io","notes_text":"Primary canonical resource for slides, links, and assignment instructions.","keywords":["website","usc-cs356","resources","slides","assignments"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Resources","importance_score":5,"file_hash":"<sha256 placeholder> :contentReference[oaicite:5]{index=5}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":7,"chunk_index":0,"title":"Office Hours","summary":"Information about office hours and how to join (Zoom links when remote).","main_text":"Office Hours. Click to see Zoom links (when online).","notes_text":"Students should check the class website for live links and schedules.","keywords":["office hours","Zoom","help","TA","instructor"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Office Hours","importance_score":4,"file_hash":"<sha256 placeholder> :contentReference[oaicite:6]{index=6}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":8,"chunk_index":0,"title":"Class Schedule and Slides","summary":"Notes on where to find the schedule and slide deck materials and access password.","main_text":"Class Schedule and Slides. Password: “trojan356”.","notes_text":"Password protects slide downloads; students should use the course website.","keywords":["schedule","slides","password","trojan356","access"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Schedule & Access","importance_score":4,"file_hash":"<sha256 placeholder> :contentReference[oaicite:7]{index=7}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":9,"chunk_index":0,"title":"Exam Schedule & Samples","summary":"Concrete dates and times for midterms and final; instruction to report conflicts ASAP.","main_text":"Ask ASAP if you have conflicts! MT1: Fri, Oct 3 (4-6pm). MT2: Fri, Nov 7 (4-6pm). Final: Sat, Dec 13 (4:30-6:30pm).","notes_text":"Students must notify instructors early about scheduling conflicts.","keywords":["exam","MT1","MT2","final","schedule"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Exams","importance_score":6,"file_hash":"<sha256 placeholder> :contentReference[oaicite:8]{index=8}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":10,"chunk_index":0,"title":"Resources","summary":"Primary textbook and online class resources are listed for students.","main_text":"Resources: Textbook — Computer Systems: A Programmer’s Perspective (Bryant & O’Hallaron, 2015). Piazza: piazza.com/usc/fall2025/csci356/home. Programming Server (shared) used for assignments: usc-cs356.github.io/assignments/hellolab.html (warm-up, not graded, due Tue 9/2).","notes_text":"Central reading and discussion platforms; server link provided for hands-on labs.","keywords":["textbook","Piazza","programming server","assignments","hellolab"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Resources & Textbook","importance_score":5,"file_hash":"<sha256 placeholder> :contentReference[oaicite:9]{index=9}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":11,"chunk_index":0,"title":"Assignments","summary":"Slide header linking to assignments; details appear on other slides.","main_text":"Assignments. (Slide acts as header for assignment details; see Assignment Repos and programming server links.)","notes_text":"Assignments include labs on bitwise ops, assembly reverse engineering, and programming in C.","keywords":["assignments","labs","bitwise","assembly","C"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Assignments","importance_score":5,"file_hash":"<sha256 placeholder> :contentReference[oaicite:10]{index=10}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":12,"chunk_index":0,"title":"Assignment Repos on GitHub","summary":"How assignment repositories are distributed and repository management rules.","main_text":"Assignment Repos on GitHub. A new repository is shared for each lab: github.com/usc-cs356-fall25/paolieri_hellolab. Invitation by email or via the repo invitations page. Push date is used to calculate late days (2 late-day tokens per assignment, 5 total across semester). Use ./grade to check your grade before submitting.","notes_text":"Important operational details for working with GitHub repos and submission timing.","keywords":["GitHub","repos","push date","late days","./grade"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Assignments & GitHub","importance_score":6,"file_hash":"<sha256 placeholder> :contentReference[oaicite:11]{index=11}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":13,"chunk_index":0,"title":"VS Code Remote","summary":"Mention of VS Code Remote environment support for course work.","main_text":"VS Code Remote. (Slide indicates use of remote development environment—details on website.)","notes_text":"Students may use VS Code remote development for labs; see the course site for setup.","keywords":["VS Code","remote","development","lab","environment"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Development Environment","importance_score":4,"file_hash":"<sha256 placeholder> :contentReference[oaicite:12]{index=12}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":14,"chunk_index":0,"title":"Policy to Get Help","summary":"Honor-code and help policy describing allowed and disallowed help sources.","main_text":"Policy to get help: Don’t discuss assignments with other students (not even high-level). Don’t search for help online or use AI tools (ChatGPT). Ask instructors or TAs on Piazza or in office hours—but don’t expect them to fix bugs or write code.","notes_text":"Strict integrity policy; emphasizes independent work and instructor-mediated help only.","keywords":["help policy","academic integrity","Piazza","office hours","no-ai"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Help Policy","importance_score":8,"file_hash":"<sha256 placeholder> :contentReference[oaicite:13]{index=13}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":15,"chunk_index":0,"title":"Academic Integrity","summary":"Consequences for academic dishonesty and sharing solutions.","main_text":"Academic Integrity: Students sharing or using solutions of others (including AI tools) will be reported to OAI to be assigned an “F”.","notes_text":"Clear consequences—important for course compliance and student conduct.","keywords":["integrity","plagiarism","AI prohibition","OAI","consequences"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Academic Integrity","importance_score":9,"file_hash":"<sha256 placeholder> :contentReference[oaicite:14]{index=14}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":16,"chunk_index":0,"title":"Motivation","summary":"High-level motivation explaining why system-level course topics matter.","main_text":"Motivation. (Slide introduces motivation for studying system-level details—see subsequent slides for specifics.)","notes_text":"Sets the stage for deeper technical topics in later units.","keywords":["motivation","systems","why study","context","overview"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Motivation","importance_score":4,"file_hash":"<sha256 placeholder> :contentReference[oaicite:15]{index=15}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":17,"chunk_index":0,"title":"CSCI 356 — Course Journey","summary":"Overview of core areas the course will cover, including memory, assembly, caches, and CPU internals.","main_text":"An exciting journey unveiling the mysteries of: Translation of C programs to assembly instructions; Use of main memory (RAM) as stack and heap through virtual memory; Design and use of memory caches; Processor organization, including pipelines and out-of-order execution.","notes_text":"High-level syllabus themes to motivate subsequent units.","keywords":["assembly","virtual memory","caches","pipelines","out-of-order"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Course Themes","importance_score":7,"file_hash":"<sha256 placeholder> :contentReference[oaicite:16]{index=16}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":18,"chunk_index":0,"title":"But why? (Abstractions vs Reality)","summary":"Contrasts programming abstractions with hardware reality and why students must understand underlying systems.","main_text":"But why? You take the blue pill... the story ends, you wake up at your laptop coding and believing: CPUs have Python’s “+” op; int/float are as in math; reading from any address takes the same time; instructions executed in the order you write them. CSCI 356 explores hardwareland and the rabbit hole of buffer overflows, caches, virtual memory.","notes_text":"A rhetorical slide motivating the need to look beneath high-level abstractions.","keywords":["abstractions","hardware","buffers","virtual memory","caches"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Motivation","importance_score":7,"file_hash":"<sha256 placeholder> :contentReference[oaicite:17]{index=17}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":19,"chunk_index":0,"title":"Objectives","summary":"Learning objectives covering programming issues (bugs, security, performance) and hardware issues (security and performance).","main_text":"Objectives: Understand programming issues — numerical errors/bugs (int, float), compilation (assembly, linking), security (buffer overflows), performance (poor use of caches, memory leaks). Understand hardware issues — security (Spectre/Meltdown), performance (branch misprediction).","notes_text":"Enumerates both software-level and hardware-level learning goals for the course.","keywords":["objectives","numerical errors","compilation","Spectre","performance"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Objectives","importance_score":8,"file_hash":"<sha256 placeholder> :contentReference[oaicite:18]{index=18}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":20,"chunk_index":0,"title":"Abstractions vs Reality","summary":"Statement that programming abstractions are useful but can hide important hardware details causing bugs or vulnerabilities.","main_text":"Abstractions vs Reality. Programming abstractions are good until reality hits. When ignored: bugs or security vulnerabilities. It is important to understand the underlying hardware: sometimes abstractions don’t provide the control, security, or performance you need. Let’s check a few examples…","notes_text":"Transition slide preparing several concrete examples (ints/floats, assembly, caches, architecture).","keywords":["abstractions","hardware","bugs","vulnerabilities","motivation"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Conceptual Framing","importance_score":7,"file_hash":"<sha256 placeholder> :contentReference[oaicite:19]{index=19}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":21,"chunk_index":0,"title":"Reality 1: int/float","summary":"Explains why integers and floating-point numbers differ from their mathematical counterparts and when assumptions fail.","main_text":"Reality 1: int/float — “ints are not integers and floats aren’t reals”. Is x*x >= 0? int: not always (overflow). float: yes (unless x is NaN). Is (x+y)+z == x+(y+z)? int: yes (even with overflow). float: not always, due to rounding or overflow.","notes_text":"Key takeaway: finite precision and representation limitations lead to surprising behavior; important for correctness and security.","keywords":["integers","overflow","floating-point","rounding","associativity"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Numeric Representations","importance_score":9,"file_hash":"<sha256 placeholder> :contentReference[oaicite:20]{index=20}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":22,"chunk_index":0,"title":"Example: Integer Overflow","summary":"C code and runtime output demonstrating integer overflow with small integer types.","main_text":"Example: Integer Overflow. C code: unsigned char sum(unsigned char x, unsigned char y) { return x + y; } signed char square(signed char x) { return x * x; } main prints sum(255,1), square(12), square(20). Output: 0 -112 -112. Explanation: finite number of bits; 255+1 creates carry dropped, showing wrap-around behavior.","notes_text":"Concrete example illustrating why operations on finite-width types can behave non-intuitively.","keywords":["integer overflow","wraparound","unsigned char","signed char","C example"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Integer Overflow","importance_score":9,"file_hash":"<sha256 placeholder> :contentReference[oaicite:21]{index=21}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":23,"chunk_index":0,"title":"Example: Float Rounding","summary":"A C example showing floating-point rounding and non-associativity of float addition.","main_text":"Example: Float Rounding. C code uses float x = 1000000.0f and checks x + 0.01f == x, and variants showing -x + (x + 0.01f) == 0.0f vs (-x + x) + 0.01f == 0.01f. Output: 1 1 1. Explanation: finite significant digits; 1,000,000 + 0.01 can equal 1,000,000 due to lack of precision. Float addition not associative; order of operations matters.","notes_text":"Demonstrates numerical analysis practices: add numbers of similar magnitude first to reduce rounding error.","keywords":["floating-point","rounding","precision","associativity","C example"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Floating-Point Behavior","importance_score":9,"file_hash":"<sha256 placeholder> :contentReference[oaicite:22]{index=22}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":24,"chunk_index":0,"title":"Reality 2: Assembly Matters","summary":"Explains why knowing assembly is critical for debugging, systems programming, and vulnerability analysis.","main_text":"Reality 2: Assembly Matters. Compilers are better than us at optimizing code, but knowing assembly is critical when: tracking down some bugs (including compiler bugs); implementing system software (OS/compilers) that uses hardware features; analyzing vulnerabilities of binary programs.","notes_text":"Motivates later units on x86-64 assembly and reverse engineering.","keywords":["assembly","compilers","systems","vulnerabilities","reverse engineering"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Assembly & Systems","importance_score":8,"file_hash":"<sha256 placeholder> :contentReference[oaicite:23]{index=23}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":25,"chunk_index":0,"title":"Example: Buffer Overflows","summary":"C code example that demonstrates how overflowing a small buffer can overwrite a return address and hijack control flow.","main_text":"Example: Buffer Overflows. Code: void unreachable() { printf(\"Impossible.\\n\"); } void hello() { char buffer[6]; scanf(\"%s\", buffer); printf(\"Hello, %s!\\n\", buffer); } int main() { hello(); return 0; } Compiled with -no-pie and run with crafted input, the buffer of 6 bytes can be overflowed to overwrite the return address with the address of unreachable, producing 'Hello, World!' and then 'Impossible.' This shows a security bug—understanding stack layout is necessary to prevent it.","notes_text":"This slide contains the canonical stack-corruption buffer overflow example used to motivate stack layout and exploit mitigation discussions.","keywords":["buffer overflow","stack","return address","scanf","exploit"],"images":[{"description":"Stack frame diagram showing layout before and after function call: local variables, return address, and caller frame. Diagram teaches how buffer overflow can overwrite return address.","labels":["buffer[6]","return address","local variables","stack frame"],"position":{"x":0.55,"y":0.15,"width":0.4,"height":0.6}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":0,"topic":"Buffer Overflows","importance_score":10,"file_hash":"<sha256 placeholder> :contentReference[oaicite:24]{index=24}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":26,"chunk_index":0,"title":"Runtime Stack","summary":"Visual depiction and explanation of the runtime stack and how function calls allocate return addresses and local variables.","main_text":"Runtime Stack. After we call a function, a 'return address' and its local variables are allocated on the runtime stack. The slide shows repeated stacked frames: main()'s locals, address of 'return 0', char buffer[6], and other local variables, illustrating push/pop behavior as functions are called/return.","notes_text":"This diagram supports understanding of buffer overflow mechanics and frame layout used in later security units.","keywords":["runtime stack","stack frame","local variables","return address","call/return"],"images":[{"description":"Stack frames drawn vertically showing caller and callee regions: main() locals, pushed return address, callee local buffer[6], further frames for scanf and printf. Educates about memory layout during calls.","labels":["main() locals","return addr","buffer[6]","scanf","printf"],"position":{"x":0.48,"y":0.12,"width":0.45,"height":0.72}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":0,"topic":"Runtime Stack","importance_score":10,"file_hash":"<sha256 placeholder> :contentReference[oaicite:25]{index=25}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":27,"chunk_index":0,"title":"Buffer Overflows in the Wild","summary":"Real-world reference to a high-profile buffer overflow vulnerability and exploit writeup.","main_text":"Buffer Overflows in the Wild: Quoted text — 'Unfortunately, it's the same old story. A fairly trivial buffer overflow programming error in C++ code in the kernel parsing untrusted data, exposed to remote attackers.' Reference: Google Project Zero writeup (iOS zero-click vulnerability).","notes_text":"Provides a real-world security incident reference to motivate secure coding and system-level understanding.","keywords":["real-world","vulnerability","Google Project Zero","iOS","kernel"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Security Incidents","importance_score":8,"file_hash":"<sha256 placeholder> :contentReference[oaicite:26]{index=26}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":28,"chunk_index":0,"title":"Reality 3: There is more than Big O","summary":"Explains why asymptotic complexity is insufficient; constant factors and microarchitectural behavior matter to performance.","main_text":"Reality 3: There is more than Big O. There’s more to performance than asymptotic bounds or operation counts. Constant factors matter—O(n) could be 10*n or 350*n. Even exact operation counts are not enough: the same memory read instruction can take very different amounts of time depending on prior instructions and microarchitectural state.","notes_text":"This motivates learning about caches, memory hierarchy, and CPU microarchitecture for real performance engineering.","keywords":["performance","Big O","constant factors","microarchitecture","memory latency"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Performance Considerations","importance_score":8,"file_hash":"<sha256 placeholder> :contentReference[oaicite:27]{index=27}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":29,"chunk_index":0,"title":"Example: Caches","summary":"Introduces CPU caches and explains the performance benefit of accessing cache-resident data.","main_text":"Example: Caches. No need to access main memory if data is in CPU cache — huge saving (1ns vs 100ns). Caches read entire 'blocks'. How can we take advantage?","notes_text":"Leads to next slide describing cache-friendly code patterns and memory layout effects.","keywords":["cache","cache block","latency","memory hierarchy","performance"],"images":[{"description":"Illustration or block graphic showing a memory block/line and cache concept—teaches that caches read blocks and locality matters.","labels":["block","cache line","cache"],"position":{"x":0.6,"y":0.2,"width":0.35,"height":0.5}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":0,"topic":"Caches","importance_score":9,"file_hash":"<sha256 placeholder> :contentReference[oaicite:28]{index=28}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":30,"chunk_index":0,"title":"Cache-Friendly Code","summary":"Shows how loop ordering affects memory access patterns and cache performance for a 2D array.","main_text":"Cache-Friendly Code. When N is large: for (i=0; i<N; i++) for (j=0; j<N; j++) access(array[i][j]); is much faster than swapping the loops (j outer, i inner) because array[N][N] is stored row-major: 0,0 0,1 0,2 1,0 1,1 1,2 ... Access patterns that follow memory layout exploit spatial locality.","notes_text":"Teaching point: prefer loop orders that iterate contiguous memory first to exploit cache lines.","keywords":["cache-friendly","row-major","loop order","locality","spatial locality"],"images":[{"description":"Small 3x3 grid visual that maps array indices to linear memory layout to teach row-major ordering.","labels":["0,0","0,1","1,0","memory layout","row-major"],"position":{"x":0.55,"y":0.2,"width":0.4,"height":0.5}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":0,"topic":"Cache-Friendly Programming","importance_score":9,"file_hash":"<sha256 placeholder> :contentReference[oaicite:29]{index=29}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":31,"chunk_index":0,"title":"Caches & Memory Wall","summary":"References the growing processor-memory performance gap and cites Hennessy & Patterson.","main_text":"Caches & Memory Wall. Reference: Hennessy and Patterson, Computer Architecture — A Quantitative Approach (2003). Processor-Memory Performance Gap: CPU performance improvements (~7%/year) vs memory (~55%/year) — caches matter!","notes_text":"Establishes historical context for why caches are central to performance engineering.","keywords":["memory wall","Hennessy","Patterson","performance gap","caches"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Memory Wall & Caches","importance_score":8,"file_hash":"<sha256 placeholder> :contentReference[oaicite:30]{index=30}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":32,"chunk_index":0,"title":"Reality 4: Architecture Matters","summary":"Explains that transistor scaling alone isn't enough—computer architecture choices enable effective use of transistor budgets.","main_text":"Reality 4: Architecture Matters. Moore’s Law: transistor counts grow exponentially. Computer architecture: putting them to work. Given many transistors, how do we produce equivalent increases in computational ability? Options include more on-chip cache memory, complex cores executing instructions in parallel, and multi-core CPUs/GPUs requiring parallel programming.","notes_text":"Sets up later units on CPU architecture, multicore, and GPU programming.","keywords":["architecture","Moore's Law","on-chip cache","multi-core","GPU"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Computer Architecture","importance_score":8,"file_hash":"<sha256 placeholder> :contentReference[oaicite:31]{index=31}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":33,"chunk_index":0,"title":"(Empty/Visual Slide Placeholder)","summary":"Slide contains imagery or a visual used in lecture; text omitted in PDF snippet.","main_text":"(Visual/diagram slide—content not transcribed in PDF text snippet.)","notes_text":"Placeholder: slide likely contained images/diagrams; no extractable text in PDF preview.","keywords":["visual","diagram","placeholder","image-only"],"images":[],"layout":{"num_text_boxes":0,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":0,"topic":"Visual Illustration","importance_score":3,"file_hash":"<sha256 placeholder> :contentReference[oaicite:32]{index=32}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":34,"chunk_index":0,"title":"(Empty/Visual Slide Placeholder 2)","summary":"Another slide likely containing images or timeline visuals; no transcribed text available in PDF extract.","main_text":"(Visual/diagram slide—content not transcribed in PDF snippet.)","notes_text":"Placeholder for a visual element from the lecture slides.","keywords":["visual","diagram","timeline","placeholder"],"images":[],"layout":{"num_text_boxes":0,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":0,"topic":"Visual Illustration","importance_score":3,"file_hash":"<sha256 placeholder> :contentReference[oaicite:33]{index=33}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":35,"chunk_index":0,"title":"Single-Core CPU: Pentium 4 (2000)","summary":"Simple block diagram labeling L1 instruction/data caches and L2 cache for a single-core CPU (Pentium 4 example).","main_text":"Single-Core CPU: Pentium 4 (2000). Diagram labels L2 Cache, L1 Data, L1 Instruction. (Slide illustrates cache hierarchy and relationship to core.)","notes_text":"Useful for comparing multi-level caches and later evolution to multi-core designs.","keywords":["Pentium 4","L1","L2","cache hierarchy","single-core"],"images":[{"description":"Block diagram of a single-core CPU showing L1 instruction cache, L1 data cache, and L2 cache to teach the cache hierarchy for a single core.","labels":["L2 Cache","L1 Data","L1 Instr."],"position":{"x":0.35,"y":0.2,"width":0.6,"height":0.6}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":0,"topic":"Cache Hierarchy","importance_score":7,"file_hash":"<sha256 placeholder> :contentReference[oaicite:34]{index=34}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":36,"chunk_index":0,"title":"Instruction-Level Parallelism","summary":"Explains pipelining and out-of-order execution as ways to execute multiple instructions concurrently.","main_text":"Instruction-Level Parallelism. Pipeline: work on many instructions at the same time, like an assembly line where the input is the sequence of assembly instructions of our program. Out-of-Order Execution: execute instructions of a program in a different order; independent instructions can use CPU resources while others are stuck.","notes_text":"Introduces CPU performance techniques that will be revisited in CPU architecture units.","keywords":["ILP","pipeline","out-of-order","parallelism","execution"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"ILP & Pipelines","importance_score":8,"file_hash":"<sha256 placeholder> :contentReference[oaicite:35]{index=35}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":37,"chunk_index":0,"title":"(Empty/Technical Slide)","summary":"Slide with supporting technical visuals or charts that did not extract as text in the PDF preview.","main_text":"(Technical visual content—no extractable text in PDF snippet.)","notes_text":"Likely contained a small chart or figure related to pipeline performance or power; not transcribed.","keywords":["visual","chart","pipeline","power","placeholder"],"images":[],"layout":{"num_text_boxes":0,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":0,"topic":"Technical Visual","importance_score":4,"file_hash":"<sha256 placeholder> :contentReference[oaicite:36]{index=36}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":38,"chunk_index":0,"title":"Power and Frequency Limits","summary":"Explains power constraints (Power ∝ Capacitance × V² × Frequency) and why frequency scaling slowed after early 2000s.","main_text":"Power ∝ (Capacitive Load × V^2 × Frequency). After 2003, could not reduce capacitive load and voltage to increase frequency under same power consumption; higher power led to dissipation issues. This constrained CPU frequency scaling and drove architectural changes.","notes_text":"Context for why multi-core designs and complexity replaced simple frequency scaling as the primary performance strategy.","keywords":["power","frequency","V^2","capacitive load","frequency scaling"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Power & Frequency","importance_score":7,"file_hash":"<sha256 placeholder> :contentReference[oaicite:37]{index=37}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":39,"chunk_index":0,"title":"42 Years of Microprocessors (Timeline)","summary":"Visual timeline of microprocessor evolution over 42 years; slide likely contains historical die maps and chip photos.","main_text":"42 Years of Microprocessors. (Timeline/visual showing evolution of processors across decades.)","notes_text":"Historical context slide; visuals not transcribed into PDF text snippet.","keywords":["history","microprocessors","timeline","evolution","processors"],"images":[],"layout":{"num_text_boxes":0,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":0,"topic":"History of Microprocessors","importance_score":4,"file_hash":"<sha256 placeholder> :contentReference[oaicite:38]{index=38}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":40,"chunk_index":0,"title":"Quad-Core CPU: Nehalem (2008)","summary":"Slide indicating a quad-core Nehalem die/architecture as an example of multi-core evolution.","main_text":"Quad-Core CPU: Nehalem (2008). (Slide likely contains die photo and labels showing cores and caches.)","notes_text":"Used to illustrate early multi-core architectures and on-chip cache increases.","keywords":["Nehalem","quad-core","2008","multi-core","die"],"images":[{"description":"Die photo or schematic of Intel Nehalem quad-core CPU showing cores and cache regions. Educates on multi-core chip layouts.","labels":["quad-core","cache","cores","Nehalem"],"position":{"x":0.45,"y":0.15,"width":0.5,"height":0.7}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":0,"topic":"Multi-core Evolution","importance_score":6,"file_hash":"<sha256 placeholder> :contentReference[oaicite:39]{index=39}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":41,"chunk_index":0,"title":"Quad-Core CPU: Haswell (2013)","summary":"Haswell die example showing architectural improvements and cache arrangements.","main_text":"Quad-Core CPU: Haswell (2013). (Visual/die map illustrating improvements over previous microarchitectures.)","notes_text":"Comparison point showing architectural progression and cache/core organization changes across generations.","keywords":["Haswell","2013","quad-core","die map","architecture"],"images":[{"description":"Haswell die/architecture image used to highlight architecture evolution and cache/core placement.","labels":["Haswell","cores","cache"],"position":{"x":0.45,"y":0.15,"width":0.5,"height":0.7}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":0,"topic":"Multi-core Evolution","importance_score":6,"file_hash":"<sha256 placeholder> :contentReference[oaicite:40]{index=40}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":41,"chunk_index":1,"title":"Hexa-Core CPU: Coffee Lake (2017)","summary":"Coffee Lake die map example (hexa-core) to show core scaling and die complexity in 2017.","main_text":"Hexa-Core CPU: Coffee Lake (2017). (Slide references a die map image to illustrate a hexa-core layout and increasing die complexity.)","notes_text":"Die map reference link provided in slide notes (researchgate).","keywords":["Coffee Lake","2017","hexa-core","die map","Intel"],"images":[{"description":"Die map of Coffee Lake showing six cores and shared cache; teaches core-count scaling and die complexity.","labels":["Coffee Lake","hexa-core","die map"],"position":{"x":0.45,"y":0.15,"width":0.5,"height":0.7}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":0,"topic":"Multi-core Evolution","importance_score":6,"file_hash":"<sha256 placeholder> :contentReference[oaicite:41]{index=41}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":42,"chunk_index":0,"title":"Octa-Core CPU: Ryzen 5000 (2020)","summary":"Example of AMD Ryzen 5000 octa-core architecture as a modern multi-core example (2020).","main_text":"Octa-Core CPU: Ryzen 5000 (2020). (Slide illustrates a modern octa-core architecture showing multiple cores and cache structures.)","notes_text":"Shows trend of increasing core counts and on-chip resources to improve throughput.","keywords":["Ryzen 5000","2020","octa-core","AMD","multi-core"],"images":[{"description":"Schematic or die photo of Ryzen 5000 showing eight cores and cache hierarchy; used to teach contemporary multicore designs.","labels":["Ryzen 5000","octa-core","cores","cache"],"position":{"x":0.45,"y":0.15,"width":0.5,"height":0.7}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":0,"topic":"Multi-core Evolution","importance_score":6,"file_hash":"<sha256 placeholder> "}}
{"deck_name":"CS356_Unit00_Intro","slide_number":43,"chunk_index":0,"title":"Deca-core CPU: Apple M4 (2024)","summary":"Apple M4 cited as a 10-core example (2024) illustrating continued core-count growth in modern processors.","main_text":"Deca-core CPU; Apple M4 (2024). (Slide mentions a deca-core Apple M4 chip as an example of modern high-core-count SoCs.)","notes_text":"Example used to highlight that SoC designs now include many cores and specialized accelerators.","keywords":["Apple M4","2024","deca-core","SoC","cores"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Contemporary SoCs","importance_score":5,"file_hash":"<sha256 placeholder> :contentReference[oaicite:43]{index=43}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":44,"chunk_index":0,"title":"Thread-Level Parallelism","summary":"Explains thread-level parallelism and challenges like synchronization and cache coherence across cores.","main_text":"Thread-Level Parallelism. A multicore CPU can run multiple threads in parallel. How can we split work into parallel tasks? Easy for independent programs/server requests; harder when tasks interact and need synchronization primitives (hardware locks). Each core has its own cache—how to let other cores know about local changes? (cache coherence).","notes_text":"Sets the stage for concurrency, synchronization, and cache coherence topics in later units.","keywords":["thread-level parallelism","synchronization","cache coherence","hardware locks","multicore"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Parallelism & Coherence","importance_score":8,"file_hash":"<sha256 placeholder> :contentReference[oaicite:44]{index=44}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":45,"chunk_index":0,"title":"GPU: Nvidia H100 (2023)","summary":"High-level specs for the Nvidia H100 GPU and emphasis that special programming is required to exploit GPU cores.","main_text":"GPU: Nvidia H100 (2023). 144 Streaming Multiprocessors, each including 128 FP32 cores (18,432 FP32 cores total). 48 peak FP32 TFLOPS, ~80 billion transistors (4 nm), 350 W. We cannot exploit these cores unless we write parallel programs designed for GPU architectures! (See CSCI 451, Parallel and Distributed Computation).","notes_text":"Illustrates extreme parallelism available in GPUs and motivates specialized programming models (CUDA, OpenCL).","keywords":["GPU","Nvidia H100","SMs","FP32","parallel programming"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"GPU Architecture","importance_score":7,"file_hash":"<sha256 placeholder> :contentReference[oaicite:45]{index=45}"}}
{"deck_name":"CS356_Unit00_Intro","slide_number":46,"chunk_index":0,"title":"Course Outline","summary":"Complete unit-by-unit course outline listing primary topics taught across the semester.","main_text":"Course Outline: Encoding of Integers/Floats/Strings (Units 1-3); x86-64 Assembly (Units 4-7); Buffer Overflow Attacks and ARM (Units 8-9); Caches & Cache-conscious Programming (Unit 10); Virtual Memory (Unit 11); Heap Management & Garbage Collection (Unit 12); Compilation & Linking, VMs & JIT (Unit 13); CPU Architecture, Cache Coherence (Units 14-16).","notes_text":"Acts as the semester roadmap — useful for students to identify when unit topics will be covered.","keywords":["course outline","virtual memory","caches","assembly","heap management"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":0,"topic":"Syllabus & Roadmap","importance_score":9,"file_hash":"<sha256 placeholder> "}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 1, "chunk_index": 0, "title": "Unit 1: Integers — Cover", "summary": "Introductory slide for Unit 1: integers and modulo arithmetic; establishes the unit name and focus.", "main_text": "Unit 1: Integers — And modulo arithmetic! This unit introduces binary data representation, integer types in C, signed vs unsigned, 2's complement, overflow, base conversions, and practical consequences for programming.", "notes_text": "Cover slide. Use as top-level anchor for Unit 1 content.", "keywords": ["integers", "binary", "modulo", "2's complement", "representation", "CS356"], "images": [{"description": "Title text 'Unit 1: Integers' and subtitle 'And modulo arithmetic!'. No informative diagram.", "labels": ["Unit 1", "Integers", "Modulo arithmetic"], "position": {"x": 0.05, "y": 0.05, "width": 0.9, "height": 0.9}}], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Unit 1: Integers — Cover", "importance_score": 3, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 2, "chunk_index": 0, "title": "Binary Data: bits and bytes", "summary": "Explains basic binary building blocks: bits and bytes and typical C variable sizes.", "main_text": "Computers use binary data. 1 bit = 0 or 1. 1 byte = sequence of 8 bits (256 combinations). C variables commonly use 1, 2, 4, 8 bytes (8, 16, 32, 64 bits). With n bits there are 2^n combinations. Question posed: With a sequence of bits such as 01000001, what is its interpreted value?", "notes_text": "Introduce idea that bits alone are ambiguous — need representation context to assign meaning.", "keywords": ["bit", "byte", "2^n", "C types", "binary", "01000001", "representation"], "images": [{"description": "Small illustrative binary sequence '01000001' shown as an example to ask about interpretation.", "labels": ["01000001"], "position": {"x": 0.1, "y": 0.25, "width": 0.5, "height": 0.2}}], "layout": {"num_text_boxes": 1, "num_images": 1, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Binary Data: bits and bytes", "importance_score": 6, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 3, "chunk_index": 0, "title": "Interpreting Binary Data: bits + context", "summary": "Shows that information equals bits plus representation system; same bits can mean different values under different systems.", "main_text": "To understand the value of a binary sequence we need a representation system. Information = Bits + Context (representation system). Example: 01000001 interpreted as ASCII is decimal 65 ('A'). The same sequence could also represent an unsigned integer or an instruction like 'inc %ecx' in x86 assembly.", "notes_text": "Emphasize that raw bits are ambiguous: the 'pair of glasses' (context) determines meaning.", "keywords": ["information", "representation system", "ASCII", "unsigned", "assembly", "interpretation"], "images": [{"description": "Three-column example mapping of the same bits to Unsigned, ASCII, and x86 instruction interpretations.", "labels": ["Unsigned Binary", "ASCII", "x86 Assembly", "01000001", "65", "'A'", "inc %ecx"], "position": {"x": 0.05, "y": 0.15, "width": 0.9, "height": 0.7}}], "layout": {"num_text_boxes": 1, "num_images": 1, "dominant_visual_type": "mixed"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Interpreting Binary Data: bits + context", "importance_score": 8, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 4, "chunk_index": 0, "title": "Binary representation systems overview", "summary": "Lists common binary representation systems used in computing: unsigned, signed variations, floating point, and text encodings.", "main_text": "Common representation systems:\n• Unsigned integers: plain binary interpreted as non-negative values.\n• Signed integers: 2's complement (primary), 1's complement, excess-N, signed magnitude.\n• Floating-point numbers: IEEE 754 standard.\n• Strings: ASCII, ISO-8859-1, UTF-8, UTF-16.", "notes_text": "Use this slide as taxonomy for later slides that deep-dive into unsigned vs signed vs floating and textual encodings.", "keywords": ["unsigned", "signed", "2's complement", "IEEE 754", "ASCII", "UTF-8", "representation systems"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Binary representation systems overview", "importance_score": 7, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 5, "chunk_index": 0, "title": "Unsigned integers (concept)", "summary": "Introduces unsigned integers and the base-2 system as the foundation for non-negative integer representation.", "main_text": "Unsigned integers represent non-negative values by interpreting bit positions as powers of two. An n-bit unsigned integer ranges from 0 to 2^n - 1. The slide sets up subsequent base explanations and examples.", "notes_text": "Prepare students to see base-2 and base-16 concrete conversions next.", "keywords": ["unsigned integers", "base-2", "2^n", "range", "non-negative"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Unsigned integers (concept)", "importance_score": 7, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 6, "chunk_index": 0, "title": "Unsigned integers: Base 10 refresher", "summary": "Explains positional notation in base 10 as a stepping stone to understand binary positional notation.", "main_text": "Positional number systems: the value of digits depends on place value powers of the radix r. Example: (934)_10 = 9*10^2 + 3*10^1 + 4*10^0. This slide reinforces the general positional concept before moving to binary.", "notes_text": "Useful to explicitly compare decimal to binary positional semantics.", "keywords": ["positional", "base-10", "radix", "place value", "MSD", "LSD"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Unsigned integers: Base 10 refresher", "importance_score": 5, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 7, "chunk_index": 0, "title": "Positional number systems (general)", "summary": "Formalizes positional notation formula and introduces MSD/LSD terminology.", "main_text": "A positional number (a3 a2 a1 a0 . a-1 a-2)_r evaluates to sum(ai * r^i) over indexes. Left-most digit = Most Significant Digit (MSD); right-most = Least Significant Digit (LSD). Digits range from 0 to r-1. This mathematical definition underpins binary and hex representations.", "notes_text": "Keep the formula handy when converting between bases.", "keywords": ["MSD", "LSD", "place value", "radix", "positional notation"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Positional number systems (general)", "importance_score": 6, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 8, "chunk_index": 0, "title": "Unsigned integers: Base 2 (binary)", "summary": "Defines binary positional values and gives examples for integer and fractional binary numbers.", "main_text": "Binary (base 2): digits 0 or 1; place values are powers of two. Example: (1011)_2 = 1*2^3 + 0*2^2 + 1*2^1 + 1*2^0 = 11 decimal. Fractional binary uses negative powers: (1.11)_2 = 1 + 1/2 + 1/4 = 1.75 decimal.", "notes_text": "Emphasize how fractional binary differs and why repeated fractions can produce repeating representations.", "keywords": ["binary", "base-2", "LSB", "MSB", "fractional binary"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Unsigned integers: Base 2 (binary)", "importance_score": 8, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 9, "chunk_index": 0, "title": "Unsigned integers: Base 16 (hexadecimal)", "summary": "Introduces hexadecimal notation and its relationship to binary (4 bits per hex digit).", "main_text": "Hexadecimal digits 0-F map to values 0-15. Example: (1A5)_16 = 1*16^2 + 10*16^1 + 5 = 421 decimal. Hex fractional example (AC.1)_16 equals 160 + 12 + 1/16 = 172.0625 decimal. Important conversion rule: 4 binary bits correspond to one hex digit.", "notes_text": "Hex is a compact human-friendly representation of binary for debugging and memory dumps.", "keywords": ["hex", "hexadecimal", "base-16", "binary-hex mapping", "4-bit", "0x notation"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Unsigned integers: Base 16 (hexadecimal)", "importance_score": 7, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 10, "chunk_index": 0, "title": "Hex — Binary — Decimal mapping quick table", "summary": "Provides a quick reference mapping of hex digits to 4-bit binary and decimal values.", "main_text": "Table mapping 0..F to binary nibbles and decimal values. Use this table to convert between hex, binary, and decimal easily. Examples: 101001110.1100_2 -> 14E.C_16; 1101011.10100_2 -> 6B.A_16.", "notes_text": "This mapping enables fast manual conversions and is often memorized for low-level programming.", "keywords": ["hex table", "nibble", "conversion", "binary", "decimal", "reference"], "images": [{"description": "A small conversion table mapping hex digits 0-F to binary 4-bit patterns and decimal equivalents.", "labels": ["0", "0000", "0", "F", "1111", "15"], "position": {"x": 0.05, "y": 0.2, "width": 0.6, "height": 0.6}}], "layout": {"num_text_boxes": 1, "num_images": 1, "dominant_visual_type": "mixed"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Hex — Binary — Decimal mapping quick table", "importance_score": 6, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 11, "chunk_index": 0, "title": "Base 2 to base 10 method", "summary": "Explains adding powers of two where bits are 1 to convert binary to decimal.", "main_text": "To convert binary to decimal, add the powers of two corresponding to set bits. Example place values: 2^0=1, 2^1=2, 2^2=4, 2^3=8, ... Continue as needed. Use this to interpret binary strings into decimal integers.", "notes_text": "Encourage students to underline set bits and sum the corresponding powers for clarity.", "keywords": ["binary to decimal", "powers of two", "conversion algorithm", "place values"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Base 2 to base 10 method", "importance_score": 5, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 12, "chunk_index": 0, "title": "Conversion examples and practice", "summary": "Shows worked examples converting bit patterns to decimal values, including fractions.", "main_text": "Examples: binary rows with place values yielding examples such as 133, 255, 7, 0.875 in decimal. These illustrate the method and fractional binary interpretations using 1/2, 1/4, 1/8 place values.", "notes_text": "Students should practice converting both integer and fractional binary numbers by hand to build intuition.", "keywords": ["examples", "practice", "fractional binary", "place values"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Conversion examples and practice", "importance_score": 4, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 13, "chunk_index": 0, "title": "Base 10 to Base 2 (algorithm)", "summary": "Describes greedy selection of powers of two or the division-remainder algorithm to convert decimal to binary.", "main_text": "Two methods:\n1) Greedy: choose the largest power of two ≤ number, mark 1, subtract, repeat.\n2) Division-remainder: repeatedly divide by 2 and record remainders (right-to-left). Example: 25_10 -> 11001_2 via place-value selection; 87_10 -> (1010111)_2 via repeated division remainders.", "notes_text": "Division-remainder is recommended for fractional conversion when done left-to-right with multiplications.", "keywords": ["decimal to binary", "division-remainder", "greedy algorithm", "conversion"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Base 10 to Base 2 (algorithm)", "importance_score": 6, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 14, "chunk_index": 0, "title": "Converting fractions (base 10 to base 2)", "summary": "Explains left-to-right multiply-by-2 method to get fractional binary digits.", "main_text": "To convert fractional decimal part to binary: multiply the fraction by 2, take the integer part as next binary digit, keep fractional remainder and repeat. Example: 0.1_10 -> 0.000110011..._2 (repeating). This demonstrates repeating binary expansions for many decimal fractions.", "notes_text": "Useful to explain why 0.1 cannot be represented exactly in binary floating-point.", "keywords": ["fraction conversion", "multiply by 2", "repeating binary", "0.1 problem"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Converting fractions (base 10 to base 2)", "importance_score": 8, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 15, "chunk_index": 0, "title": "Base 10 to Base 16", "summary": "States that the same conversion algorithms extend to other bases such as base 16.", "main_text": "Conversion techniques generalize to other bases. Example: 75_10 = 4B_16 (use left-to-right selection or divide by 16). The slide reinforces that methods are base-agnostic.", "notes_text": "Remind students of hex convenience when grouping binary by nibbles.", "keywords": ["decimal to hex", "base-16 conversion", "4B_16"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Base 10 to Base 16", "importance_score": 4, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 18, "chunk_index": 0, "title": "Binary arithmetic (overview)", "summary": "Notes that binary arithmetic uses same algorithms as decimal with base-specific carries and borrows.", "main_text": "Binary addition, subtraction, multiplication, and division operate with the same algorithms as decimal, but carries occur when sums reach 2 rather than 10. Borrowing uses base-2. The slide includes small addition and subtraction examples in binary.", "notes_text": "Point out that hardware uses these columnar methods implemented in logic circuits.", "keywords": ["binary arithmetic", "carry", "borrow", "addition", "subtraction"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Binary arithmetic (overview)", "importance_score": 6, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 19, "chunk_index": 0, "title": "Finite range of integers", "summary": "Explains that with n bits there are only 2^n possible values and gives examples like room numbering.", "main_text": "With n bits we can represent 2^n distinct values, numbered from 0..2^n-1 for unsigned integers. Examples: 3-digit room numbers vs 4-digit — similarly binary with n digits yields 2^n rooms. This motivates overflow and modular arithmetic.", "notes_text": "Use visual analogies (hotel room numbers) to ground the finite nature of bit fields.", "keywords": ["2^n", "finite range", "overflow", "modular arithmetic"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Finite range of integers", "importance_score": 7, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 20, "chunk_index": 0, "title": "Unsigned integer types in C (x86-64)", "summary": "Table of common unsigned integer C types and sizes (uint8_t, uint16_t, uint32_t, uint64_t) and ranges.", "main_text": "Unsigned types on x86-64: unsigned char / uint8_t (1 byte, 8 bits, 0..255), unsigned short / uint16_t (2 bytes, 0..65535), unsigned int / uint32_t (4 bytes, 0..2^32-1), unsigned long / uint64_t (8 bytes, 0..2^64-1). Pro tip: approximate powers of two using 1k=2^10≈10^3 etc.", "notes_text": "Mention differences on 32-bit architectures where long and pointers may be 32-bit.", "keywords": ["uint8_t", "uint16_t", "uint32_t", "uint64_t", "C types", "x86-64"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Unsigned integer types in C (x86-64)", "importance_score": 8, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 21, "chunk_index": 0, "title": "Unsigned overflow behavior (wraparound)", "summary": "Demonstrates wraparound: adding 1 to max unsigned value yields zero; subtraction wraps similarly (modulo arithmetic).", "main_text": "Unsigned overflow wraps modulo 2^n. Example: for 8-bit unsigned char, range is 0..255. 11111111 + 1 = 00000000, and 00000000 - 1 = 11111111. Addition and subtraction are performed modulo 256 in this case. Detect addition overflow by checking if x+y < x.", "notes_text": "Explain why many low-level algorithms rely on modulo arithmetic properties.", "keywords": ["unsigned overflow", "wraparound", "modulo", "detection", "x+y<x"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Unsigned overflow behavior (wraparound)", "importance_score": 9, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 23, "chunk_index": 0, "title": "Signed integers (concept)", "summary": "Introduces signed integer ranges and that MSB commonly indicates sign.", "main_text": "Signed integer systems split the representable combinations into positive (including zero) and negative halves. Typical range: -2^{n-1} .. 2^{n-1}-1. Convention: MSB=1 indicates negative, MSB=0 indicates non-negative. Several encoding systems exist; 2's complement is the common standard.", "notes_text": "Set up next slides which dive into 2's complement math and advantages.", "keywords": ["signed integers", "MSB", "range", "negative", "2's complement"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Signed integers (concept)", "importance_score": 9, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 26, "chunk_index": 0, "title": "Two's complement: definition (numeric place-value view)", "summary": "Defines 2's complement by treating the MSB place value as negative, producing range -2^{n-1}..2^{n-1}-1.", "main_text": "2's complement assigns the MSB a place value of -2^{n-1} rather than +2^{n-1}. For n bits this yields range -2^{n-1} to 2^{n-1}-1. E.g., 4-bit 2's complement ranges -8..+7. Positive numbers share same bit patterns as unsigned representation.", "notes_text": "Emphasize that this representation enables simple hardware for addition/subtraction since signed arithmetic uses same bitwise addition rules.", "keywords": ["2's complement", "MSB negative", "range", "place value"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Two's complement: definition (numeric place-value view)", "importance_score": 10, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 35, "chunk_index": 0, "title": "Two's complement: algorithmic definition", "summary": "Gives algorithm to compute -x by bitwise inversion (1's complement) plus 1, highlighting modulo arithmetic.", "main_text": "Given x in n bits, obtain -x by flipping all bits (1's complement) and adding 1, dropping any final carry. Example: 2 (0010) -> flip -> 1101 -> add 1 -> 1110 which encodes -2 in 4-bit two's complement. This demonstrates subtraction via addition: A - B = A + (~B + 1).", "notes_text": "Tie back to machine-level implementation: subtraction implemented by adders using two's complement transformation of subtrahend.", "keywords": ["negation", "bit flip", "~B+1", "subtraction", "modulo arithmetic"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Two's complement: algorithmic definition", "importance_score": 10, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 41, "chunk_index": 0, "title": "Two's complement arithmetic: addition & subtraction", "summary": "States rules: add as usual, drop final carry; subtraction convert to addition of two's complement.", "main_text": "Addition: add columns right-to-left; signs do not require special cases — add and drop final carry-out. Subtraction: convert A-B into A + (-B) = A + (~B + 1) and drop carry-out. These behaviors are the 'secret' of modulo arithmetic and are used in homework (DataLab) and CS:APP sections.", "notes_text": "Reinforce that the same hardware adder can handle signed and unsigned addition if drop/carry is managed appropriately.", "keywords": ["two's complement arithmetic", "adders", "drop carry", "A + ~B + 1", "CS:APP"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Two's complement arithmetic: addition & subtraction", "importance_score": 10, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 45, "chunk_index": 0, "title": "Overflow detection for addition and subtraction", "summary": "Describes how to detect overflow for unsigned and signed arithmetic and gives example conditions.", "main_text": "Overflow occurs when result cannot be represented with given bits. Unsigned addition overflow: x+y < x. Signed overflow: when adding two positives yields negative or two negatives yield positive (sign mismatch). For subtraction, similar tests apply by treating as addition with negated operand.", "notes_text": "Provide examples and encourage students to implement overflow checks in small routines.", "keywords": ["overflow detection", "signed overflow", "unsigned overflow", "x+y<x", "sign test"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Overflow detection for addition and subtraction", "importance_score": 9, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 49, "chunk_index": 0, "title": "Casting between integer types: extension and truncation", "summary": "Explains bit-preserving casts when same size, sign/zero extension when increasing size, and truncation when decreasing size.", "main_text": "Casting between types of same size preserves bit pattern but changes interpretation (e.g., char -1 -> unsigned char 255). To larger sizes use zero extension for unsigned types and sign extension for signed (replicate MSB). To smaller sizes truncation drops high-order bits; careful not to remove all sign bits. Examples in C demonstrate outputs.", "notes_text": "Students should trace extension/truncation examples to understand surprising printed values when casting.", "keywords": ["casting", "zero extension", "sign extension", "truncation", "C examples"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Casting between integer types: extension and truncation", "importance_score": 9, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name": "CS356_Unit01_Integers", "slide_number": 53, "chunk_index": 0, "title": "Practice examples: casting and numeric interpretation", "summary": "Practice code exercises to predict output when casting negative signed numbers to unsigned types and back.", "main_text": "Practice: examples show printf outputs of casting signed negative numbers into unsigned short or unsigned int and back. Example expected outputs: v = -10000, uv = 55536; u = 4294967295, tu = -1. These exercises build intuition about bit reinterpretation and sign extension.", "notes_text": "Encourage students to run provided snippets and predict outputs before executing.", "keywords": ["practice", "casting examples", "printf", "interpretation", "sign-extension"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 1, "topic": "Practice examples: casting and numeric interpretation", "importance_score": 8, "file_hash": "a99891138a253f191db747d169a4d84e4840ccaa0ea62f7dbe8a94b9ec0679df"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":1,"chunk_index":0,"title":"Unit 2: Integer Operations","summary":"Title slide introducing Unit 2 on integer operations and bit/byte manipulation.","main_text":"This unit introduces how computers represent and manipulate integers at the bit and byte level. It sets up the themes for the lecture: arithmetic in C, overflow behavior, bitwise operators, and shift-based optimizations. The focus is on understanding low-level integer behavior to reason about correctness, performance, and hardware effects (e.g., two’s complement wraparound, bit masks, and logical vs arithmetic shifts).","notes_text":"","keywords":["integer operations","unit overview","bits","bytes","two’s complement","C arithmetic"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":2,"topic":"Unit Overview","importance_score":4,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":2,"chunk_index":0,"title":"Arithmetic in C","summary":"Section header marking the start of C integer arithmetic topics.","main_text":"Begins the section on integer arithmetic in C. The upcoming slides explain how standard operations (+, −, *, /, %) work on integers, including their byte-level effects and language-defined rounding/remainder rules.","notes_text":"","keywords":["Arithmetic in C","integer arithmetic","operators"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":2,"topic":"C Integer Arithmetic","importance_score":4,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":3,"chunk_index":0,"title":"Arithmetic Operations","summary":"Shows basic integer arithmetic in C with hex examples and remainder sign rule.","main_text":"Demonstrates that mainstream languages support integer arithmetic and shows concrete hex examples. With x=0xFF (255) and y=2, the slide illustrates: addition yields 257, subtraction 253, multiplication 510, division 127, and remainder 1. It emphasizes that integer division truncates toward zero and that the remainder can be computed as x − (x/y)*y. The remainder carries the sign of the dividend x, so x % (−2) still gives a positive remainder when x is positive.","notes_text":"","keywords":["addition","subtraction","multiplication","division","remainder","hex example","truncation"],"images":[{"description":"Code-style arithmetic example showing hex values and their byte results for +,−,*,/,% operations.", "labels":["x=0xFF","y=2","sum","sub","mul","div","rem"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":2,"topic":"Arithmetic Operations","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":4,"chunk_index":0,"title":"Expect Overflow (Wraparound)","summary":"Explains signed overflow behavior and wraparound examples in two’s complement.","main_text":"Shows that fixed-width integers overflow silently; this is normal machine behavior. Using 32-bit signed ints, max=0x7FFFFFFF and min=0x80000000. Examples: max+1 wraps to min, min−1 wraps to max, max*2 wraps to 0xFFFFFFFE (−2). Dividing min by −1 overflows because +min is not representable; typical hardware yields min again. Notes that signed overflow is technically undefined in C/C++, but most compilers wrap; some languages (Java, Go, Rust) guarantee wraparound.","notes_text":"","keywords":["overflow","wraparound","two’s complement","INT_MAX","INT_MIN","undefined behavior"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":2,"topic":"Overflow Behavior","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":5,"chunk_index":0,"title":"Detecting Overflow","summary":"Provides logical conditions for overflow and notes language helpers.","main_text":"Gives practical tests for overflow. Unsigned overflow can be detected by checking if addition decreases the value (x+y < x) or subtraction increases it (x−y > x). Signed overflow is detected when adding positives yields negative or adding negatives yields positive. Many languages offer “exact” arithmetic that throws exceptions if overflow occurs; e.g., Java Math.addExact, subtractExact, multiplyExact with boundary inputs.","notes_text":"","keywords":["overflow detection","unsigned overflow","signed overflow","addExact","wrap checks"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":2,"topic":"Overflow Detection","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":6,"chunk_index":0,"title":"Avoiding Overflow with Big Integers","summary":"Motivates infinite-precision integers and their costs.","main_text":"Explains that arbitrary/infinite precision integers avoid overflow but are slower. Example: Java BigInteger stores bits across an array of ints. A 63-bit value (Long.MAX_VALUE) becomes 73 bits after multiplying by 1000, showing dynamic growth. The tradeoff: correctness without overflow at the price of performance and memory overhead.","notes_text":"","keywords":["BigInteger","infinite precision","performance tradeoff","bit length"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":2,"topic":"Avoiding Overflow","importance_score":5,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":7,"chunk_index":0,"title":"Bitwise and Logical Operators","summary":"Section header introducing bitwise/logical operator behavior.","main_text":"Begins the section on bitwise and logical operators. The next slides explain how bitwise ops work per-bit, and how they differ from boolean logical ops in C-style languages.","notes_text":"","keywords":["bitwise operators","logical operators","section header"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":2,"topic":"Bitwise Operators","importance_score":4,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":8,"chunk_index":0,"title":"Modifying Individual Bits","summary":"Motivates masks by showing assignment changes all bits at once.","main_text":"Asks whether simple assignments can change only specific bits. Example questions: does x=0 clear only the LSB? does x=0xF0 set only the upper 4 bits? The slide illustrates that assignment overwrites the entire byte, so you need bitwise masks/operations to selectively modify bits without disturbing others.","notes_text":"","keywords":["bit masks","LSB","selective modification","byte overwrite"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":2,"topic":"Bit Mask Motivation","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":9,"chunk_index":0,"title":"Bitwise Operations","summary":"Defines AND/OR/XOR/NOT and emphasizes per-bit evaluation.","main_text":"Defines core bitwise operators: AND (&), OR (|), XOR (^), and NOT (~). Each operates independently on corresponding bit pairs, unlike logical operators (&&, ||, !, !=). The slide frames bitwise ops as per-bit computations with no cross-bit interaction, setting up truth-table reasoning and masking.","notes_text":"","keywords":["AND","OR","XOR","NOT","bitwise vs logical","masking"],"images":[{"description":"Diagram showing two 1-byte variables aligned by bits and producing a per-bit result for AND/OR/XOR/NOT.", "labels":["bit pairs","result"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":2,"topic":"Bitwise Operations","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":10,"chunk_index":0,"title":"Bitwise Ops on a Single Bit","summary":"Truth tables and identities for AND/OR/XOR masks.","main_text":"Shows how single-bit AND/OR/XOR can preserve, force, or flip a bit: AND can clear bits using a 0 mask; OR can set bits using a 1 mask; XOR inverts bits when masked with 1. Includes identities: 0|y=y, 1|y=1; 0&y=0, 1&y=y; 0^y=y, 1^y=~y; y^y=0; (x^y)^y=x. XOR yields 1 exactly when bits differ. These identities support mask design and algebraic simplification.","notes_text":"","keywords":["truth tables","mask","identities","XOR properties","bit flip"],"images":[{"description":"Three truth-table panels for AND/OR/XOR with mask interpretations (force/pass/invert).", "labels":["AND table","OR table","XOR table","mask"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":2,"topic":"Single-Bit Reasoning","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":11,"chunk_index":0,"title":"Bitwise Ops on Multiple Bits","summary":"Applies per-bit rules to bytes and shows C examples.","main_text":"Applies bitwise rules across all bits in parallel. With a=0xA5 and b=0xF0: a&b=0xA0 (clears lower bits), a|b=0xF5 (sets upper bits), a^b=0x55 (difference mask), ~a=0x5A (bitwise inversion). Includes a short C program printing these results and lists the operator symbols (&, |, ^, ~).","notes_text":"","keywords":["0xA5","0xF0","bitwise example","C code","byte-level ops"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":2,"topic":"Multi-Bit Operations","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":12,"chunk_index":0,"title":"Bitwise vs Logical Operators","summary":"Contrasts boolean semantics with per-bit semantics.","main_text":"Bitwise operators act on each bit independently. Logical operators treat entire operands as boolean: any nonzero is true, zero is false, and results are 0/1. Example: x=1,y=2 gives x&&y=1 but x&y=0. Logical NOT (!) flips truthiness; bitwise NOT (~) flips every bit, often yielding a nonzero value even if input was small. Key equivalence: !!x equals (x!=0).","notes_text":"","keywords":["bitwise vs logical","truthiness","&& vs &","!!x","boolean semantics"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":2,"topic":"Bitwise vs Logical","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":13,"chunk_index":0,"title":"Application: Swapping via XOR","summary":"Shows classic XOR-swap and compares to temp swap.","main_text":"Explains two swap methods. Traditional swap uses a temporary variable. XOR swap avoids temp by exploiting XOR properties: x=x^y, y=x^y (becomes old x), x=x^y (becomes old y). Illustrated with hex values (0x59,0xD3) and the bitwise reasoning that equal operands cancel and XOR is reversible.","notes_text":"","keywords":["XOR swap","temporary variable","commutative","reversible"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":2,"topic":"XOR Applications","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":14,"chunk_index":0,"title":"Application: Find the Unique Element","summary":"Uses XOR cancellation to find a non-duplicated number in O(n).","main_text":"Problem: in an array where every value appears twice except one, find the single value in linear time. XOR is commutative and self-inverse, so duplicates cancel (x^x=0). XORing all elements yields the unique element. Slide includes a C function that accumulates xor_all over the array and returns it.","notes_text":"","keywords":["XOR cancellation","single number","O(n)","commutative","x^x=0"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":2,"topic":"XOR Interview Patterns","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":15,"chunk_index":0,"title":"Exercises: Odd and Multiple of 4","summary":"Practice using masks to test parity/modulus without %.","main_text":"Exercises using bit masks. To test oddness, isolate the least significant bit: x&1 is 1 for odd, 0 for even. To test multiple-of-4, check if the two LSBs are both zero: !(x&3) returns true only when x mod 4 = 0.","notes_text":"","keywords":["parity","LSB","masking","multiple of 4","x&1","x&3"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":2,"topic":"Masking Exercises","importance_score":5,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":16,"chunk_index":0,"title":"Shift Operators","summary":"Section header for left/right shift operations.","main_text":"Introduces shift operators. The following slides define left shift (<<) and right shift (>>), including logical vs arithmetic behavior and common compiler optimizations for multiplication/division by powers of two.","notes_text":"","keywords":["shift operators","<<",">>","section header"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":2,"topic":"Shifts Overview","importance_score":4,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":17,"chunk_index":0,"title":"Left Shift (x << n)","summary":"Defines left shift and bit insertion/discard rules.","main_text":"Left shift moves bits left by n positions. Zeros shift in on the right; bits shifted out on the left are discarded. The operation is the same for signed and unsigned types for left shift. Example with an 8-bit value shows dropped MSBs and new 0s in LSB positions.","notes_text":"","keywords":["left shift","discarded bits","zeros shifted in","multiply by 2^n"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":2,"topic":"Left Shifts","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":18,"chunk_index":0,"title":"Left Shift Example","summary":"Works through multi-size examples including a sign-bit case.","main_text":"Provides numeric examples of left shift across sizes. A 64-bit hex constant shifted by 16 moves upper bits out and appends 16 zeros. A 32-bit int shifted by 8 similarly moves bytes. A char z=0x81 (1000 0001) left-shifted by 1 becomes 0000 0010 after the leading 1 is dropped — showing truncation at width boundaries.","notes_text":"","keywords":["left shift example","width truncation","0x81","byte movement"],"images":[{"description":"Example code and bit diagrams illustrating left shifts on long, int, and char with dropped MSB.", "labels":["0x1122334455667788","0x11223344","0x81"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":2,"topic":"Left Shift Examples","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":19,"chunk_index":0,"title":"Left Shift Uses","summary":"Shows shifts as bit-moves and fast multiplication.","main_text":"Two main uses: (1) move a bit/pattern within a word (e.g., 1<<31 isolates the sign bit position), and (2) fast multiplication by powers of two: x<<n equals x·2^n. Compilers replace constant multiplies with shifts when profitable; example shows imul for general multiply and shl for multiply-by-16.","notes_text":"","keywords":["bit move","1<<31","fast multiply","compiler optimization","shl"],"images":[{"description":"Visual comparing arithmetic multiply with shift-based multiply and disassembly showing shl usage.", "labels":["shl","imul","2^n"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":2,"topic":"Shift Optimizations","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":20,"chunk_index":0,"title":"Multiplying by Non-Powers of Two","summary":"Explains decomposing constants into power-of-two sums/differences.","main_text":"Even when the multiplier isn’t a power of two, shifts help by rewriting constants. Example: 17x = 16x + x, so compute (x<<4)+x. Exercise shows multiple decompositions for 14x: 8x+4x+2x (three shifts, two adds) or 16x−2x (two shifts, one add). Compilers often pick the cheapest form automatically.","notes_text":"","keywords":["constant multiplication","decomposition","17x","14x","shifts plus adds"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":2,"topic":"Strength Reduction","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":21,"chunk_index":0,"title":"Right Shift (x >> n)","summary":"Defines logical vs arithmetic right shift and sign extension.","main_text":"Right shift moves bits right by n. Bits shifted out on the right are discarded. For unsigned types, zeros shift in on the left (logical shift). For signed types, the MSB is replicated to preserve sign (arithmetic shift): positive numbers shift in 0s, negative numbers shift in 1s. Examples show 0xFFFFFFFF >>2 giving 0x3FFFFFFF for unsigned, but staying negative for signed.","notes_text":"","keywords":["right shift","logical shift","arithmetic shift","sign extension","MSB replication"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":2,"topic":"Right Shifts","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":22,"chunk_index":0,"title":"Right Shift Example","summary":"Shows different results for signed vs unsigned right shift.","main_text":"Example with x=0x80001122. Signed right shift by 8 replicates MSB, yielding a value with leading 1s. Unsigned right shift by 8 inserts 0s, yielding a smaller positive value. This highlights that >> depends on signedness.","notes_text":"","keywords":["signed vs unsigned","right shift example","0x80001122","sign extension"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":2,"topic":"Right Shift Examples","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":23,"chunk_index":0,"title":"Right Shift Uses","summary":"Covers bit-moves and fast division by powers of two, with caveat for signed values.","main_text":"Uses mirror left shift: (1) move patterns (x>>31 extracts sign as all-0s/all-1s), and (2) fast division by powers of two: unsigned x>>n equals floor(x/2^n). Compilers replace division by constants with shr/sar when valid. For signed division, biasing may be needed to match truncation toward zero.","notes_text":"","keywords":["fast division","x>>n","shr","sar","sign extraction","biasing"],"images":[{"description":"Diagram comparing division instruction (idiv) to shift-based division (shr/sar) and showing rounding caveat.", "labels":["idiv","shr","sar","2^n"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":2,"topic":"Division by Shifting","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":24,"chunk_index":0,"title":"Problem: Dividing by Shifting (Negative Numbers)","summary":"Shows that arithmetic shifting doesn’t match C/Java rounding for negatives.","main_text":"For positives, x/2 equals x>>1. For negatives, mismatch arises: C/Java integer division truncates toward zero, but arithmetic right shift rounds toward negative infinity. Example: −5/2 gives −2, while (−5>>1) yields −3. Main takeaway: signed division by shifting fails for non-multiples of 2^n because rounding direction differs.","notes_text":"","keywords":["signed division","rounding toward zero","arithmetic shift rounding","-5/2 vs -5>>1"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":2,"topic":"Signed Shifts Pitfall","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":25,"chunk_index":0,"title":"The Issue: Rounding Direction","summary":"Visualizes why two’s complement shifting rounds away from zero for negatives.","main_text":"Explains rounding mismatch using number-line intuition. Dropping fractional bits after shifting corresponds to rounding down (toward −∞) for negatives, not toward 0. Thus arithmetic shift produces a smaller (more negative) integer than standard truncation. The slide summarizes: two’s-complement division by shifting rounds to the next smallest integer unless corrected.","notes_text":"","keywords":["rounding","two’s complement","toward -infinity","number line","truncation mismatch"],"images":[{"description":"Number-line diagrams comparing truncation toward zero vs shift-based rounding for positive and negative values.", "labels":["rounding toward 0","away from 0"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":2,"topic":"Rounding Explanation","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":26,"chunk_index":0,"title":"Biasing for Signed Division by Shifting","summary":"Introduces biasing with 2^n−1 for negative inputs.","main_text":"To fix signed division by shifting, add a bias before shifting when x is negative. Dividing by 2^n using x>>n works for x≥0 or negative multiples of 2^n, but fails otherwise. Bias idea: if x<0, add (2^n−1), a run of n ones, which does not change multiples of 2^n but corrects rounding for others.","notes_text":"","keywords":["biasing","2^n-1","signed division","correct rounding","arithmetic shift"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":2,"topic":"Biasing Technique","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":27,"chunk_index":0,"title":"Biasing Examples","summary":"Applies biasing to several negatives to match true division.","main_text":"Works examples: For division by 4 (n=2), bias is 3. −8/4 equals (−8>>2) so bias is optional; −7/4 differs from (−7>>2), so compute (−7+3)>>2. For division by 16 (n=4), bias is 15: (−20+15)>>4 yields −1, matching truncation toward zero. Emphasizes when bias is needed vs harmless.","notes_text":"","keywords":["bias examples","-7/4","-20/16","2^n-1 bias","signed shift fix"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":2,"topic":"Biasing Examples","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit02_IntegerOps","slide_number":28,"chunk_index":0,"title":"CS:APP Practice 2.43 (Tweaked)","summary":"Reverse-engineers constants M and N from shift/add optimized code.","main_text":"Practice problem: given optimized arithmetic using shifts and adds, infer constants. The optimized code does x<<=5 then subtracts original x, implying x*(32−1)=31x, so M=31. For y, it adds 3 when negative then shifts right by 2, meaning divide by 4 with biasing, so N=4. Summarizes strength reduction and signed division correction together.","notes_text":"","keywords":["practice 2.43","strength reduction","M=31","N=4","biasing","shift optimization"],"images":[{"description":"Highlighted C/assembly translation showing x scaled by (1<<5)−1 and y divided by 4 with bias.", "labels":["M","N","x<<5","y>>2","bias"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":2,"topic":"Integrated Practice","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":1,"chunk_index":0,"title":"Unit 3 — Floating Point: IEEE 754 Representation","summary":"Introduces Unit 3: floating point and IEEE 754 representation; sets the topic and scope for the lecture slides that follow.","main_text":"Unit 3: Floating Point — IEEE 754 Representation. This slide serves as the unit title page announcing that the module covers floating point representation and the IEEE 754 standard. It identifies the unit (Unit 3) and the broad topic (Floating Point / IEEE 754 Representation), preparing students for detailed discussion of floating vs fixed point, formats, special values, rounding, and programmer implications in the following slides.","notes_text":"Title/cover slide — no technical details beyond unit identification. Use as citation anchor for the whole unit.","keywords":["Floating Point","IEEE 754","Unit 3","Floating vs Fixed","Representation"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Overview / IEEE 754","importance_score":6,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:0]{index=0}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":2,"chunk_index":0,"title":"Floating vs Fixed Point (overview)","summary":"High-level contrast between floating-point and fixed-point number systems, motivating floating point for very large and very small values.","main_text":"Floating point is used to represent both very small fractional numbers and very large numbers (examples: Avogadro's number ~6.022×10^23, Boltzmann's constant ~1.38×10^-23). 32- or 64-bit integers cannot represent such wide ranges, so floating-point formats (e.g., float and double in C) allocate bits differently to provide larger dynamic range. The slide emphasizes that with the same number of total bit combinations, float must space representable values non-uniformly to achieve range advantage over integers.","notes_text":"Motivation slide: show why floating point is necessary (range) and mention that floats trade off uniform density for range. Good to reference later when explaining exponent and significand fields.","keywords":["range","precision","float","double","dynamic range","integer limitations"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Motivation / Floating vs Fixed","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:1]{index=1}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":3,"chunk_index":0,"title":"Floating Point Intuition: spacing across magnitudes","summary":"Illustrates how floating point redistributes representable values to cover both very small and very large magnitudes with examples and a small numeric diagram.","main_text":"Same number of combinations (bits) can be used to represent values with different spacing: floats place denser representable values near zero (small magnitudes) and sparser spacing at large magnitudes. The slide uses small decimal values (e.g., 0.0, 0.1, 0.2, 0.3, -0.1, -0.2, -0.3) and large numbers to emphasize that a float must 'space values differently' to extend range beyond what an integer can cover. Visual axis or number-line style representation demonstrates non-uniform spacing of representable numbers.","notes_text":"Conceptual figure: useful when later explaining mantissa + exponent — the exponent controls scale while mantissa controls density within that scale.","keywords":["value spacing","density","dynamic range","mantissa","exponent"],"images":[{"description":"Number-line style illustration showing dense representable values near zero and sparser spacing at large magnitudes (visual aid for float spacing).","labels":["0.0","0.1","0.2","0.3","-0.1","-0.2","-0.3"],"position":{"x":0.05,"y":0.15,"width":0.9,"height":0.6}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":3,"topic":"Floating Point Intuition","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:2]{index=2}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":4,"chunk_index":0,"title":"Fixed Point (Base 10) — examples and ranges","summary":"Explains fixed-point decimal representations with examples (unsigned integers, fixed with 1 or 3 decimals) and their ranges and rounding errors.","main_text":"Fixed point in base 10 with a fixed number of digits (example: 6 digits total) can be interpreted in multiple ways: unsigned integers (000000–999999) give range [0, 10^6 - 1] with absolute rounding error ≤1/2; fixed-point with one decimal (00000.0 … 99999.9) gives range [0, 10^5 - 0.1] with abs. error ≤0.1/2; fixed-point with three decimals (000.000 … 999.999) gives range [0, 10^3 - 0.001] with abs. error ≤0.001/2. The slide notes that representation error (rounding) exists—add/sub of fixed point is exact (except overflow), while mul/div cause additional error.","notes_text":"Sets up contrast to floating point: fixed-point has uniform spacing, predictable absolute error, but limited range. Useful when explaining why floating-point uses exponent to vary scale.","keywords":["fixed-point","range","absolute error","decimal digits","rounding","overflow"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Fixed Point Examples","importance_score":7,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:3]{index=3}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":5,"chunk_index":0,"title":"Floating Point (Base 10) — exponent idea and biased exponent","summary":"Demonstrates base-10 floating point by moving the decimal point with an exponent, and introduces biased exponent storage to represent negative and positive exponents.","main_text":"Floating-point uses an exponent to move the decimal point and thus trade precision for range. Examples show how a mantissa with an exponent covers different ranges and precisions (e.g., 1.2345×10^5 vs 1.2345×10^-1). The slide explains biased exponent encoding for a single decimal-digit example (BIAS=4): stored digit = exponent + BIAS, so stored digits 0..9 map to exponents -4..+5. It also mentions normalized notation: one nonzero digit before the point in scientific notation (i.e., normal form).","notes_text":"This slide connects decimal intuition with the binary floating-point representation that follows; biased exponent concept will be generalized to Excess-127 / Excess-1023 in IEEE formats.","keywords":["exponent","biased exponent","normalization","mantissa","range","scientific notation"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Exponent & Bias","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:4]{index=4}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":6,"chunk_index":0,"title":"Perils of Floating Point — precision loss example","summary":"Shows a practical example where adding a small value to a very large floating-point number has no effect due to limited significant digits.","main_text":"Perils of floating point: finite number of significant digits causes loss of small increments when added to large values. Example: 123450 + 0.10000 should be 123450.1, but encoding with limited significant digits can represent 123450 and 123450.1 identically, causing the 0.1 to be lost. The slide emphasizes that floating point extends range at the cost of reduced density around large magnitudes and demonstrates how small additions can be swallowed by large values.","notes_text":"Illustrative caution for programmers: order of operations and grouping matters; adding small values into big ones can be lost. This motivates later advice on associativity and numerical stability.","keywords":["precision loss","rounding","significant digits","catastrophic cancellation","numerical stability"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Precision Loss","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:5]{index=5}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":7,"chunk_index":0,"title":"Reality of Floats — C example showing non-associativity","summary":"Presents a C program demonstrating that floating-point addition is not associative and some common surprising behaviors when adding small numbers to large floats.","main_text":"C code example demonstrates surprising float behavior: with float x = 1000000.0f, expressions like (x + 0.01f) == x evaluate to true because adding 0.01f does not change x due to limited precision. The program prints boolean results showing pitfalls: -x + (x + 0.01f) == 0.0 may hold (bad), while (-x + x) + 0.01f == 0.01f (better) — showing different parenthesizations matter. Key point: floats have finite significant digits; operations are not associative, so adding numbers of similar magnitude first is recommended.","notes_text":"Concrete demonstration useful when discussing associativity and compiler optimizations. Reference for later slides on programming implications.","keywords":["C example","associativity","precision","float quirks","programming advice"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"FP behavior in code","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:6]{index=6}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":9,"chunk_index":0,"title":"Floating Point in Base 2","summary":"Introduces binary floating point, its analogy to decimal scientific notation, and the three fields: sign, exponent, and fraction.","main_text":"Binary floating point mirrors decimal scientific notation. Decimal scientific notation has the form ±D.DDD × 10^exp; binary floating point instead uses ±b.bbbb × 2^exp. IEEE formats implement three fields: a sign bit (0 for positive, 1 for negative), an exponent field (encoded with a bias), and a fraction field (mantissa/significand) that stores bits after an implicit leading 1 in normalized numbers. This slide introduces the structural components that define all IEEE 754 floating-point values.","notes_text":"Foundation slide: prepares students for normalization rules and IEEE single/double specifications on subsequent slides.","keywords":["binary","scientific notation","mantissa","sign bit","exponent","fraction"],"images":[{"description":"Diagram showing three fields labeled S, Exp., and Fraction, representing components of binary floating point.","labels":["S","Exp.","Fraction"],"position":{"x":0.1,"y":0.3,"width":0.8,"height":0.4}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":3,"topic":"Binary Floating Point","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:0]{index=0}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":10,"chunk_index":0,"title":"Normalized Floating Point","summary":"Defines normalization in binary floating point and explains why the leading 1 in normalized values is implicit and not stored.","main_text":"Correct normalized scientific notation in decimal requires exactly one nonzero digit before the decimal point (e.g., 7.54×10^14 instead of 0.754×10^15). Binary normalization follows the same rule: normalized values always have the form ±1.bbbbbb × 2^exp. Because the leading bit is always 1 for any nonzero normalized binary number, IEEE floating point omits this bit from storage, effectively giving an extra bit of precision. Hardware must normalize values before storage, e.g., a computed intermediate result 0.001101 × 2^5 is normalized to 1.101000 × 2^2.","notes_text":"The implicit leading-1 rule is critical when discussing precision, subnormals/denormals, and the width of the fraction field.","keywords":["normalization","implicit bit","leading one","binary scientific notation","precision"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Normalization","importance_score":10,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:1]{index=1}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":11,"chunk_index":0,"title":"IEEE 754 Introduction","summary":"Introduces the IEEE 754 floating-point standard used for representing real numbers in computers.","main_text":"This slide marks the transition from conceptual binary floating-point notation to the standardized IEEE 754 format, which defines how hardware encodes sign, exponent, fraction (including normalization, biases, special values, rounding behavior, and binary formats). It sets the stage for upcoming details specific to single-precision (32-bit) and double-precision (64-bit) layouts.","notes_text":"High-level transition slide; no numerical details but important structurally as an anchor for the section.","keywords":["IEEE 754","standard","floating point","format","specification"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"IEEE 754 Overview","importance_score":7,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:2]{index=2}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":12,"chunk_index":0,"title":"IEEE 754 Floating Point Formats — Single and Double Precision","summary":"Details the IEEE 754 single-precision and double-precision formats, including field sizes, exponent biases, and approximate decimal ranges.","main_text":"IEEE 754 single precision (float in C) uses 32 bits: 1 sign bit, 8 exponent bits (Excess-127), and 23 fraction bits. Its approximate decimal precision is about 7 significant digits across a range of roughly 10±38. Double precision (double in C) uses 64 bits: 1 sign, 11 exponent bits (Excess-1023), and 52 fraction bits, giving about 16 significant decimal digits and a range of approximately 10±308. For both formats, the fraction bits represent the mantissa after the implicit leading 1 in normalized numbers.","notes_text":"Useful slide for memorization: bit widths, biases, and approximate decimal equivalence are commonly tested.","keywords":["single precision","double precision","exponent bias","fraction bits","float","double","range"],"images":[{"description":"Bitfield diagrams showing 1 sign bit, exponent field, and fraction field for single and double precision.","labels":["S","Exp.","Fraction","1","8","23","11","52"],"position":{"x":0.05,"y":0.2,"width":0.9,"height":0.5}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":3,"topic":"IEEE Formats","importance_score":10,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:3]{index=3}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":13,"chunk_index":0,"title":"Single-Precision Examples","summary":"Works through examples of encoding and decoding IEEE 754 single-precision values using sign, exponent, and fraction fields.","main_text":"Examples illustrate how to interpret 32-bit IEEE floating-point bit patterns. For instance, a bitstring beginning with sign=1, exponent=10000010₂ (130 decimal), and a given fraction represents a negative normalized binary value −1.1100110 × 2^3, which may be converted to decimal (e.g., −14.375). Additional examples show how to encode decimal fractions (e.g., 0.6875) into normalized binary scientific notation and corresponding IEEE fields. The slide demonstrates converting between raw bit patterns (e.g., hex like 000003F3) and the underlying numeric value.","notes_text":"Very useful for problem sets requiring manual IEEE 754 conversion. Students should practice extracting exponent (subtract bias), reconstructing significand, and renormalizing when necessary.","keywords":["IEEE 754","single precision","conversion","exponent decoding","fraction","hex encoding"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":3,"topic":"IEEE Examples","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:4]{index=4}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":14,"chunk_index":0,"title":"Excess-N Exponent Encoding","summary":"Explains Excess-N (bias) exponent representation and illustrates how adding a bias allows signed exponents to be stored as unsigned values.","main_text":"With k exponent bits, the unsigned range is 0…2^k−1. Excess-N representation defines stored_value = true_exponent + N. For single precision, the bias is 127; for double, 1023. Examples: an exponent of +1 becomes stored exponent 1+127 = 128 (10000000₂); an exponent of −2 in double becomes stored exponent −2 + 1023 = 1021 (binary 01111111101₂). Excess-N gives monotonic ordering: larger exponents correspond to larger stored unsigned integers, simplifying comparison. The slide also shows how storing unsigned integers and subtracting a central offset maps them to symmetric signed exponent ranges.","notes_text":"Critical for understanding exponent fields and special values (all 0s and all 1s) in IEEE formats.","keywords":["Excess-127","Excess-1023","biased exponent","signed exponent","storage","IEEE 754"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Exponent Bias","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:5]{index=5}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":15,"chunk_index":0,"title":"Why Not Two’s Complement for Exponents?","summary":"Shows why IEEE 754 uses biased exponents instead of two’s complement by comparing ordering behavior and numeric comparison logic.","main_text":"Two’s complement makes sign bit the most significant bit, causing incorrect ordering of exponents when comparing floating-point values. Because floating-point numbers are ordered first by exponent and then by fraction, exponents must be comparable using unsigned integer comparison. Examples show two’s-complement encoding producing misleading orderings (e.g., negative exponents appearing numerically larger). Excess-127 preserves correct ordering: larger exponents always have larger stored values. The slide lists example mappings between two’s complement, stored value, and excess-127 interpretations.","notes_text":"This conceptual slide justifies the storage design of IEEE exponents and supports students in reasoning about sorting and comparing FP numbers.","keywords":["two’s complement","biased exponent","ordering","comparison","Excess-127","IEEE design"],"images":[{"description":"Table comparing two’s complement vs stored exponent vs Excess-127 exponent interpretations.","labels":["2’s comp.","Stored Value","Excess-127"],"position":{"x":0.1,"y":0.3,"width":0.8,"height":0.5}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"comparison"},"metadata":{"course":"CS356","unit":3,"topic":"Exponent Ordering","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:6]{index=6}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":16,"chunk_index":0,"title":"Reserved Exponent Values in IEEE 754","summary":"Explains that exponent fields of all 1s or all 0s are reserved for special meanings and lists the valid exponent range for single precision.","main_text":"In IEEE 754 formats, exponent values of all 1s (binary 111…111) and all 0s (000…000) are reserved for special-number encodings: infinities, NaNs, zeros, and subnormal numbers. For single precision (8-bit exponent), the valid exponent range after bias removal is −126 to +127. The slide includes a table mapping stored exponent values 0–255 to excess-127 interpretations and indicates which entries are reserved.","notes_text":"This slide is foundational for the upcoming explanation of special values including ±0, ±∞, denormals, and NaNs.","keywords":["reserved exponent","special values","range","bias","subnormal"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Exponent Range","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":":contentReference[oaicite:7]{index=7}"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":17,"chunk_index":0,"title":"IEEE Exponent Special Values","summary":"Explains how exponent patterns of all zeros or all ones encode zeros, denormals, infinities, and NaNs depending on the fraction field.","main_text":"IEEE 754 uses exponent values 000…0 and 111…1 for special categories. When the exponent is all zeros and the fraction field is also zero, the number is ±0 depending on the sign bit. When the exponent is zero and the fraction is nonzero, the number is denormalized (a subnormal), interpreted as ±0.bbbbbb × 2^−126 in single precision. When the exponent is all ones and the fraction is zero, the number represents ±∞. When the exponent is all ones and the fraction is nonzero, the encoding represents NaN (Not-a-Number), used for undefined operations such as 0/0 or √(negative). The slide displays a table mapping exponent patterns and fraction patterns to meanings.","notes_text":"This table is essential for understanding corner cases in floating-point arithmetic and behavior in exceptional operations.","keywords":["special values","zero","infinity","NaN","denormal","subnormal","IEEE 754"],"images":[{"description":"Field table showing how exponent and fraction patterns encode zero, denormalized numbers, infinity, and NaN.","labels":["Exp.","Fraction","±0","Denormalized","±∞","NaN"],"position":{"x":0.1,"y":0.25,"width":0.8,"height":0.5}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"table"},"metadata":{"course":"CS356","unit":3,"topic":"Special Exponent Patterns","importance_score":10,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":18,"chunk_index":0,"title":"Special Values: Zero and Infinity (C demonstration)","summary":"Shows bit-level encodings for +0, −0, +∞, −∞, max and min floats, and how division by zero produces infinities.","main_text":"A C program prints raw IEEE 754 encodings of special floating-point values using a union that reinterprets float bits as integers. The slide demonstrates that +0.0 and −0.0 differ only in the sign bit (00000000 vs 80000000). +∞ has encoding 0x7F800000; −∞ is 0xFF800000. FLT_MAX (largest normalized float) encodes as 0x7F7FFFFF, and FLT_TRUE_MIN (smallest positive subnormal) encodes as 0x00000001. The slide shows that dividing 1.0 by ±0.0 yields ±∞ and that large results overflow to infinity, while tiny results underflow to zero. It also shows that +0.0 == −0.0 evaluates to true in comparisons, despite different bit patterns.","notes_text":"Highlights how floating point treats signed zero, infinity, and overflow/underflow; useful when debugging numeric edge cases.","keywords":["signed zero","infinity","encoding","overflow","underflow","C example"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Zeros and Infinities","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":19,"chunk_index":0,"title":"Special Values: NaN (Not-a-Number)","summary":"Demonstrates “sticky” NaN behavior, bit patterns, and non-reflexive comparisons using C code.","main_text":"NaNs occur when the exponent is all ones and the fraction field is nonzero. A C program example shows that 0/0 produces a NaN with encoding such as 0xFFC00000, where the fraction holds an ‘error code’. The NaN payload propagates across operations: multiplying the NaN by 42 still yields the same NaN bit pattern, demonstrating stickiness. Additionally, NaN comparisons are always false, even comparing a NaN to itself. Expressions like (nan >= 0.0), (nan < 0.0), and (nan == nan) all evaluate to false. This captures IEEE 754’s definition that NaNs represent invalid numeric results.","notes_text":"Important for reasoning about floating-point comparisons and why ‘isnan()’ checks are necessary.","keywords":["NaN","Not-a-Number","sticky behavior","comparison false","0/0","exception"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"NaN Behavior","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":20,"chunk_index":0,"title":"Transition to Denormalized (Subnormal) Numbers","summary":"Explains how denormalized numbers allow a smooth transition from normalized values to zero by using an implicit 0.x significand.","main_text":"When the exponent field is all zeros and the fraction is nonzero, IEEE interprets the significand as 0.bbbbbb rather than the normalized 1.bbbbbb. The effective exponent is fixed at −126 (for single precision), giving the smallest representable magnitudes. Examples show: 0 00000001 000…0 corresponds to normalized 1.0 × 2^−126; 0 00000000 100…0 corresponds to 0.1₂ × 2^−126 = 2^−127; and 0 00000000 010…0 corresponds to 2^−128. This mechanism fills the gap between the smallest normal number (~2^−126) and zero with gradually smaller subnormals.","notes_text":"Understanding the smooth transition helps students grasp underflow behavior and the significance of gradual underflow in IEEE arithmetic.","keywords":["denormal","subnormal","implicit zero","underflow","fraction field","IEEE 754"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Subnormals","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":21,"chunk_index":0,"title":"Floating Point vs Fixed Point — Range vs Precision","summary":"Compares floating point and fixed point by emphasizing floating point’s wider range but lower precision for large magnitudes.","main_text":"Single-precision floating point provides roughly 7 significant decimal digits across an enormous exponent range (10^±38), whereas a 32-bit signed integer represents only about ±2 billion. Floating point achieves this larger range because its 23 fraction bits are distributed across exponentially growing intervals controlled by the exponent. However, it sacrifices precision: not all numbers in the representable range can be encoded exactly, and spacing grows with magnitude. Double precision expands this to around 16 significant digits and range up to 10^±308. The slide reiterates the central tradeoff: range vs. precision.","notes_text":"Important conceptual contrast: fixed point has uniform spacing, while floating point has nonuniform spacing tied to exponent.","keywords":["range","precision","floating point","fixed point","dynamic range","significant digits"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Range vs Precision","importance_score":7,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":22,"chunk_index":0,"title":"12-bit “IEEE Short” Format (Class-Specific)","summary":"Defines a simplified 12-bit floating-point format used for instructional purposes, including sign, exponent, and fraction bit allocations.","main_text":"The slide introduces a fictional 12-bit floating-point format used solely for class exercises. It contains: 1 sign bit; 5 exponent bits using Excess-15 representation (stored exponent = true exponent + 15); and 6 fraction bits representing the significand bits after the implicit leading 1. Reserved exponent patterns follow the same conventions as IEEE formats. The slide provides a labeled diagram highlighting the fields and how normalization works for this toy format.","notes_text":"Useful for homework problems requiring manual conversions; simpler than 32-bit IEEE format but conceptually identical.","keywords":["12-bit float","toy format","Excess-15","fraction bits","sign bit","normalization"],"images":[{"description":"Bitfield diagram of the 12-bit floating-point format showing sign, exponent, and fraction fields.","labels":["S","Exp.","Fraction","1","5 bits","6 bits"],"position":{"x":0.1,"y":0.3,"width":0.8,"height":0.45}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":3,"topic":"Instructional Float Format","importance_score":6,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":23,"chunk_index":0,"title":"12-bit Format Examples","summary":"Works through example encodings and decodings of the class’s 12-bit floating-point format, including renormalization steps.","main_text":"Examples include converting binary patterns like 1 10100 101101 into numeric values. For instance, with sign=1, exponent=10100₂ (20 decimal), the true exponent is 20−15=5, giving −1.101101 × 2^5 = −54.5. Another example converts 0 10011 010111: exponent=19 decimal → true exponent=4, giving +1.010111 × 2^4 = +21.75. Additional examples demonstrate negative exponents, such as 1 01101 100000: exponent=13 decimal → true exponent=−2, producing −0.375. These examples illustrate normalization, exponent decoding, and binary-to-decimal conversion steps.","notes_text":"Good practice examples for understanding conversion process identical to IEEE 754 but using smaller fields.","keywords":["12-bit","conversion","examples","binary","exponent decoding","significand"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":3,"topic":"12-bit Examples","importance_score":7,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":24,"chunk_index":0,"title":"Rounding (Section Introduction)","summary":"Introduces the concept of rounding in floating-point arithmetic and prepares for detailed rounding-mode explanations.","main_text":"This slide introduces the need for rounding in floating-point arithmetic. Because floating-point formats have limited precision, many operations (integer-to-float conversion, floating add/sub, multiplication, and division) produce more bits than can fit in the available fraction field, requiring rounding to keep the result representable. The slide sets up the detailed explanation of rounding modes and guard/round/sticky bits in the following slides.","notes_text":"Only a section heading; no detailed rules yet. Acts as a transition into rounding methods.","keywords":["rounding","precision limit","floating point operations","overflow of fraction","rounding introduction"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Rounding Overview","importance_score":5,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":25,"chunk_index":0,"title":"The Need to Round","summary":"Shows why rounding is required in floating-point arithmetic by illustrating operations that generate more bits than can be stored.","main_text":"Rounding is essential because floating-point formats cannot store all bits that arise from arithmetic operations. Integer-to-float conversion may produce a binary representation with more fraction bits than allowed. Floating add/sub may require alignment of exponents, causing low-order bits to be shifted out. Floating multiply/divide typically creates products or quotients with more bits than the fraction field can accommodate. The slide shows examples: converting +725 into normalized binary yields 1.011010101 × 2^9, which has more fraction bits than the format supports; adding numbers alike in exponent can produce numerous bits; multiplying significands generates a product with many fractional bits, requiring rounding to fit into the target format.","notes_text":"Key conceptual point: overflow of fraction bits is normal; rounding is required and unavoidable in IEEE arithmetic.","keywords":["rounding","fraction bits","normalization","overflow of precision","add/sub","mul/div"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Why Rounding Is Needed","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":26,"chunk_index":0,"title":"Rounding Methods","summary":"Describes the four primary IEEE 754 rounding modes, including round-to-nearest-even, toward zero, toward +∞, and toward −∞.","main_text":"IEEE 754 defines multiple rounding modes. (1) Round to Nearest, Ties to Even: chooses the representable value closest to the exact result; if exactly halfway, choose the one whose least significant bit is even. This reduces long-term statistical bias. (2) Round Toward Zero (Chopping): truncates bits beyond the representable range, moving the result toward zero. (3) Round Toward +∞ (Round Up): rounds to the smallest representable value greater than or equal to the exact value. (4) Round Toward −∞ (Round Down): rounds to the greatest representable value less than or equal to the exact value. The slide concisely illustrates each rule conceptually.","notes_text":"Students must recognize when each mode is used and how ties are resolved; round-to-nearest-even is the IEEE default.","keywords":["rounding modes","nearest even","round toward zero","round up","round down","IEEE 754"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Rounding Modes","importance_score":10,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":27,"chunk_index":0,"title":"Number Line View of Rounding Methods","summary":"Uses number-line diagrams to visualize how each rounding mode chooses between adjacent representable values.","main_text":"The slide shows four number-line illustrations. In each, representable floating values are marked as discrete points; green regions represent exact results lying between them. For round-to-nearest, the exact value is mapped to the closest representable number; halfway cases are resolved using the even rule. For round-to-zero, values are truncated toward zero: negative exact values round up toward zero, positives round down. For round-to-+∞, values round upward; for round-to-−∞, values round downward. The diagrams demonstrate which direction the green intervals collapse to under each rounding mode.","notes_text":"Useful visual reference for students who think in terms of geometric intuition rather than bit patterns.","keywords":["rounding","number line","visualization","nearest","toward zero","toward infinity"],"images":[{"description":"Four number-line diagrams illustrating round-to-nearest, round-to-zero, round-to-+infinity, and round-to-−infinity.","labels":["0","+∞","−∞","round to nearest","round to zero","round to +∞","round to −∞"],"position":{"x":0.07,"y":0.18,"width":0.86,"height":0.65}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":3,"topic":"Rounding Visualization","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":28,"chunk_index":0,"title":"Many More Rounding Details","summary":"Placeholder slide indicating that additional rounding-related mechanisms exist beyond the major rounding modes.","main_text":"This slide indicates that floating-point arithmetic contains many more nuanced rounding behaviors and edge cases that go beyond the major modes previously discussed. While no specific content is listed, the slide implies additional subtleties exist in rounding intermediates, extended precision, fused operations, and architecture-dependent behaviors.","notes_text":"Section divider; no technical content to extract, but signals upcoming rounding examples.","keywords":["rounding","IEEE 754","details","precision","arithmetic"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Additional Rounding Notes","importance_score":3,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":29,"chunk_index":0,"title":"Rounding to Nearest in Base 10","summary":"Explains decimal rounding rules analogous to IEEE’s round-to-nearest-even mode using representative decimal examples.","main_text":"Using decimal analogies, the slide shows how round-to-nearest-even works. Values between 1.2351 and 1.2399 round up to 1.24; values between 1.2301 and 1.2349 round down to 1.23. For exact halfway cases like 1.2350, the tie is resolved by rounding to the option with an even digit in the last place—here, 1.24. For 1.2450, the even choice is also 1.24. This even-tie-breaking rule reduces systematic upward or downward drift over repeated operations by making the probability of rounding up or down roughly balanced.","notes_text":"Establishes intuition for tie-to-even in familiar decimal settings before translating to binary versions.","keywords":["decimal rounding","nearest even","tie-breaking","rounding bias","IEEE analogy"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Decimal Rounding Analogy","importance_score":6,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":30,"chunk_index":0,"title":"Rounding to Nearest in Base 2 (GRS Bits)","summary":"Shows how binary halfway cases are detected and resolved using guard, round, and sticky bits.","main_text":"In binary rounding, the exact halfway point corresponds to the bit pattern 10…0 in the extra bits beyond the fraction field. Hardware keeps additional bits—Guard (G), Round (R), and Sticky (S)—to determine whether the truncated portion is exactly half, more than half, or less than half. If GRS = 100…0 (i.e., only the guard bit is 1 and all others are zero), the value is exactly halfway and is rounded to the even representable value. If the additional bits indicate more than halfway (1x…x), the result rounds up. If less than halfway (0x…x), the result rounds down. Examples use values like 10.10000₂ for exactly half (round to 2), 10.10010₂ for more than half (round to 3), and 10.00010₂ for less than half (round to 2).","notes_text":"The GRS mechanism is central to all practical floating-point rounding implementations.","keywords":["GRS bits","guard","round","sticky","binary rounding","halfway cases"],"images":[{"description":"Diagram showing a binary significand with extra Guard, Round, and Sticky bits used to determine rounding direction.","labels":["GRS","bit field"],"position":{"x":0.08,"y":0.25,"width":0.84,"height":0.5}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":3,"topic":"Binary Rounding Mechanics","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":31,"chunk_index":0,"title":"Rounding to Nearest in Base 2 — Examples","summary":"Provides examples showing how additional bits guide rounding actions, including renormalization when rounding causes overflow of the fraction.","main_text":"Several rounding cases are shown. If additional bits are 110, the value is more than halfway, so the fraction increments (round up). In some cases, adding 1 to the fraction overflows, requiring renormalization (e.g., 1.111111 × 2^4 + a small increment becomes 1.000000 × 2^5). If additional bits are 001 (less than halfway), the fraction remains unchanged. The slide walks through three concrete examples, illustrating when rounding causes fraction overflow and subsequent exponent adjustment.","notes_text":"This slide concretizes how rounding interacts with normalization rules and exponent adjustments.","keywords":["rounding examples","renormalization","fraction overflow","GRS bits","nearest even"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":3,"topic":"Rounding Examples Binary","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":32,"chunk_index":0,"title":"Round-to-Nearest: Halfway Case Examples","summary":"Shows detailed halfway rounding cases and demonstrates which candidate representable value has an even least significant bit.","main_text":"Examples illustrate three halfway cases: (1) 1.001100100 × 2^4 has two rounding options (1.001100 or 1.001101) and rounds down because the even option is selected. (2) A case like 1.111111100 × 2^4 rounds up, causing renormalization to 1.000000 × 2^5. (3) Another example 1.001101100 × 2^4 yields two candidates (1.001101 or 1.001110) and rounds up to the one with even LSB. These examples emphasize the tie-to-even rule and show that renormalization may occur even in halfway cases.","notes_text":"Important because halfway cases are subtle; students frequently misapply the even rule.","keywords":["halfway case","round-to-even","tie-breaking","renormalization","binary rounding"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Halfway Rounding","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":33,"chunk_index":0,"title":"Round to Zero (Chopping)","summary":"Shows how the round-toward-zero mode simply discards extra bits, producing the truncated significand.","main_text":"In round-toward-zero mode, also known as chopping, the Guard, Round, and Sticky bits are discarded, and the fraction is left unchanged. Three examples are shown: 1.001100001×2⁴, 1.001101101×2⁴, and 1.001100111×2⁴. In each case, even though the GRS bits vary, no rounding direction is applied—extra bits are dropped and the representable prefix becomes the final stored significand. This mode always moves the result toward zero (reducing magnitude), regardless of sign.","notes_text":"This rounding mode is commonly used when converting floats to ints; it is simple but can produce bias.","keywords":["round toward zero","chopping","GRS bits","truncation","rounding mode"],"images":[{"description":"Three examples illustrating truncated significands after dropping G, R, and S bits.","labels":["GRS"],"position":{"x":0.12,"y":0.30,"width":0.78,"height":0.40}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":3,"topic":"Rounding Toward Zero","importance_score":6,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":34,"chunk_index":0,"title":"Rounding Implementation (GRS and Sticky Bit)","summary":"Explains how hardware uses guard, round, and sticky bits to implement rounding efficiently.","main_text":"To implement rounding without storing an unbounded number of extra bits, hardware keeps only a limited set of bits beyond the fraction: Guard bits (several), a Round bit, and a Sticky bit. The Sticky bit is the OR of all bits that follow the Guard and Round bits, indicating whether any trailing bits were nonzero. Example: 1.01001010010₂×2⁴ becomes 1.010010101₂×2⁴ after rounding using GRS bits. With GRS, hardware determines whether to round up, round down, or tie-to-even using only three bits, making rounding efficient and precise enough for IEEE 754 rules.","notes_text":"This slide is key for understanding hardware-level rounding operations and the use of extended precision pipelines.","keywords":["guard bit","round bit","sticky bit","rounding hardware","GRS"],"images":[{"description":"Diagram showing significand with Guard, Round, and Sticky bits after the stored fraction field.","labels":["GRS"],"position":{"x":0.08,"y":0.30,"width":0.84,"height":0.45}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":3,"topic":"Hardware Rounding","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":35,"chunk_index":0,"title":"Implications for Programmers","summary":"Highlights why programmers must account for rounding, non-associativity, and subtle float behaviors when writing numerical code.","main_text":"This introductory slide notes that floating-point rounding and representation have real consequences for software. It prepares for subsequent slides that show examples of association errors, catastrophic cancellation, and pitfalls in arithmetic expressions. Programmers must be aware of precision limits, rounding modes, and ordering of operations to produce stable and predictable numeric results.","notes_text":"Section divider leading into practical programming concerns, not yet containing equations.","keywords":["programming","numerical stability","rounding effects","non-associativity","precision"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Programming Implications","importance_score":5,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":36,"chunk_index":0,"title":"Floating Point Addition and Subtraction","summary":"Shows that FP add/sub are not associative and demonstrates catastrophic cancellation with examples.","main_text":"Floating-point addition and subtraction are not associative: (a+b)+c ≠ a+(b+c). Examples are provided: (0.0001 + 98475) − 98474 ≠ 0.0001 + (98475 − 98474). Due to rounding, 98475 − 98474 = 1, but 0.0001 + 98475 rounds to 98475.0 in many formats, making the left side reduce to 1, while the right side yields 1.0001. Another example involving extremely large numbers (1 + 1.11…1 × 2¹²⁷ − 1.11…1 × 2¹²⁷) shows how huge magnitudes swallow small values entirely. Catastrophic cancellation also appears when subtracting nearly equal numbers, e.g., 9.999 − 9.998 = 0.001, producing one meaningful digit from four.","notes_text":"Students should reorder computations to add numbers of similar magnitude first to reduce error.","keywords":["non-associativity","cancellation","floating point add","floating point sub","numerical error"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Addition/Subtraction Issues","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":37,"chunk_index":0,"title":"Floating Point Multiplication and Division","summary":"Explains non-associativity in FP multiply/divide and shows how overflow and underflow occur depending on evaluation order.","main_text":"Floating-point multiplication and division are not associative and also violate distributivity over addition. Examples: (big1 × big2) / (big3 × big4) may overflow during the first multiplication; computing 1/big3 × 1/big4 × big1 × big2 may underflow early; computing (big1/big3) × (big2/big4) often avoids overflow/underflow. Also, a*(b+c) ≠ a*b + a*c because rounding intervenes. A cautionary note on integer operations in C: expressions like F = (9/5)*C + 32 behave differently than F = (9*C)/5 + 32 due to integer truncation.","notes_text":"Highlights that operation ordering matters, and integer vs float operations must be considered separately.","keywords":["multiplication","division","non-associative","overflow","underflow","C integer division"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Multiplication/Division Behavior","importance_score":7,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":38,"chunk_index":0,"title":"Floating Point Comparisons (== and <)","summary":"Warns against direct equality or loop-counter comparisons with floats and suggests using tolerances.","main_text":"Equality comparisons on floats are unreliable because many decimal values (e.g., 0.1, 0.2, 0.3) are not exactly representable in binary. Example: (0.1f + 0.2f == 0.3f) evaluates to false. Floats also should not be used as loop counters because increments accumulate rounding error; e.g., incrementing t by 0.1 may never reach exactly 1.0. A safer approach is to test whether two floats differ by less than a small epsilon. Python’s isclose(x,y) is referenced as an example of a principled approximate comparison method.","notes_text":"Classic pitfall for students transitioning from pure math to numerical computing.","keywords":["floating point compare","equality","epsilon","loop counter","precision error"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Comparison Pitfalls","importance_score":9,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":39,"chunk_index":0,"title":"Floating Point & Compiler Optimizations","summary":"Shows how compiler transformations can change numerical results because FP operations are not associative.","main_text":"Given expressions x = a + b + c and y = b + c + d, a compiler might apply common-subexpression elimination to compute temp = b + c and reuse it. However, in floating point, changing the grouping of additions can yield different results due to rounding. The slide references a discussion by Linus Torvalds regarding the use of -ffast-math, which allows compilers to violate strict IEEE 754 associativity rules to gain performance at the cost of accuracy or predictability.","notes_text":"Key idea: optimization is constrained because mathematically equivalent expressions are not numerically equivalent in FP.","keywords":["compiler optimization","ffast-math","associativity","numerical stability","common subexpression"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Compiler Effects","importance_score":6,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":40,"chunk_index":0,"title":"Floating Point & Casting","summary":"Describes how overflow and rounding occur when converting between ints, floats, and doubles.","main_text":"Casts between numeric types behave differently depending on size and precision. int → float: no overflow but rounding may occur because float has only 24 bits of precision. int → double: no rounding for 32-bit ints because double has 53 bits of precision. float → double: safe (no rounding). double → float: overflow possible if exponent is too large; rounding occurs because float has fewer fraction bits. float/double → int: overflow possible; fractional part is truncated (round toward zero). The slide also poses a question about casting from long, hinting at platform-specific integer widths.","notes_text":"Crucial slide for students writing C programs involving mixed numeric types.","keywords":["casting","overflow","rounding","int to float","double to float","truncate"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"Casting Behavior","importance_score":7,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":41,"chunk_index":0,"title":"References for Floating Point","summary":"Lists recommended external resources explaining floating-point arithmetic and common pitfalls.","main_text":"The slide references several authoritative resources: The Floating-Point Guide (floating-point-gui.de), Goldberg’s classic paper 'What Every Computer Scientist Should Know About Floating-Point Arithmetic', and 'Losing My Precision: Tips for Handling Tricky Floating Point Arithmetic'. These resources cover binary representation, pitfalls, precision issues, rounding, and best practices for working with floating point in software.","notes_text":"No technical content beyond listing external references.","keywords":["reference","floating point guide","Goldberg paper","precision","numerical analysis"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":3,"topic":"References","importance_score":2,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name":"CS356_Unit03_Floats","slide_number":42,"chunk_index":0,"title":"Hints for DataLab","summary":"Provides C-oriented guidance for converting floating-point bit patterns to integers, including handling of exponent extremes.","main_text":"Hints include: To compute absolute value, manipulate the sign bit. To compare floats without '==', evaluate whether |x−y| < epsilon. For float-to-int conversion: if the exponent is too big, return 0x80000000 (overflow); if too small, return 0; otherwise, extract and shift the integer portion of the significand (e.g., converting a bit pattern like 10100100.101…). The slide repeats the full Excess-127 exponent table for reference, showing special cases such as +∞, −∞, and NaN for exponent 255.","notes_text":"Directly relevant for assignments in bit-manipulation labs requiring manual IEEE-754 decoding.","keywords":["DataLab","float to int","absolute value","comparison","exponent table","overflow"],"images":[{"description":"Exponent table showing stored values, excess-127 mapping, and special encodings such as INF and NaN.","labels":["Excess-127","255","NaN","inf"],"position":{"x":0.1,"y":0.25,"width":0.8,"height":0.45}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"table"},"metadata":{"course":"CS356","unit":3,"topic":"DataLab Hints","importance_score":8,"file_hash":"<sha256 placeholder>","source_file":"/mnt/data/CS356Unit03_Floats.pdf"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 1, "chunk_index": 0, "title": "Unit 4: Assembly", "summary": "Unit 4: Assembly With a tour of x86-64", "main_text": "Unit 4: Assembly\nWith a tour of x86-64", "notes_text": "", "keywords": ["Unit", "Assembly", "tour", "x86-64"], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.4001736534966363, "y": 0.27167544894748263, "width": 0.19965282016330296, "height": 0.3194444783528646}}], "layout": {"num_text_boxes": 4, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Unit 4: Assembly", "importance_score": 9, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 2, "chunk_index": 0, "title": "Computer Organization", "summary": "Computer Organization", "main_text": "Computer Organization", "notes_text": "", "keywords": ["Computer", "Organization"], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Computer Organization", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 3, "chunk_index": 0, "title": "CPU, Memory & I/O", "summary": "CPU, Memory & I/O (aka IP)", "main_text": "CPU, Memory & I/O\n(aka IP)\nRegisters provide fast, temporary storage for data\nbeing processed to avoid the need to repeatedly\naccess the slower main memory (RAM)\nArithmetic and Logic Unit", "notes_text": "", "keywords": ["CPU", "Memory", "I/O", "aka", "Registers", "provide", "fast", "temporary", "storage"], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.15106900533040366, "y": 0.20086174858940972, "width": 0.6978619469536675, "height": 0.7991380479600695}}], "layout": {"num_text_boxes": 5, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "CPU, Memory & I/O", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 4, "chunk_index": 0, "title": "Inside the CPU", "summary": "Inside the CPU For each instruction: fetch, decode, execute", "main_text": "Inside the CPU\nFor each instruction: fetch, decode, execute\nregisters\n00 00 00 00 00 00 00 01\n%rax\nFF FF FF FF FF FF FF FF\n%rbx\n11 22 33 44 55 66 77 88\n%rcx\n00 00 00 00 11 22 33 44\n%rdx\n00 00 00 00 00 40 00 50\n%rip\nMemory controller / Bus Interface\ncurrent instruction\n48 01 c3\nmeans “addq %rax,%rbx”:\ni.e., “rbx = rax+rbx”\nArithmetic Logic Unit (ALU)\nimplements operations on\nregisters, e.g. +, -, &, …\nrax\nrbx\nrax + rbx\n+", "notes_text": "", "keywords": ["%rax", "%rbx", "%rcx", "%rdx", "%rip", "rbx", "instruction", "registers", "rax"], "images": [], "layout": {"num_text_boxes": 20, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Inside the CPU", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 5, "chunk_index": 0, "title": "Fetching the next instruction", "summary": "Fetching the next instruction %rip is the address of the next instruction", "main_text": "Fetching the next instruction\n%rip is the address of the next instruction\nregisters\n00 00 00 00 00 00 00 01\n%rax\n00 00 00 00 00 00 00 00\n%rbx\n11 22 33 44 55 66 77 88\n%rcx\n00 00 00 00 11 22 33 44\n%rdx\n00 00 00 00 00 40 00 50\n%rip\nMemory controller / Bus Interface\ncurrent instruction\nwe need to fetch an\ninstruction from memory\nArithmetic Logic Unit (ALU)\nimplements operations on\nregisters, e.g. +, -, &, …\nfetch one instruction at the given address\nread from 0x400050\ninstr. 48 89 c8", "notes_text": "", "keywords": ["%rip", "instruction", "%rax", "%rbx", "%rcx", "%rdx", "next", "address", "registers"], "images": [], "layout": {"num_text_boxes": 17, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Fetching the next instruction", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 6, "chunk_index": 0, "title": "Slide 6", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 19, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 6", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 7, "chunk_index": 0, "title": "Slide 7", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 15, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 7", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 8, "chunk_index": 0, "title": "Slide 8", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 18, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 8", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 9, "chunk_index": 0, "title": "Slide 9", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 18, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 9", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 10, "chunk_index": 0, "title": "Slide 10", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 32, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 10", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 11, "chunk_index": 0, "title": "Slide 11", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.511991458468967, "y": 0.1963035413953993, "width": 0.2642251756456163, "height": 0.42276031494140626}}, {"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.7965413411458333, "y": 0.5629045952690972, "width": 0.169354248046875, "height": 0.37935255262586803}}], "layout": {"num_text_boxes": 8, "num_images": 2, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 11", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 12, "chunk_index": 0, "title": "Slide 12", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 24, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 12", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 13, "chunk_index": 0, "title": "Slide 13", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.05544620090060764, "y": 0.3421346706814236, "width": 0.30739559597439237, "height": 0.5231538899739583}}], "layout": {"num_text_boxes": 8, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 13", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 14, "chunk_index": 0, "title": "Slide 14", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 41, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 14", "importance_score": 9, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 15, "chunk_index": 0, "title": "Slide 15", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 15", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 16, "chunk_index": 0, "title": "Slide 16", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 10, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 16", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 17, "chunk_index": 0, "title": "Slide 17", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 8, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 17", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 18, "chunk_index": 0, "title": "Slide 18", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 37, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 18", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 19, "chunk_index": 0, "title": "Slide 19", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 10, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 19", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 20, "chunk_index": 0, "title": "Slide 20", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 7, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 20", "importance_score": 9, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 21, "chunk_index": 0, "title": "Slide 21", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 18, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 21", "importance_score": 9, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 22, "chunk_index": 0, "title": "Slide 22", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.09594685236612956, "y": 0.17957560221354166, "width": 0.7925878630744086, "height": 0.7832633463541666}}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 22", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 23, "chunk_index": 0, "title": "Slide 23", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.09594799677530924, "y": 0.180244140625, "width": 0.7925853623284234, "height": 0.7819276258680555}}], "layout": {"num_text_boxes": 3, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 23", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 24, "chunk_index": 0, "title": "Slide 24", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.09594816631740993, "y": 0.18024495442708333, "width": 0.7925811237759061, "height": 0.7819245741102431}}], "layout": {"num_text_boxes": 3, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 24", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 25, "chunk_index": 0, "title": "Slide 25", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.016666666666666666, "y": 0.22138895670572917, "width": 0.9666662428114149, "height": 0.41103624131944444}}], "layout": {"num_text_boxes": 6, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 25", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 26, "chunk_index": 0, "title": "Slide 26", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.016666666666666666, "y": 0.2080555894639757, "width": 0.9666664123535156, "height": 0.42122670491536457}}], "layout": {"num_text_boxes": 7, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 26", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 27, "chunk_index": 0, "title": "Slide 27", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.016666666666666666, "y": 0.22138892279730904, "width": 0.9666662428114149, "height": 0.38991580539279513}}], "layout": {"num_text_boxes": 7, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 27", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 28, "chunk_index": 0, "title": "Slide 28", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.016666666666666666, "y": 0.19472232394748265, "width": 0.9666670057508681, "height": 0.39840850830078123}}], "layout": {"num_text_boxes": 5, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 28", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 29, "chunk_index": 0, "title": "Slide 29", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.016666666666666666, "y": 0.18138892279730903, "width": 0.966666751437717, "height": 0.39071984185112846}}], "layout": {"num_text_boxes": 5, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 29", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 30, "chunk_index": 0, "title": "Slide 30", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.016666666666666666, "y": 0.18138902452256944, "width": 0.9666670905219183, "height": 0.3988773600260417}}], "layout": {"num_text_boxes": 7, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 30", "importance_score": 9, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 31, "chunk_index": 0, "title": "Slide 31", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.016666666666666666, "y": 0.19472232394748265, "width": 0.9666664123535156, "height": 0.43749026828342014}}], "layout": {"num_text_boxes": 4, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 31", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 32, "chunk_index": 0, "title": "Slide 32", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.016666666666666666, "y": 0.18139126247829862, "width": 0.9611503601074218, "height": 0.48359578450520835}}], "layout": {"num_text_boxes": 4, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 32", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 33, "chunk_index": 0, "title": "Slide 33", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.016666666666666666, "y": 0.1462029351128472, "width": 0.9595389472113716, "height": 0.5321171061197917}}], "layout": {"num_text_boxes": 7, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 33", "importance_score": 9, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 34, "chunk_index": 0, "title": "Slide 34", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 6, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 34", "importance_score": 9, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 35, "chunk_index": 0, "title": "Slide 35", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 35", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 36, "chunk_index": 0, "title": "Slide 36", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.46768917507595487, "y": 0.16774715847439237, "width": 0.4951443142361111, "height": 0.5405325995551216}}], "layout": {"num_text_boxes": 9, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 36", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 37, "chunk_index": 0, "title": "Slide 37", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 29, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 37", "importance_score": 9, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 38, "chunk_index": 0, "title": "Slide 38", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.5260389116075304, "y": 0.1835564507378472, "width": 0.4392279307047526, "height": 0.675572984483507}}], "layout": {"num_text_boxes": 9, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 38", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 39, "chunk_index": 0, "title": "Slide 39", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.041346785757276745, "y": 0.24930447048611112, "width": 0.4392279280556573, "height": 0.675572984483507}}, {"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.523870340983073, "y": 0.24930426703559028, "width": 0.43403371175130206, "height": 0.6755731879340278}}], "layout": {"num_text_boxes": 2, "num_images": 2, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 39", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 40, "chunk_index": 0, "title": "Slide 40", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.041346785757276745, "y": 0.24930447048611112, "width": 0.4392279280556573, "height": 0.675572984483507}}, {"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.5252101050482856, "y": 0.25327901204427083, "width": 0.4334455278184679, "height": 0.6703981526692708}}], "layout": {"num_text_boxes": 2, "num_images": 2, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 40", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 41, "chunk_index": 0, "title": "Slide 41", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 11, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 41", "importance_score": 9, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 42, "chunk_index": 0, "title": "Slide 42", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 27, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 42", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 43, "chunk_index": 0, "title": "Slide 43", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 27, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 43", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 44, "chunk_index": 0, "title": "Slide 44", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.10140066146850586, "y": 0.8028916422526041, "width": 0.04841447406344944, "height": 0.072906494140625}}, {"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.1881571027967665, "y": 0.7382421196831597, "width": 0.024197090996636285, "height": 0.03643798828125}}], "layout": {"num_text_boxes": 29, "num_images": 2, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 44", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 45, "chunk_index": 0, "title": "Slide 45", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 7, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 45", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 46, "chunk_index": 0, "title": "Slide 46", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.05973399480183919, "y": 0.20186803181966145, "width": 0.04841447406344944, "height": 0.072906494140625}}], "layout": {"num_text_boxes": 9, "num_images": 1, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 46", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 47, "chunk_index": 0, "title": "Slide 47", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 7, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 47", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit04_Assembly", "slide_number": 48, "chunk_index": 0, "title": "Slide 48", "summary": "", "main_text": "", "notes_text": "", "keywords": [], "images": [{"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.09308315912882487, "y": 0.18760684543185763, "width": 0.022474532657199436, "height": 0.033843994140625}}, {"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.0533869743347168, "y": 0.22227349175347222, "width": 0.02247452735900879, "height": 0.033843994140625}}, {"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.09308315912882487, "y": 0.2791623942057292, "width": 0.022474532657199436, "height": 0.033843994140625}}, {"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.09308315912882487, "y": 0.37071797688802083, "width": 0.022474532657199436, "height": 0.033843994140625}}, {"description": "Diagram/figure illustrating the slide's concept (see slide for details).", "labels": [], "position": {"x": 0.6926372528076172, "y": 0.4126378038194444, "width": 0.2500000423855252, "height": 0.35}}], "layout": {"num_text_boxes": 17, "num_images": 5, "dominant_visual_type": "diagram"}, "metadata": {"course": "CS356", "unit": 4, "topic": "Slide 48", "importance_score": 6, "file_hash": "sha256_placeholder"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 1, "chunk_index": 0, "title": "Unit 5: Assembly Operations", "summary": "Unit 5: Assembly Operations", "main_text": "Unit 5: Assembly Operations\nx86-64 Instruction Set Architecture\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 2, "chunk_index": 0, "title": "Data Movement", "summary": "Data Movement", "main_text": "Data Movement\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 3, "chunk_index": 0, "title": "", "summary": "", "main_text": "\n\nmov Suffixes & Data Size\nByte operations only access the 1-byte at the specified address\n(Assume start address = A)\n\nByte\n\n63\n0\nWord\n15\nQuad Word\n63\n0\nmovb\nmovw\nmovl\n7\n0\n63\n0000 0000\n63\n0\nDouble Word\n31\nmovq\n7654 3210\nfedc ba98\nA+4\nA\n7654 3210\nfedc ba98\nA+4\nA\nWord operations access the 2-bytes starting at the specified address\n7654 3210\nfedc ba98\nA+4\nA\nWord operations access the 4-bytes starting at the specified address\n7654 3210\nfedc ba98\nA+4\nA\nWord operations access the 8-bytes starting at the specified address\nProcessor Register\nMemory / RAM\nmovl zeros the upper bits\nmovw leaves upper bits unaffected\nmovb leaves upper bits unaffected\nCS:APP 3.4.2\n\u2039#\u203a\nMoves data between memory and processor register\nAlways provide the LS-Byte address (little-endian) of the desired data\nSize is explicitly defined by the instruction suffix ('mov[bwlq]') used\nRecall:  Start address should be divisible by size of access", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 54, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 4, "chunk_index": 0, "title": "Transfer from Memory ", "summary": "Transfer from Memory ", "main_text": "Transfer from Memory \nInstruction format\nmov[b,w,l,q] src, dst\nInitial Conditions:\n\nExamples:\nmovb 0x207, %al\nmovw 0x202, %ax\nmovl 0x204, %eax\nmovq 0x200, %rax\n\n\n7654 3210\nfedc ba98\n0x00204\n0x00200\nffff ffff 1234 5678\nrax\n7654 3210 fedc ba98\nrax\nffff ffff 1234 5676\nrax\n0000 0000 7654 3210\nrax\nffff ffff 1234 fedc\nrax\nmovl zeros the upper bits of dest. reg\nMemory / RAM\nProcessor Register\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 20, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 5, "chunk_index": 0, "title": "Transfer to Memory", "summary": "Transfer to Memory", "main_text": "Transfer to Memory\nInstruction format\nmov[b,w,l,q] src, dst\nInitial Conditions:\n\n\nExamples:\nmovb %al, 0x4e5\n\nmovl %eax, 0x4e0\n0000 0000\n0000 0000\n0x00204\n0x00200\n0000 7800\n0000 0000\n0x004e4\n0x004e0\nffff ffff 1234 5678\nrax\n0000 7800\n1234 5678\n0x004e4\n0x004e0\nMemory / RAM\nProcessor Register\nmovl changes only 4 bytes here\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 20, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 6, "chunk_index": 0, "title": "Immediates must be source operand", "summary": "Immediates must be source operand\nIndica", "main_text": "Immediates must be source operand\nIndicate with '$' and can be specified in decimal (default) or hex (start with 0x)\nmovq can only support a 32-bit immediate (and will then sign-extend that value to fill the upper 32-bits)\nUse movabsq for a full 64-bit immediate value\nImmediate Values (Constants)\nExamples\n\nmovl    $0xfe1234, %eax\nmovw    $0xaa55, %ax\nmovb    $20, %al\nmovq    $-1, %rax\nmovabsq $0x123456789ab, %rax\nmovq    $-1, 0x4e0\n\n7654 3210\nfedc ba98\n0x00204\n0x00200\nffff ffff 1234 5678\nrax\n0000 0000 00fe 1234\nrax\nffff ffff ffff ffff\nrax\n0000 0000 00fe aa55\nrax\n0000 0000 00fe aa14\nrax\nffff ffff\nffff ffff\n0x004e4\n0x004e0\n0000 0123 4567 89ab\nrax\nMemory / RAM\nProcessor Register\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 26, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 7, "chunk_index": 0, "title": "Remember: Zero/Sign Extension", "summary": "Remember: Zero/Sign Extension", "main_text": "Remember: Zero/Sign Extension\n2\u2019s complement = Sign Extension (replicate sign bit):\nUnsigned = Zero Extension (always add leading 0\u2019s):\n111011 = 00111011\n011010 = 00011010\n110011 = 11110011\npositive\nnegative\nIncrease a 6-bit number to 8-bit number by zero extending\n\nSign bit is just repeated as many times as necessary\nExtension is the process of increasing the number of bits used to represent a number without changing its value\nWhy does it work?\u000b 111\u2026 = -128 + 64 + 32 = -32  and  1\u2026 = -32 \n", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 13, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 8, "chunk_index": 0, "title": "Zero / Sign Extension Moves", "summary": "Zero / Sign Extension Moves", "main_text": "Zero / Sign Extension Moves\nNormal mov does not affect upper portions of registers\u000b(with exception of movl)\nmovzxy will zero-extend the upper portion (up to size y)\nmovzbw (move a byte from the source but zero-extend it\u000bto a word in the destination register)\nmovzbw, movzbl, movzbq, movzwl, movzwq (but no movzlq!)\nmovsxy will sign-extend the upper portion (up to size y)\nmovsbw (move a byte from the source but sign-extend it\u000bto a word in the destination register)\nmovsbw, movsbl, movsbq, movswl, movswq, movslq\ncltq is equivalent to  movslq %eax,%rax  (but shorter encoding)\nNote: the destination must be a register\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 9, "chunk_index": 0, "title": "Zero/Signed Move Variations", "summary": "Zero/Signed Move Variations", "main_text": "Zero/Signed Move Variations\nInitial Conditions:\n\nmovl   0x200, %eax\nmovslq 0x200, %rax\nmovzwl 0x202, %eax\nmovsbw 0x201, %ax\nmovsbl 0x206, %eax\nmovzbq %dl,   %rax\n\n7654 3210\nfedc ba98\n0x00204\n0x00200\n0123 4567 89ab cdef\nrdx\nffff ffff fedc ba98\nrax\n0000 0000 0000 0054\nrax\n0000 0000 0000 fedc\nrax\n0000 0000 0000 ffba\nrax\n0000 0000 0000 00ef\nrax\nMemory / RAM\nTreat these instructions as a sequence where one affects the next.\n0000 0000 fedc ba98\nrax", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 22, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 10, "chunk_index": 0, "title": "Summary", "summary": "Summary", "main_text": "Summary\nAccess to different size portions of a register requires different names in x86-64\u000b(e.g. %al, %ax, %eax, %rax)\nMoving to a register may involve extension\n32-bit moves always set the upper 32 bits to 0\nMoving to memory never involves zero- or sign-extending since memory is broken into finer granularities", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 11, "chunk_index": 0, "title": "Why So Many Oddities & Variations", "summary": "Why So Many Oddities & Variations", "main_text": "Why So Many Oddities & Variations\n70s\n80s\n90s\nThe x86 instruction set has been around for nearly 40 years and each new processor has had to maintain backward compatibility (support the old instruction set) while adding new functionality\nIf you wore one clothing article from each decade you'd look funny too and have a lot of oddities", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 5, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 12, "chunk_index": 0, "title": "Addressing Modes", "summary": "Addressing Modes", "main_text": "Addressing Modes\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 13, "chunk_index": 0, "title": "What Are Addressing Modes", "summary": "What Are Addressing Modes", "main_text": "What Are Addressing Modes\nRecall an operand must be:\nAn immediate value (e.g., $0x42)\nA register value (e.g., %rax)\nA value from memory (e.g., 0x42)\nTo access a memory location \u000bwe must supply an address\u2026\nHowever, there can be many ways to select an address\nA constant address 0x1234\nThe pointer to an object, or an object member \u201cobj->member\u201d\nThe element of an array a[i]\nWays to specify an operand are known as addressing modes\nMem.\nInst.\nProc.\nA\nD\n...\nInst.\n400\nData\n401\nData\nReg.\nALU\n...\nReg.\n\n\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 20, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 14, "chunk_index": 0, "title": "Addressing Modes", "summary": "Addressing Modes", "main_text": "Addressing Modes\nDifferent ways to specify source values and output location.\nImmediate: $imm to use a constant input value, e.g., $0xFF or  $255\nRegister: %reg to use the value contained in a register, e.g., %rax \nMemory reference\nAbsolute: addr, e.g., 0x1122334455667788 [use a fixed address]\nIndirect: (%reg), e.g., (%rax) [use address contained in a q register]\nBase+displacement: imm(%reg), e.g., 16(%rax) [add a displacement]\nIndexed: (%reg1,%reg2), e.g., (%rax,%rbx) [add another register]\nIndexed+displacement: imm(%reg1,%reg2)  [add both]\nScaled indexed: imm(%reg1,%reg2,c)  [use address: imm+reg1+reg2*c]\u000bRestriction: c must be one of 1, 2, 4, 8\u000bVariants: omit imm or reg1 or both. E.g., (,%rax,4)\n(A memory reference specifies the first byte.)\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 15, "chunk_index": 0, "title": "x86-64 Addressing Modes", "summary": "x86-64 Addressing Modes", "main_text": "x86-64 Addressing Modes\n\u2020Known as the scale factor and can be {1,2,4, or 8}\nImm = Constant, R[x] = Content of register x, M[addr] = Content of memory @ addr.\nPurple values =   effective address   (EA) = Actual address used to get the operand \nCS:APP 3.4.1\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 4, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 16, "chunk_index": 0, "title": "Immediate Mode", "summary": "Immediate Mode", "main_text": "Immediate Mode\nSpecifies a constant stored in the instruction as the operand\nImmediate is denoted with '$' and can be in hex or decimal\n\n\n\n15\n0\n63\n7654 3210\nfedc ba98\nProcessor\nMemory / RAM\n0000 0000 1234 5678\nrax\n0000 0000 0000 0200\nrbx\n31\n0000 0000 0000 0002\nrcx\nffff ffff ffff 0005\nrdx\n0x00204\n0x00200\ncc55 aa33\n0x00208\nmovw  $5,  %dx \nIntruc\n\n\n\n\nSource is immediate mode, Destination is register mode\nInitial val. of %rdx = \nffff ffff ffff ffff\n\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 35, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 17, "chunk_index": 0, "title": "Register Mode", "summary": "Register Mode", "main_text": "Register Mode\nSpecifies the contents of a register as the operand\n\n\n\n15\n0\n63\n7654 3210\nfedc ba98\nProcessor\nMemory / RAM\n0000 0000 1234 5678\nrax\n0000 0000 0000 0200\nrbx\n31\n0000 0000 0000 0002\nrcx\n0000 0000 1234 5678\nrdx\n0x00204\n0x00200\ncc55 aa33\n0x00208\nmovq  %rax, %rdx \nIntruc\n\n\n\n\nBoth operands in this example are using Register Mode\nInitial val. of %rdx = \nffff ffff ffff ffff\n\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 35, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 18, "chunk_index": 0, "title": "Direct Addressing Mode", "summary": "Direct Addressing Mode", "main_text": "Direct Addressing Mode\nUse the value located at a constant memory address\u000b stored in the instruction\nAddress can be specified in decimal or hex\n\n\n\n15\n0\n63\n7654 3210\nfedc ba98\nProcessor\nMemory / RAM\n0000 0000 1234 5678\nrax\n0000 0000 0000 0200\nrbx\n31\n0000 0000 0000 0002\nrcx\nffff ffff ffff ff55\nrdx\n0x00204\n0x00200\ncc55 aa33\n0x00208\nmovb  0x20a, %dl \nIntruc\n\n\nSource is using Direct Addressing mode\nInitial val. of %rdx = \nffff ffff ffff ffff\n\n\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 34, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 19, "chunk_index": 0, "title": "Indirect Addressing Mode", "summary": "Indirect Addressing Mode", "main_text": "Indirect Addressing Mode\nForm: (%reg)         (parentheses indicate indirection)\nUse the value located at a memory address given by a register (similar to dereferencing a pointer in C, e.g., \u201c*ptr\u201d)\n\n\n15\n0\n63\n7654 3210\nfedc ba98\nProcessor\nMemory / RAM\n0000 0000 1234 5678\nrax\n0000 0000 0000 0200\nrbx\n31\n0000 0000 0000 0002\nrcx\n0000 0000 fedc ba98\nrdx\n0x00204\n0x00200\ncc55 aa33\n0x00208\nmovl  (%rbx), %edx \nIntruc\n\n\nSource is using Indirect Addressing mode\nInitial val. of %rdx = \nffff ffff ffff ffff\n\n\nEA=\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 35, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 20, "chunk_index": 0, "title": "Indirect with Displacement", "summary": "Indirect with Displacement", "main_text": "Indirect with Displacement\nForm: d(%reg)\nUse the value located at address \u201cregister value + constant\u201d\n\n\n\n15\n0\n63\n7654 3210\nfedc ba98\nProcessor\nMemory / RAM\n0000 0000 1234 5678\nrax\n0000 0000 0000 0200\nrbx\n31\n0000 0000 0000 0002\nrcx\nffff ffff ffff aa33\nrdx\n0x00204\n0x00200\ncc55 aa33\n0x00208\nmovw  8(%rbx), %dx \nIntruc\n\n\nSource is using Base with Displacement Addressing mode\nInitial val. of %rdx = \nffff ffff ffff ffff\n\n\n0000 0200\n+        8\n0000 0208\nEA=\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 36, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 21, "chunk_index": 0, "title": "Indirect with Displ. Example", "summary": "Indirect with Displ. Example", "main_text": "Indirect with Displ. Example\nUseful to access members of a struct or object\nstruct mystruct {\u000b  int x;\u000b  int y;\n};\nstruct mystruct data[3];\n\nint main()\n{\n  for(int i=0; i<3; i++){\u000b    data[i].x = 1;    \n    data[i].y = 2;\n  }\u000b}\nC Code\nmovq   $0x0200,%rbx\u000bloop 3 times {\n  movl  $1, (%rbx)\u000b  movl  $2, 4(%rbx)\u000b  addq  $8, %rbx\n}\u000b`\n\n0000 0001\n0000 0002\nMemory / RAM\n0x00210\n0x0020c\n0000 0002\n0x00214\n0000 0002\n0000 0001\n0x00204\n0x00200\n0000 0001\n0x00208\ndata[0].x\ndata[0].y\ndata[1].x\ndata[1].y\ndata[2].x\ndata[2].y\nAssembly\n0000 0000 0000 0200\nrbx\n0000 0200\n+        4\n0000 0204\nEA=\n0000 0000 0000 0208\n0000 0000 0000 0210\n1\n3\n4\n1\n3\n4\n2\n2\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 41, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 22, "chunk_index": 0, "title": "Base with Scaled Index Addressing Mode", "summary": "Base with Scaled Index Addressing Mode", "main_text": "Base with Scaled Index Addressing Mode\nForm: (%reg1,%reg2,s)  [s = 1, 2, 4, or 8]\nUses the result of  \u201c %reg1 + %reg2*s \u201d  as the effective address of the actual operand in memory\n\n\n15\n0\n63\n7654 3210\nfedc ba98\nProcessor\nMemory / RAM\n0000 0000 1234 5678\nrax\n0000 0000 0000 0200\nrbx\n31\n0000 0000 0000 0002\nrcx\n0000 0000 cc55 aa33\nrdx\n0x00204\n0x00200\ncc55 aa33\n0x00208\nmovl (%rbx,%rcx,4), %edx \nIntruc\n\n\nSource is using Scaled Index Addressing mode\nInitial val. of %rdx = \nffff ffff ffff ffff\n\n\n0000 0200\n+0000 0008\n0000 0208\nEA=\n*4\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 37, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 23, "chunk_index": 0, "title": "Base with Scaled Index Example", "summary": "Base with Scaled Index Example", "main_text": "Base with Scaled Index Example\nUseful for accessing array elements\nint data[6];\n\nint main()\n{\n  for(int i=0; i<6; i++){\u000b    data[i] = i;\n // *(startCharPtr+4*i) = i;    \n  }\u000b}\nC Code\nmovq   $0x0200,%rbx\nmovq   $0, %rcx\u000bloop 6 times {\n  movl  %ecx, (%rbx,%rcx,4)\u000b  addq  $1, %rcx\n}\u000b\n\n0000 0004\n0000 0003\nMemory / RAM\n0x00210\n0x0020c\n0000 0005\n0x00214\n0000 0001\n0000 0000\n0x00204\n0x00200\n0000 0002\n0x00208\ndata[0]\ndata[1]\ndata[2]\ndata[3]\ndata[4]\ndata[5]\nAssembly\n0000 0000 0000 0200\nrbx\n0000 0200\n+        0\n0000 0200\nEA=\n1\n2\n0000 0000 0000 0000\nrcx\n*4\n0000 0000 0000 0001\n0000 0000 0000 0002\n0000 0200\n+        4\n0000 0204\nEA=\n1\n2\n3\nArray of:\nchars/bytes => Use s=1\nshorts/words => Use s=2\nints/floats/dwords => Use s=4\nlong longs/doubles/qwords => Use s=8\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 44, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 24, "chunk_index": 0, "title": "Base and Scaled Index with Displacement", "summary": "Base and Scaled Index with Displacement", "main_text": "Base and Scaled Index with Displacement\nForm:  d(%reg1,%reg2,s)     [s = 1, 2, 4, or 8]\nUses the operand located at EA:   d + %reg1 + %reg2*s\n\n\n15\n0\n63\n7654 3210\nfedc ba98\nProcessor\nMemory / RAM\n0000 0000 1234 5678\nrax\n0000 0000 0000 0200\nrbx\n31\n0000 0000 0000 0002\nrcx\nffff ffff ffff ffcc\nrdx\n0x00204\n0x00200\ncc55 aa33\n0x00208\nmovb 3(%rbx,%rcx,4), %dl \nIntruc\n\n\nSource is using Scaled Index w/ Displacement Addressing mode\nInitial val. of %rdx = \nffff ffff ffff ffff\n\n\n0000 0200\n3\n+0000 0008\n0000 020b\nEA=\n*4\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 37, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 25, "chunk_index": 0, "title": "Addressing Mode Exercises", "summary": "Addressing Mode Exercises", "main_text": "Addressing Mode Exercises\n\n\n\nmovq (%rbx), %rax\nmovl -4(%rbx), %eax\nmovb (%rbx,%rcx), %al\nmovw (%rbx,%rcx,2), %ax\nmovsbl -16(%rbx,%rcx,4), %eax\nmovw %cx, 0xe0(%rbx,%rcx,2)\n\n7654 3210\nf00d face\n0x00200\n0x001fc\ncdef 89ab 7654 3210\nrax\n0000 0000 f00d cdef\nrax\n0000 0000 f00d face\nrax\n0000 0000 f00d fa76\nrax\n0000 0000\n0003 0000\n0x002e8\n0x002e4\n0000 0000 ffff ffce\nrax\n0000 0000 0000 0200\nrbx\ndead beef\n0x001f8\n0000 0000 0000 0003\nrcx\ncdef 89ab\n0x00204\nProcessor Registers\nMemory / RAM\nTreat these instructions as a sequence where one affects the next.\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 32, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 26, "chunk_index": 0, "title": "Quiz", "summary": "Quiz", "main_text": "Quiz\nConsider these in  movl \u2026 , %ebx\n%eax\n0x104\n$0x108\n(%rax)\n4(%rax)\n9(%rax,%rdx)\n260(%rcx,%rdx)\n0xFC(,%rcx,4)\n(%rax,%rdx,4)\n\n\n\nValues at each memory address:\n0x100: 0xFF\n0x104: 0xAB\n0x108: 0x13\n0x10C: 0x11\nValues in registers:\n%rax: 0x100\n%rcx: 0x1\n%rdx: 0x3\n\nWhat goes in  %ebx  is:\n0x100\n0xAB\n0x108\n0xFF\n0XAB\n0x11\n0x13\n0xFF\n0x11\n\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 6, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 27, "chunk_index": 0, "title": "Addressing Mode Examples", "summary": "Addressing Mode Examples", "main_text": "Addressing Mode Examples\nMain Memory\n1A 1B 1C 1D\n00 00 00 00\n1A 1B 1D 00\n7000\n7004\n7008\n\n\nAssume %rdx = 0x1111 2222 3333 4444\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 12, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 28, "chunk_index": 0, "title": "Instruction Limits on Addressing Modes", "summary": "Instruction Limits on Addressing Modes", "main_text": "Instruction Limits on Addressing Modes\nTo make the HW faster and simpler, there are restrictions on the combination of addressing modes\nAids overlapping the execution of multiple instructions\nNot allowed: memory locations for both operands\nmovl 2000, (%rax)        is not allowed \nmovl (%rax), (%rax)    is not allowed \naddl (%rax), (%rax)    is not allowed \nTo move mem->mem use two move instructions\u000bwith a register as the intermediate storage location\nLegal move combinations:\nImm -> Reg\nImm -> Mem\nReg -> Reg\nReg -> Mem\nMem -> Reg\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 29, "chunk_index": 0, "title": "Quiz: Spot the Mistake", "summary": "Quiz: Spot the Mistake", "main_text": "Quiz: Spot the Mistake\nThese are all wrong! Why?\n\n movb $0xF, (%ebx)\n movl %rax, (%rsp)\n movw (%rax), 4(%rsp)\n movb %al, %sl\n movq %rax, $0x123\n movl %eax, %rdx\n movb %si, 8(%rbp)\n\n\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 30, "chunk_index": 0, "title": "Arithmetic Operations", "summary": "Arithmetic Operations", "main_text": "Arithmetic Operations\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 31, "chunk_index": 0, "title": "At a glance", "summary": "At a glance", "main_text": "At a glance\nUnary (with q / l / w / b variants)\nincq x  is equivalent to    x++\ndecq x  is equivalent to    x--\nnegq x  is equivalent to    x = -x\nnotq x  is equivalent to    x = ~x\n\u000bBinary (with q / l / w / b variants)\naddq  x,y  is equivalent to    y += x\nsubq  x,y  is equivalent to    y -= x\nimulq x,y  is equivalent to    y *= x\nandq  x,y  is equivalent to    y &= x\n orq  x,y  is equivalent to    y |= x\nxorq  x,y  is equivalent to    y ^= x\nsalq  n,y  is equivalent to    y = y << n  n is $imm or  %cl (mod 32 or 64) \nsarq  n,y  is equivalent to    y = y >> n  arithmetic: fill in sign bit from left\nshrq  n,y  is equivalent to    y = y >> n  logical: fill in zeros from left\nAny instruction that generates a 32-bit value for a register also sets the high-order portion of the register to 0.\n\nExcept for right shift, all instructions are the same for signed/unsigned values (thanks to 2\u2019s-complement)\nimulq works for signed/unsigned but keeps only the least significant half\u000b of the result \u2026 more on this later\n\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 6, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 32, "chunk_index": 0, "title": "ALU Instructions", "summary": "ALU Instructions", "main_text": "ALU Instructions\nPerform arithmetic/logic operation on the given size of data\nRestriction: Both operands cannot be memory\nFormat\nadd[b,w,l,q] src2, src1/dst\nExamples\naddq %rbx, %rax  (%rax += %rbx)\n  subq %rbx, %rax  (%rax -= %rbx)\n\nWork from right->left->right\nCS:APP 3.5\n\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 7, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 33, "chunk_index": 0, "title": "Arithmetic/Logic Operations", "summary": "Arithmetic/Logic Operations", "main_text": "Arithmetic/Logic Operations\nInitial Conditions:\n\n\naddl $0x12300, %eax\n\naddq %rdx, %rax\n\nandw 0x200, %ax\n7654 3210\n0f0f ff00\n0x00204\n0x00200\nffff ffff 1234 5678\nrdx\n0000 0000 cc34 cd55\nrax\nffff ffff de69 23cd\nrax\nffff ffff de69 2300\nrax\nConsider the instructions as a sequence. Rules:\naddl, subl, etc. zero out the upper 32-bits\naddq, subq, etc. can only support a 32-bit immediate (they sign-extend that value to fill the upper 32-bits)\n0000 0000 cc33 aa55\nrax\nProcessor Registers\nMemory / RAM\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 20, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 34, "chunk_index": 0, "title": "Arithmetic/Logic Operations", "summary": "Arithmetic/Logic Operations", "main_text": "Arithmetic/Logic Operations\nInitial Conditions:\n\n\norb  0x203, %al\n\nsubw $14, %ax\n\naddl $0x12345, 0x204\n7654 3210\n0f0f ff00\n0x00204\n0x00200\nffff ffff 1234 5678\nrdx\nffff ffff de69 230f\nrax\nConsider the instructions as a sequence. Rules:\naddl, subl, etc. zero out the upper 32-bits\naddq, subq, etc. can only support a 32-bit immediate (they sign-extend that value to fill the upper 32-bits)\n7655 5555\n0f0f ff00\n0x00204\n0x00200\nffff ffff de69 2301\nrax\nffff ffff de69 2300\nrax\nProcessor Registers\nMemory / RAM\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 22, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 35, "chunk_index": 0, "title": "lea Instruction", "summary": "lea Instruction", "main_text": "lea Instruction\nRecall the exotic addressing modes supported by x86\n\n\nThe hardware has to support the calculation of the effective address (i.e., 3 adds + 1 mul [by 2, 4, or 8])\nMeanwhile normal add and mul instructions can only do 1 operation at a time\nIdea:  Create an instruction that can use the address calculation hardware but for normal arithmetic ops\nlea = Load Effective Address\nleaq 80(%rdx,%rcx,2),%rax  // %rax = 80+%rdx+2*%rcx\nComputes the \"address\" and just puts it in the destination for later\u000b(doesn't load anything from memory) \n\n\nCS:APP 3.5.1\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 4, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 36, "chunk_index": 0, "title": "lea Examples", "summary": "lea Examples", "main_text": "lea Examples\nInitial Conditions\n\nleal (%rdx,%rcx),%eax\n\nleaq -8(%rbx),%rax\n\nleaq 12(%rdx,%rcx,2),%rax\n\n\n\n0000 0089 1234 4000\nrdx\n0000 0000 1234 4020\nrax\nffff ffff ff00 02f8\nrax\n0000 0089 1234 404c\nrax\nCaveats:\nleal zeroes out the upper 32-bits\nffff ffff ff00 0300\nrbx\nProcessor Registers\n0000 0000 0000 0020\nrcx\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 18, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 37, "chunk_index": 0, "title": "About \u201coffset(x, y, multiplier)\u201d", "summary": "About \u201coffset(x, y, multiplier)\u201d", "main_text": "About \u201coffset(x, y, multiplier)\u201d\nrbx is all F\u2019s, so it\u2019s -1\u2026 we need to add -1*2+6 to rcx in this example to obtain the EA (applies to mov, lea, add...)\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 38, "chunk_index": 0, "title": "Optimization with lea", "summary": "Optimization with lea", "main_text": "Optimization with lea\n// x is stored inside %edi\n// return value in %eax\nint f1(int x)\n{\n  return 9*x+1;\n}\nf1:\n    movl  %edi,%eax   # tmp = x\n    sall  $3,  %eax   # tmp *= 8\n    addl  %edi,%eax   # tmp += x\n    addl  $1,  %eax   # tmp += 1\n    retq\nOriginal Code\nUnoptimized\nx86 Convention:  %edi/%rdi used for first argument, %eax/%rax used for return value\nf1:\n    leal  1(%rdi,%rdi,8)  ,%eax\n    retq\nOptimized With lea Instruction\n\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 10, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 39, "chunk_index": 0, "title": "Reusing addresses with  lea", "summary": "Reusing addresses with  lea", "main_text": "Reusing addresses with  lea\n// ptr is stored inside %rdi\nvoid incr(int *ptr) {\n  *ptr = *ptr + 2;\n}\n\n// data is stored inside %rdi\n// i    is stored inside %esi\nvoid f(int *data, int i) {\n  // uses address of data[i]\n  data[i] = 1;\n  // uses the same address\n  incr(&data[i]);  \n}\n\n\nincr:\n  // load int from address rdi,\n  // add 2 and store it back\n  movl    (%rdi), %eax\n  addl    $2, %eax\n  movl    %eax, (%rdi)\n  ret\n\nf:\n  // save addr of data[i] in rdi\n  movslq  %esi, %rsi\n  leaq    (%rdi,%rsi,4), %rdi\n  // write one to that address\n  movl    $1, (%rdi)\n  // pass same address to incr\n  call    incr\n  ret\n\n\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 5, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 40, "chunk_index": 0, "title": "Compiler Example 1", "summary": "Compiler Example 1", "main_text": "Compiler Example 1\n// data = %rdi\n// val  = %rsi\n// i    = %edx\nint f1(int data[], int *val, int i)\n{\n  int sum = *val;\n  sum += data[i];\n  return sum;\n}\nf1:\n    movslq %edx,%rdx\n    movl (%rdi,%rdx,4),%eax\n    addl (%rsi),%eax\n    retq\nOriginal Code\nCompiler Output\nx86 Convention:  Return value in %rax, inputs in %rdi, %rsi, %rdx\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 7, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 41, "chunk_index": 0, "title": "struct Data {", "summary": "struct Data {\n  char c;  // 1 byte\n  int", "main_text": "struct Data {\n  char c;  // 1 byte\n  int d;   // 4 bytes\n};\n\n// ptr  = %rdi\n// x    = %esi\nvoid f1(struct Data *ptr, int x)\n{\n  ptr->c++;\n  ptr->d -= x;\n}\nCompiler Output 2\nf1:\n    movzbl (%rdi),%eax\n    addl $0x1,%eax\n    movb %al,(%rdi)\n    subl %esi,0x4(%rdi)\n    retq\nOriginal Code\nCompiler Output\nx86 Convention:  Return value in %rax, inputs in %rdi, %rsi, %rdx\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 7, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 42, "chunk_index": 0, "title": "mov and add/sub examples", "summary": "mov and add/sub examples", "main_text": "mov and add/sub examples\n\n\n0x7004\n0x7000\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 6, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 43, "chunk_index": 0, "title": "Full Multiplication and Division", "summary": "Full Multiplication and Division", "main_text": "Full Multiplication and Division\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 44, "chunk_index": 0, "title": "Binary Multiplication", "summary": "Binary Multiplication", "main_text": "Binary Multiplication\nLike decimal multiplication, compute partial products, shift them, then tally up\n3-5x more expensive on modern CPUs\nMultiplying two n-bit numbers yields at most a  (2*n)-bit product \n  0 1 1 0\n*  0 1 0 1\n0 1 1 0\n(6)\n(5)\n\nSum of the partial products\n  0 0 0 0\n  0 1 1 0\n  + 0 0 0 0\n 0 0 1 1 1 1 0\nPartial Products\n\nThe way we extend these depends on signed/unsigned\u2026\nBut the lower half of the result is not affected\u2026 that\u2019s why \nimulq x,y works \u000bfor signed and unsigned\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 13, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 45, "chunk_index": 0, "title": "Instruction Variants", "summary": "Instruction Variants", "main_text": "Instruction Variants\n\u2039#\u203a\nSome instructions have many variants\nimulq %rax,%rbx  is equivalent to \u201crbx = rax*rbx\u201d\nOutput rbx has the same number of bits!\nIt\u2019s the same operation for signed/unsigned!\nimulq %rbx  is a different variant: \u201crdx:rax = rax*rbx\u201d\nOnly one argument, the other is always %rax\nMost significant 64 bits of the result in %rdx, \u000bLeast significant 64 bits in %rax\n%rdx is correct only for signed multiplication", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 46, "chunk_index": 0, "title": "x86 multiply", "summary": "x86 multiply", "main_text": "x86 multiply\nSince the result of a multiplication requires twice as many bits as the inputs, the x86 architecture splits the output across two registers (%rdx and %rax)\nFormat:   mul[l,q] src \t(Unsigned multiply)\u000b\t\t      imul[l,q] src \t(Signed multiply)\nOperation: \tLong   %edx:%eax = %eax * src (mull)\u000b\t\t          Quad   %rdx:%rax = %rax * src (mulq)\nImplicit 2nd operand is %eax or %rax\nResults is split across %edx:%eax (or %rdx:%rax)\nMSBs (Upper half) are saved to %edx (or %rdx)\nLSBs (Lower half) are saved in %eax (or %rax)\n\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 47, "chunk_index": 0, "title": "x86 multiply Examples", "summary": "x86 multiply Examples", "main_text": "x86 multiply Examples\nSince the result of a multiplication requires twice as many bits as the inputs, the x86 architecture splits the output across two registers (%rdx and %rax)\n\n0000 0000 0000 0010\n0000 0000 ABCD ABCD\neax\nmull %ebx\nebx\n0000 0000 0000 000A\n0000 0000 BCDA BCD0\neax\nedx\n0000 0010\n* ABCD ABCD\n0000 000A BCDA BCD0\n0000 0000 0000 0010\nABCD ABCD ABCD ABCD\nrax\nmulq %rbx\nrbx\n0000 0000 0000 000A\nBCDA BCDA BCDA BCD0\nrax\nrdx\n0000 0000 0000 0010\n* ABCD ABCD ABCD ABCD\n0000 0000 0000 000A BCDA BCDA BCDA BCD0\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 23, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 48, "chunk_index": 0, "title": "Binary Division", "summary": "Binary Division", "main_text": "Binary Division\nDividing two n-bit numbers may yield an \u000bn-bit quotient and n-bit remainder\nDivision operations on a modern processor can take 17-41 times longer than addition operations\n\n10\n1 0 1 1\n\n0 1 0 1 r.1\n-1 0\n0 1\n-0 0\n1 1\n-1 0\n0 1\n(2)10\n(11)10\n(5 r.1)10\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 16, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 49, "chunk_index": 0, "title": "Since an n-bit division produces two n-bit results (quotient and remainder), the x86 architecture splits the output across two registers (%rdx and %rax)", "summary": "Since an n-bit division produces two n-b", "main_text": "Since an n-bit division produces two n-bit results (quotient and remainder), the x86 architecture splits the output across two registers (%rdx and %rax)\nFormat:\tdiv[l,q] src \t(Unsigned divide)\u000b\t\t    idiv[l,q] src \t(Signed divide)\nOperation:   %eax = %edx:%eax / src  (divl)\u000b           %edx = %edx:%eax % src\u000b           %rax = %rdx:%rax / src  (divq)\u000b           %rdx = %rdx:%rax % src\t\t      \t\nImplicit dividend is in %edx:%eax (or %rdx:%rax)\nDivisor is specified as src 32-bit (or 64 bits)\nQuotient goes in %eax (%rax), remainder in %edx (%rdx)\nx86 division\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 50, "chunk_index": 0, "title": "x86 division Examples", "summary": "x86 division Examples", "main_text": "x86 division Examples\nSince the result of a division is twice as many bits as the input operands, the x86 architecture splits the output across two registers (%rdx and %rax)\n\n0000 0000 0000 000A\nedx\ndivl %ebx\n0000 0000 0000 0001\n0000 0000 ABCD ABCD\neax\nedx\nA BCDA BCD1\n/          0000 0010\nQuotient = ABCD ABCD\u000bRemainder =        1\n0000 0000 0000 000A\nrdx\ndivq %rbx\n0000 0000 0000 0001\nABCD ABCD ABCD ABCD\nrax\nrdx\nA BCDA BCDA BCDA BCD1\n/                    0000 0000 0000 0010\nQuotient =           ABCD ABCD ABCD ABCD\nRemainder =                            1\n0000 0000 BCDA BCD1\neax\nBCDA BCDA BCDA BCD1\nrax\n0000 0000 0000 0010\nebx\n0000 0000 0000 0010\nrbx\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 27, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 51, "chunk_index": 0, "title": "Assembly Translation", "summary": "Assembly Translation", "main_text": "Assembly Translation\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 52, "chunk_index": 0, "title": "Translation to Assembly", "summary": "Translation to Assembly", "main_text": "Translation to Assembly\nWe will now see some C code and its assembly translation\nA few things to remember:\nData variables live in memory: use stack for local variables\u000b(unless we don\u2019t use pointers and always keep them in registers)\nData must be brought into registers before being processed\nYou often need an address/pointer in a register to \u000bload/store data to/from memory\nGenerally, you will need 4 steps to translate C to assembly:\nSetup a pointer in a register\nLoad data from memory to a register (mov)\nProcess data (add, sub, and, or, shift, etc.)\nStore data back to memory (mov)\n\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 53, "chunk_index": 0, "title": "Translating HLL to Assembly", "summary": "Translating HLL to Assembly", "main_text": "Translating HLL to Assembly\nVariables are simply locations in memory\nA variable name really translates to an address in assembly\nPurple = Pointer init\nBlue = Read data from mem.\nRed = ALU op\nGreen = Write data to mem.\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 4, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356_Unit05_AssemblyOps", "slide_number": 54, "chunk_index": 0, "title": "Translating HLL to Assembly", "summary": "Translating HLL to Assembly", "main_text": "Translating HLL to Assembly\nPurple = Pointer init\nBlue = Read data from mem.\nRed = ALU op\nGreen = Write data to mem.\n\u2039#\u203a", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 5, "topic": "Assembly Operations", "importance_score": 5, "file_hash": "05db8c105d255638f7dde070ebf0b18005f5182c87cbabc170e093a4826c84b3"}}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 1, "chunk_index": 0, "title": "Unit 6: Stack & Procedures", "summary": "This slide introduces the main topic of the unit, which is the system stack and its use in procedures, specifically adhering to the Linux x86-64 Application Binary Interface (ABI) conventions.", "main_text": "Unit 6: Stack & Procedures. And their Linux x86-64 ABI conventions…", "notes_text": "", "keywords": ["Stack", "Procedures", "Assembly", "Linux", "x86-64", "ABI", "Conventions"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 2, "chunk_index": 0, "title": "Introduction to the System Stack", "summary": "This slide defines the system stack as a reserved memory region used by each process thread, noting its Last-In-First-Out (LIFO) policy and its primary uses for local function data and managing function calls.", "main_text": "System Stack. Each thread of a process (= running program) has its own stack: Just a reserved region of memory that 'grows toward lower addresses'. The special register %rsp points to the top of the stack. The policy is Last-In-First-Out (LIFO). Used for: Local data of functions (if registers are not sufficient), and Function calls (to save the 'return address').", "notes_text": "The stack is a fundamental data structure in program execution. The growth towards lower addresses is a key convention on x86-64 architecture, meaning pushing data decrements the stack pointer, and popping increments it.", "keywords": ["System Stack", "Thread", "Process", "Memory", "Lower Addresses", "%rsp", "Stack Pointer", "LIFO", "Local Data", "Function Calls", "Return Address"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 3, "chunk_index": 0, "title": "Stack Visualization and Growth Direction", "summary": "This slide visually depicts the stack within memory, illustrating that it grows towards lower memory addresses and is managed by the %rsp register, which points to the initial 'top' of the stack.", "main_text": "Stack visualization within Memory/RAM. The stack grows towards lower addresses. The register %rip (Instruction Pointer) and %rsp (Stack Pointer) are shown. Initial 'top' of the stack is indicated. Memory addresses are decreasing as the stack grows (e.g., from $0x7fffffff8$ down to $0x7fffffe0$).", "notes_text": "This diagram establishes the memory map convention for the stack, which is critical for understanding push and pop operations.", "keywords": ["Stack", "Memory", "RAM", "%rsp", "Stack Pointer", "Lower Addresses", "%rip", "Instruction Pointer", "Initial Top", "Visualization"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 4, "chunk_index": 0, "title": "The PUSH Operation (pushq %rax)", "summary": "This slide explains the x86-64 `pushq` assembly instruction, which allocates 8 bytes on the stack and copies a 64-bit register's content, equivalent to decrementing %rsp by 8 bytes and then storing the register's value to the new %rsp address.", "main_text": "Push operation: The instruction is `pushq %rax`. This allocates 8 bytes on the stack and then copies the content of register %rax. This is equivalent to: `subq $8,%rsp` (decrement %rsp by 8) and `movq %rax,(%rsp)` (save %rax to address %rsp). The visualization shows %rsp moving from $0x7fffffff8$ to $0x7fffffff0$ after the push, with the content of %rax ($1111 2222 3333 4444$) now stored on the stack.", "notes_text": "The pushq operation is a convenience instruction that combines the stack pointer adjustment and the memory write, always dealing with 8-byte (quad-word) values in x86-64.", "keywords": ["Push", "pushq", "%rax", "Allocate", "8 bytes", "Stack", "%rsp", "decrement", "subq", "movq", "Stack Pointer Adjustment", "Visualization"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 5, "chunk_index": 0, "title": "The POP Operation (popq %rdx)", "summary": "This slide explains the x86-64 `popq` assembly instruction, which copies the top 8 bytes of the stack into a 64-bit register (%rdx) and then deallocates them by incrementing %rsp, noting that the data is not actually erased from memory until overwritten.", "main_text": "Pop operation: The instruction is `popq %rdx`. This copies the top 8 bytes of the stack to register %rdx and then deallocates them. This is equivalent to: `movq (%rsp),%rdx` (save 8 bytes from address %rsp to register %rdx) and `addq $8,%rsp` (increment %rsp by 8). Note: pop does not erase the data on the stack; it simply moves the %rsp. The next push will overwrite the old value. The visualization shows the value ($1111 2222 3333 4444$) being moved from the stack to %rdx, and %rsp moving from $0x7fffffff0$ to $0x7fffffff8$.", "notes_text": "Like pushq, popq is a combined operation. The crucial point is that stack deallocation is a logical operation via the pointer (%rsp), not a physical memory clear.", "keywords": ["Pop", "popq", "%rdx", "Copy", "8 bytes", "Stack", "%rsp", "increment", "movq", "addq", "Deallocate", "Overwrite", "LIFO"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 6, "chunk_index": 0, "title": "Direct Stack Manipulation for Local Variables (Allocation/Deallocation)", "summary": "This slide discusses how the stack pointer, %rsp, is directly manipulated using `subq` and `addq` for allocating and deallocating blocks of memory larger than 8 bytes, which is common for local variables like buffers or arrays in functions.", "main_text": "Direct manipulation of %rsp is used to allocate more than 8 bytes on the stack, such as for a buffer (large array of data) or many local variables. The compiler can manage the stack without `pushq`/`popq` by: Decrementing %rsp (to allocate) and Incrementing %rsp (to deallocate). When a function completes, its local variables must be deallocated from the stack.\n\nC Code Example: `int some_procedure() { char buffer[24]; buffer[0] = 'A'; ... }`\n\nAssembly Implementation (excerpt):\n`some_procedure:`\n`// prologue: allocate 24 bytes`\n`subq $24,%rsp`\n`// write over the first byte`\n`movb $65,(%rsp)`\n`...`\n`// epilogue: deallocate 24 bytes`\n`addq $24, %rsp`\n`ret`", "notes_text": "This manual stack adjustment is the typical method used by compilers to create space for a function's stack frame components (like local variables) efficiently.", "keywords": ["Direct Manipulation", "%rsp", "subq", "addq", "Allocate", "Deallocate", "Buffer", "Local Variables", "Arrays", "Compiler Management", "Prologue", "Epilogue"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 7, "chunk_index": 0, "title": "Example: Reading an Integer (scanf) and Stack Usage", "summary": "This example shows the C and assembly code for a function that reads an integer using `scanf`, illustrating how the compiler uses `subq` to allocate stack space for a local variable and `leaq` to get its address for the system call.", "main_text": "C Code Example: `int read_int() { int temp; scanf(\"%d\", &temp); return temp; }`\n\nAssembly Code (read_int):\n`subq $24, %rsp` (Allocate 24 bytes on the stack)\n`leaq 12(%rsp), %rsi` (Calculate address of 'temp' at offset 12 from %rsp and load into %rsi for scanf)\n`leaq .LC0(%rip), %rdi` (Load address of format string \"%d\" into %rdi for scanf)\n`movl $0, %eax`\n`call scanf@PLT`\n`movl 12(%rsp), %eax` (Load value of 'temp' from stack into %eax for return)\n`addq $24, %rsp` (Deallocate 24 bytes)\n`ret`\n\nThe compiler allocates **24 bytes**, but only uses **4 bytes** (at offset 12) for the 4-byte `int temp` variable.", "notes_text": "The allocation of 24 bytes, more than the 4 bytes needed for 'temp', is often due to stack alignment requirements (e.g., 16-byte alignment on x86-64 before a call instruction). The `leaq` instruction is key to obtaining the stack address of a local variable for functions like `scanf` that require a memory address (using the `&` operator).", "keywords": ["scanf", "read_int", "subq", "leaq", "%rsp", "%rsi", "%rdi", "%eax", "Local Variable", "temp", "Stack Allocation", "Return Value"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 8, "chunk_index": 0, "title": "Example: Reading and Echoing a String (scanf/printf)", "summary": "This example demonstrates allocating a stack buffer for a string in assembly, using `scanf` to read input and `printf` to echo it, showing direct manipulation of %rsp to allocate the 16-byte stack space for the 10-byte buffer.", "main_text": "C Code Example: `void echo_str() { char buffer[10]; scanf(\"%s\", buffer); printf(\"echo: %s\\n\", buffer); }`\n\nAssembly Code (echo_str):\n`subq $16, %rsp` (Allocate 16 bytes)\n`leaq 6(%rsp), %rsi` (Load address of 'buffer' at offset 6 into %rsi for scanf)\n`leaq .LC0(%rip), %rdi`\n`movl $0, %eax`\n`call scanf@PLT`\n`leaq 6(%rsp), %rsi` (Reload address of 'buffer' into %rsi for printf)\n`leaq .LC1(%rip), %rdi`\n`movl $0, %eax`\n`call printf@PLT`\n`addq $16, %rsp` (Deallocate 16 bytes)\n`ret`\n\nThe compiler allocates **16 bytes**, using **10 bytes** (at offset 6) for the `char buffer[10]` variable. The diagram shows the string \"Trojans\" stored byte-by-byte in the allocated buffer on the stack.", "notes_text": "The buffer address is calculated using `leaq` and passed in %rsi (the second argument register in the x86-64 ABI). The need to reload the address for `printf` after the `scanf` call is due to %rsi being a caller-saved register, meaning `scanf` could have modified it.", "keywords": ["String", "Buffer", "scanf", "printf", "subq", "leaq", "%rsp", "%rsi", "Stack Allocation", "String Storage", "Caller-Saved Register"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 9, "chunk_index": 0, "title": "Stack and Procedure Calls", "summary": "This slide transitions to the role of the stack in procedure (function) calls by first recalling the function of the Instruction Pointer (%rip) in fetching the next instruction.", "main_text": "Stack & Procedure Calls. Remember: Fetching the next CPU instr. %rip is the address of the next instruction.", "notes_text": "", "keywords": ["Stack", "Procedure Calls", "%rip", "Instruction Pointer", "CPU", "Instruction Fetch"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 10, "chunk_index": 0, "title": "CPU Register and Instruction Fetch Overview", "summary": "This slide provides a high-level view of the CPU's registers and the process of fetching the next instruction from memory using the Instruction Pointer (%rip).", "main_text": "Registers: Examples of 64-bit register values are shown for %rax, %rbx, %rcx, %rdx, and %rip. The %rip register contains the address of the next instruction ($0x0000000000400050$). The Memory controller/Bus Interface is used to fetch the instruction from memory at the address specified by %rip. The Arithmetic Logic Unit (ALU) implements operations on registers (e.g., +, -, &, …).", "notes_text": "This contextualizes how control flow works, which is necessary before explaining the mechanics of a function call.", "keywords": ["CPU", "Registers", "%rax", "%rip", "Instruction Fetch", "Memory Controller", "ALU", "Instruction Address"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 11, "chunk_index": 0, "title": "Mechanism of Procedure Calls", "summary": "This slide defines the goal of a procedure call in assembly: to temporarily set %rip to the procedure's starting address and, upon completion, to restore %rip to the caller's next instruction (the return address).", "main_text": "Procedure Calls: To start a procedure (an assembly function), we want %rip (address of next instr.) == address of procedure code. After it, we want %rip == address of next instr. of caller (the return address).\n\nC Code Example: `int caller(int x) { int res = avg(x,4); res = res + 1; ... }`\n\nAssembly Example:\nCaller:\n`113b callq avg` (function call)\n`1140 addl $1,%rax` (next instruction, the desired return location)\n\nCallee (avg):\n`1125 ...`\n`1130 ret` (returns to the desired return location)", "notes_text": "The core problem of a procedure call is saving the return point so execution can seamlessly resume after the procedure completes. The return address is the address of the instruction immediately following the `callq` instruction.", "keywords": ["Procedure Calls", "%rip", "Address of Procedure", "Return Address", "callq", "ret", "Caller", "Callee", "Control Flow"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 12, "chunk_index": 0, "title": "Handling Nested Procedure Calls (LIFO Requirement)", "summary": "This slide demonstrates that nested procedure calls require a Last-In-First-Out (LIFO) mechanism to ensure that control flow correctly resumes the most recent caller after each return.", "main_text": "Nested Procedure Calls: We want to resume execution of the most recent caller! The nested structure of function calls (e.g., `caller` calls `avg`, and `avg` calls `div2`) mandates a mechanism that retrieves the return addresses in reverse order of the calls.\n\nC Code Example (Nested):\n`caller` -> `avg` -> `div2`\n\nAssembly Example:\nCaller:\n`113b callq avg` (1st func. call)\n`1140 addl $1,%rax` (1st return location)\n\navg:\n`1127 callq div2` (2nd func. call)\n`1129 ret` (2nd return location)\n\ndiv2:\n`1132 ret` (returns to 2nd return location)", "notes_text": "The LIFO policy of the stack is the natural solution for this problem, as the last function called is the first one to return.", "keywords": ["Nested Calls", "LIFO", "Resume Execution", "Return Address Order", "callq", "ret", "Caller", "Callee"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 13, "chunk_index": 0, "title": "Stack Implementation of Procedure Calls and Returns", "summary": "This slide details how x86-64 uses the stack to manage procedure calls: `call` pushes the return address onto the stack and jumps, while `ret` pops the address from the stack and jumps back to it, confirming the LIFO policy for nested calls.", "main_text": "Procedure Calls & Returns: x86-64 uses the stack to save the return address!\n\n`call f`: Push %rip (address of next instruction) to the stack, then write $f$ (address of the procedure) to %rip (Instruction Pointer).\n\n`ret`: Pop 8 bytes from the stack, write them to %rip.\n\nThe **LIFO policy** of the stack allows nested function calls, as required to resume execution of the most recent caller.", "notes_text": "The `call` instruction is essentially a combined `pushq <return address>; jmp <function address>` operation, and `ret` is a combined `popq %rip; jmp %rip` operation.", "keywords": ["call", "ret", "Return Address", "Stack", "Push", "Pop", "%rip", "Procedure Address", "LIFO Policy", "Nested Calls"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 14, "chunk_index": 0, "title": "Procedure Call Sequence 1a: Initial State", "summary": "This is the initial state before executing a `call avg` instruction, showing the stack pointer (%rsp) at $0x7fffffff8$, and the Instruction Pointer (%rip) pointing to the `call avg` instruction's return address at $0x40020$.", "main_text": "Example: Procedure Call Sequence 1a - Initial conditions: About to execute the `call avg` instruction. Current top of stack (%rsp) is at $0x7fffffff8$. The next instruction (the return address) is `addl $1,%rax` at address $0x40020$. The values in registers %rdi (8) and %rsi (4) are the arguments for `avg(8, 4)` ($0x0000000000000008$ and $0x0000000000000004$).", "notes_text": "This slide sets the stage for the four-step call/return demonstration.", "keywords": ["Procedure Call Sequence", "Initial State", "call avg", "%rsp", "%rip", "Return Address", "Registers", "%rdi", "%rsi"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 15, "chunk_index": 0, "title": "Procedure Call Sequence 1b: Executing 'call avg'", "summary": "This slide demonstrates the effect of the `call avg` instruction: the return address ($0x40020$) is pushed onto the stack, %rsp is decremented to $0x7fffffff0$, and %rip is updated to the procedure's start address ($0x40180$).", "main_text": "Example: Procedure Call Sequence 1b - `call` operation (i.e., push return address & jump):\n1. Decrement stack pointer (%rsp) by 8. (%rsp moves from $0x7fffffff8$ to $0x7fffffff0$).\n2. Push RA ($0x40020$) onto stack at the new %rsp address.\n3. Update PC (%rip) to start of procedure ($0x40180$).\n\nAfter the call, %rsp is $0x7fffffff0$ and the stack holds $0x0000000000040020$ (the return address).", "notes_text": "This step is the core mechanism of transferring control to a function while ensuring a path back.", "keywords": ["Procedure Call Sequence", "call avg", "Push Return Address", "Decrement %rsp", "Update %rip", "Stack State", "Jump"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 16, "chunk_index": 0, "title": "Procedure Call Sequence 1c: Executing the Procedure Code", "summary": "This slide shows the state after the called procedure (`avg`) has executed its logic, with the return value (6) placed in %rax, but before the `ret` instruction is executed.", "main_text": "Example: Procedure Call Sequence 1c - Execute the code for the procedure. The procedure code executes, calculating the average of the arguments (8 and 4) to be 6. At the end, the return value (6, or $0x0000000000000006$) should be in %rax/%eax. %rsp remains at $0x7fffffff0$, and %rip is at $0x40180$ (pointing to the `ret` instruction within the `avg` procedure).", "notes_text": "The procedure code execution only modifies general-purpose registers (like %rax for the return value) and potentially the stack for local variables (not shown here), but the return address is preserved at the top of the stack.", "keywords": ["Procedure Execution", "Return Value", "%rax", "avg", "ret instruction", "Intermediate State"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 17, "chunk_index": 0, "title": "Procedure Call Sequence 1d: Executing 'ret'", "summary": "This slide demonstrates the effect of the `ret` instruction: the return address ($0x40020$) is retrieved from the stack and written to %rip, and %rsp is incremented back to $0x7fffffff8$.", "main_text": "Example: Procedure Call Sequence 1d - `ret` operation (i.e., pop return address into %rip):\n1. Retrieve RA ($0x40020$) from stack at (%rsp).\n2. Put it in the PC (%rip).\n3. Increment the stack pointer (%rsp) by 8. (%rsp moves from $0x7fffffff0$ to $0x7fffffff8$).\n\nAfter `ret`, %rip is set to the return address $0x40020$, and %rsp is restored to its value before the `call`.", "notes_text": "The `ret` instruction uses the address pointed to by %rsp as the target for the jump and performs the stack cleanup by incrementing %rsp.", "keywords": ["Procedure Call Sequence", "ret", "Pop Return Address", "Increment %rsp", "Restore %rip", "Stack Cleanup", "Jump"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 18, "chunk_index": 0, "title": "Procedure Call Sequence 1e: Execution Resumes in Caller", "summary": "This final step in the call sequence shows execution resuming at the instruction following the original `call avg`, with the return value successfully transferred via %rax, ready for the caller to continue its work.", "main_text": "Example: Procedure Call Sequence 1e - Execution resumes fetching the next instruction after the procedure call. %rip is $0x40020$, pointing to the `addl $1,%rax` instruction. Note: %rax and other registers have changed! The return value in %rax (6) will be used by the `addl $1,%rax` instruction.", "notes_text": "The seamless transition of control flow is achieved by the combined push/pop operations of `call` and `ret` using the stack.", "keywords": ["Procedure Call Sequence", "Execution Resumes", "Next Instruction", "%rip", "%rax", "Return Value Transfer"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 19, "chunk_index": 0, "title": "Nested Call Example: Tracing Stack and Pointers", "summary": "This slide provides a more complex example of nested calls (`call SUB1` followed by `call SUB2`), tracing the values of the stack, %rsp, and %rip at four different timepoints to illustrate the LIFO mechanism in a real-world scenario.", "main_text": "Example: Tracing the values of the stack, %rsp, and %rip at various timestamps for the following nested code structure.\n\nCode Sequence:\n$0x40015$ `call SUB1`\n$0x4001A$ `...` (Return point 1)\n\n$0x40200$ `SUB1: ...`\n`call SUB2`\n$0x40208$ `...` (Return point 2)\n`ret`\n\n$0x40380$ `SUB2: ...`\n`ret`\n\n**State 1 (Before SUB1 Call):** %rip = $0x40015$, %rsp = $0x7fffffff8$, Stack is empty (above %rsp).\n\n**State 2 (After SUB1 Call, Before SUB2 Call):** %rip = $0x40200$ (Start of SUB1), %rsp = $0x7fffffff0$. Stack has RA1 ($0x4001A$) at $0x7fffffff0$.\n\n**State 3 (After SUB2 Call, In SUB2):** %rip = $0x40380$ (Start of SUB2), %rsp = $0x7fffffee8$. Stack has RA2 ($0x40208$) at $0x7fffffee8$ and RA1 ($0x4001A$) at $0x7fffffff0$.\n\n**State 4 (After SUB2 Return, In SUB1):** %rip = $0x40208$ (Return to SUB1), %rsp = $0x7fffffff0$. Stack has RA1 ($0x4001A$) at $0x7fffffff0$. RA2 has been popped.", "notes_text": "This tracing exercise visually confirms the LIFO behavior: RA2 is pushed after RA1 and popped before RA1, guaranteeing the correct flow back to the correct caller.", "keywords": ["Nested Calls", "Stack Trace", "%rsp", "%rip", "Return Address", "SUB1", "SUB2", "LIFO", "Stack State"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 20, "chunk_index": 0, "title": "Procedure Call Conventions", "summary": "This slide introduces the conventions used in x86-64 assembly for passing arguments to procedures and returning values, a critical part of the ABI.", "main_text": "Procedure Call Conventions. This section will cover the protocol used by the caller and callee for passing arguments and the return value.", "notes_text": "", "keywords": ["Procedure Call Conventions", "ABI", "x86-64", "Arguments", "Return Value", "Protocol"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 21, "chunk_index": 0, "title": "x86-64 Argument and Return Value Conventions", "summary": "This slide details the x86-64 ABI convention for passing arguments via registers (up to six) for speed and the stack for additional arguments, and for using the %rax register for the return value.", "main_text": "Arguments and Return Value: Caller and callee use this protocol.\n\n**Arguments (up to 6):** The first six arguments are passed using registers %rdi, %rsi, %rdx, %rcx, %r8, %r9 (very fast).\n\n**Additional Arguments (7+):** Additional arguments are pushed onto the stack in reversed order (slower).\n\n**Return Value:** The return value is saved into %rax.\n\nExample: `int avg(int a, int b)`: `a` (1st arg) uses %edi, `b` (2nd arg) uses %esi. The result returns in %eax.", "notes_text": "The use of registers for the first six arguments is a major optimization in x86-64 compared to older architectures that relied heavily on the stack for all argument passing.", "keywords": ["Arguments", "Return Value", "Registers", "%rdi", "%rsi", "%rdx", "%rcx", "%r8", "%r9", "%rax", "Stack", "ABI", "Fast", "Slow"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 22, "chunk_index": 0, "title": "Example: Passing Many Arguments (Registers and Stack)", "summary": "This example shows the C and assembly code for calling a function with eight arguments, illustrating how the first six arguments use registers and the last two (7 and 8) are pushed onto the stack in reverse order.", "main_text": "Example: Calling `f1(1, 2, 3, 4, 5, 6, 7, 8)` requires passing arguments via both registers and the stack. At point (1) (before the `call f1`), the 7th and 8th arguments are passed via the stack.\n\nCaller Assembly (Setup):\n`pushq $8` (Argument 8 is pushed first)\n`pushq $7` (Argument 7 is pushed second, making it 8 bytes closer to %rsp)\n`movl $6, %r9d` (Arg 6)\n... (Args 1-5 in %edi, %esi, %edx, %ecx, %r8d)\n`call f1`\n`addq $16, %rsp` (Clean up the 16 bytes for Args 7 & 8 after return)\n\nCallee Assembly (Access):\n`addl 8(%rsp), %eax` (Accesses Argument 7, which is 8 bytes from %rsp, after the return address)\n`addl 16(%rsp), %eax` (Accesses Argument 8, which is 16 bytes from %rsp)", "notes_text": "The stack addresses for arguments 7 and 8 are offsets relative to the %rsp *after* the return address has been pushed by the `call` instruction. Argument 7 is at $8(\\%rsp)$ and Argument 8 is at $16(\\%rsp)$. The caller is responsible for cleaning up the stack arguments using `addq $16, %rsp`.", "keywords": ["Many Arguments", "Stack Arguments", "Reverse Order", "Register Arguments", "Argument Access", "Stack Cleanup", "%rsp Offset", "Return Address"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 23, "chunk_index": 0, "title": "Caller-Saved and Callee-Saved Registers", "summary": "This slide explains the convention for preserving register values during a procedure call, dividing registers into caller-saved (volatile) and callee-saved (non-volatile) to avoid unnecessary stack operations.", "main_text": "Caller-Saved and Callee-Saved Registers: Caller and Callee share the same registers! The convention exists to avoid having the caller push/pop all registers to the stack before/after every call, which would be slow.\n\n**Callee-Saved Registers:** Values of %rsp, %rbp, %rbx, %r12 to %r15 must be **preserved by the callee**. The callee must either not modify them or save them to the stack at the beginning (prologue) and restore them at the end (epilogue) of its execution.\n\n**Caller-Saved Registers:** Values of all other registers can be **overwritten by the callee**. To preserve them, the caller must save them to callee-saved registers or push them to the stack before the call and pop them after the return.", "notes_text": "The choice of which registers are callee-saved is a crucial part of the ABI, designed to optimize common scenarios. Callee-saved registers are often used for holding values that need to persist across multiple function calls.", "keywords": ["Caller-Saved Registers", "Callee-Saved Registers", "Preservation", "Stack Operations", "ABI", "%rsp", "%rbp", "%rbx", "Volatile", "Non-Volatile", "Prologue", "Epilogue"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 24, "chunk_index": 0, "title": "Example: Callee-Saved Register Use (%rbx)", "summary": "This example shows the `echo_str` function revised to save the address of the local buffer into the callee-saved register %rbx, ensuring the address persists across the `scanf` call, which is allowed to overwrite caller-saved registers like %rsi.", "main_text": "Example: The function `echo_str` (C: `scanf(\"%s\", buffer); printf(\"echo: %s\\n\", buffer);`) passes the same argument `buffer` to both `scanf` and `printf`.\n\nAssembly:\n**Prologue**\n`pushq %rbx` (Save the caller’s value of %rbx, a callee-saved register)\n`subq $16, %rsp` (Allocate buffer)\n`leaq 6(%rsp), %rbx` (Save the address of `buffer` in the callee-saved register %rbx)\n\n**Body**\n`movq %rbx, %rsi` (Copy buffer address to %rsi for `scanf`)\n`call scanf@PLT` (%rsi might be changed by `scanf`)\n`movq %rbx, %rsi` (Copy buffer address from the safe %rbx back to %rsi for `printf`)\n`call printf@PLT`\n\n**Epilogue**\n`addq $16, %rsp` (Deallocate buffer)\n`popq %rbx` (Restore the caller’s value of %rbx)\n`ret`", "notes_text": "This demonstrates a practical use of a callee-saved register to hold a pointer that must remain constant across calls to other functions (callees). The prologue/epilogue structure handles the preservation and restoration of the callee-saved register.", "keywords": ["Callee-Saved Register", "%rbx", "Prologue", "Epilogue", "Save and Restore", "buffer address", "scanf", "printf", "leaq", "movq"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 25, "chunk_index": 0, "title": "Stack Frames and Their Components", "summary": "This slide summarizes all the types of data stored on the stack during a procedure call and introduces the concept of a 'stack frame' as a dedicated region on the stack for each active function.", "main_text": "Stack Frames: The stack is used to save: caller-saved registers (by caller), arguments after the 6th (by caller), the return address (by the call instruction/caller), callee-saved registers (by callee), and local variables (by the callee). All of this data is deallocated after a function call returns. At any time, the stack contains a **stack frame** for each active procedure call (i.e., function that hasn't returned yet) with its data.", "notes_text": "The diagram visually decomposes the stack into two frames: the 'Frame for calling function P' (caller's frame) and the 'Frame for executing function Q' (callee's frame), illustrating the LIFO nature of procedure execution.", "keywords": ["Stack Frames", "Active Procedure", "Return Address", "Saved Registers", "Local Variables", "Arguments", "Deallocation", "LIFO", "Caller Frame", "Callee Frame", "Argument Build Area"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 26, "chunk_index": 0, "title": "Accessing Data in Stack Frames by Offset", "summary": "This slide explains how data within a stack frame (local variables or stack arguments) is accessed using an offset relative to the stack pointer %rsp, and provides a full assembly example of caller and callee frame management.", "main_text": "Accessing Data in Stack Frames: Usually, an **offset** is added to %rsp to select the desired data on the stack (local variables or arguments). For example, if a callee has $24$ bytes of local variables and $8$ bytes for a saved %rbx, the offset for the 7th argument is $24 + 8 + 8 = 40$ bytes (skipping local vars, saved %rbx, and the return address).\n\n**Caller Assembly (Management):**\n`pushq $8` and `pushq $7` for arguments 7 and 8.\n`call f1`\n`addq $16, %rsp` to clean up args 7-8.\n\n**Callee Assembly (Access):**\n`pushq %rbx` (Save callee-saved register)\n`subq $24, %rsp` (Allocate local buffer)\n`addl 40(%rsp), %eax` (Read Argument 7, 40 bytes from %rsp)\n`addl 48(%rsp), %eax` (Read Argument 8, 48 bytes from %rsp)\n`addq $24, %rsp` and `popq %rbx` (Epilogue clean-up)", "notes_text": "The offsets are calculated from the current %rsp, which points to the top of the callee's frame. Positive offsets move toward higher addresses, accessing the caller's frame or the stack-passed arguments.", "keywords": ["Accessing Stack Data", "Offset", "%rsp", "Local Variables", "Stack Arguments", "Caller Frame", "Callee Frame", "addl offset(%rsp)", "Stack Management"]}
{"deck_name": "CS356Unit06_AssemblyStack", "slide_number": 27, "chunk_index": 0, "title": "Local Variables: Stack vs. Registers", "summary": "This slide outlines the conditions under which local variables are allocated on the stack versus being optimized into CPU registers, based on address requirements and the number of available registers.", "main_text": "Local Variables: Stack vs Registers. For simple integers/pointers, the compiler can optimize code by using a register rather than allocating the variable on the stack (`int y = 3` in a register). This is faster.\n\nLocal variables **MUST** be allocated on the stack if:\n1. No free registers (too many locals).\n2. The `&` (address-of) operator is used, requiring an actual memory address (e.g., `scanf(\"%d\", &x)`). The variable must be addressable.\n3. For arrays (since arrays always have an address).", "notes_text": "Compilers prioritize registers for performance, but certain language features (like taking a variable's address) force allocation on the stack, which is memory-addressable.", "keywords": ["Local Variables", "Stack vs Registers", "Optimization", "Address-of Operator", "&", "Arrays", "Memory Allocation", "Register Pressure", "Compiler"]}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 1, "chunk_index": 0, "title": "Unit 7: Conditional Flow", "summary": "Unit 7: Conditional Flow Jumps, Conditional Moves, Jump Tables", "main_text": "Unit 7: Conditional Flow\nJumps, Conditional Moves, Jump Tables", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 2, "chunk_index": 0, "title": "Jumps", "summary": "Jumps", "main_text": "Jumps", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 3, "chunk_index": 0, "title": "Remember: Fetching the next CPU instr.", "summary": "Remember: Fetching the next CPU instr. The CPU continuously fetches the next instruction at %rip registers 00 00 00 00 00 00 00 01 %rax 00 00 00 00 00…", "main_text": "Remember: Fetching the next CPU instr.\nThe CPU continuously fetches the next instruction at %rip\nregisters\n00 00 00 00 00 00 00 01\n%rax\n00 00 00 00 00 00 00 00\n%rbx\n11 22 33 44 55 66 77 88\n%rcx\n00 00 00 00 11 22 33 44\n%rdx\n00 00 00 00 00 40 00 50\n%rip\nMemory controller / Bus Interface\ncurrent instruction\nwe need to fetch an\ninstruction from memory\nArithmetic Logic Unit (ALU)\nimplements operations on\nregisters, e.g. +, -, &, …\nfetch one instruction at the given address\nread from 0x400050\ninstr. 48 89 c8", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 23, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 4, "chunk_index": 0, "title": "long f(long x) {", "summary": "long f(long x) { x += 3; if (x <= 5) { x += 7; } else { x += 9; } x += 10; return x; } Jumps ●…", "main_text": "long f(long x) {\nx += 3;\nif (x <= 5) {\nx += 7;\n} else {\nx += 9;\n}\nx += 10;\nreturn x;\n}\nJumps\n●\nAssembly instructions are executed\nsequentially by incrementing %rip\n●\nTo implement control structures\nsuch as “if/then/else” “while”\n“do/while”, and “for” we need to\nmodify %rip depending on some\nloop condition\nJump instructions modify %rip\n●\nUnconditional Jumps\njmp .L1\nJump to the code at address .L1\n●\nConditional Jumps\njg .L1\nSame, but only if “>” (greater) is satisfied\nfor the previous comparison (cmp, test)\nif (x <= 5)\nx += 3;\nx += 7;\nx += 9;\nx += 10;\nreturn x;\nFALSE\nTRUE\nf:\nleaq 3(%rdi), %rax\ncmpq $5, %rax\njg   .L2\naddq $7, %rax\njmp  .L3\n.L2:\naddq $9, %rax\n.L3:\naddq $10, %rax\nret", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 49, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 5, "chunk_index": 0, "title": "Jumps & %rip", "summary": "Jumps & %rip Jump instructions add an offset (pos/neg) to %rip (i.e. %rip = %rip + offset) registers 00 00 00 00 00 00 00 01 %rax 00 00 00…", "main_text": "Jumps & %rip\nJump instructions add an offset (pos/neg) to %rip\n(i.e. %rip = %rip + offset)\nregisters\n00 00 00 00 00 00 00 01\n%rax\n00 00 00 00 00 00 00 00\n%rbx\n11 22 33 44 55 66 77 88\n%rcx\n00 00 00 00 11 22 33 44\n%rdx\n00 00 00 00 00 40 00 50\n%rip\nMemory controller / Bus Interface\ncurrent instruction\n7f 06\njg .L2 really says: “add 6 to\n%rip if the > condition was\ntrue” (so that we skip 6 bytes\nof “then branch”)\nArithmetic Logic Unit (ALU)\nimplements operations on\nregisters, e.g. +, -, &, …\nfetch one instruction at the given address\nread from 0x400050\ninstr. 48 89 c8\n0:   48 8d 47 03\n4:   48 83 f8 05\n8:   7f 06\na:   48 83 c0 07\ne:   eb 04\n10:   48 83 c0 09\n14:   48 83 c0 0a\n18:   c3\nf:\nleaq 3(%rdi), %rax\ncmpq $5, %rax\njg   .L2\naddq $7, %rax\njmp  .L3\n.L2:\naddq $9, %rax\n.L3:\naddq $10, %rax\nret\nBinary Encoding\nOffset", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 48, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 6, "chunk_index": 0, "title": "Comparisons for Conditional Jumps", "summary": "Comparisons for Conditional Jumps // For: jg, jge, jle, jl (signed comparison >, >=, <=, <) // ja, jae, jbe, jb (unsigned comparison >, >=, <=, <) // je, jne…", "main_text": "Comparisons for Conditional Jumps\n// For: jg, jge, jle, jl   (signed comparison  >, >=, <=, <)\n//      ja, jae, jbe, jb (unsigned comparison  >, >=, <=, <)\n//      je, jne   (signed/unsigned comparison ==, !=)\n// jump if %rbx >= %rax\ncmpq %rax, %rbx\njge .L1\n// For: jz/je, jnz/jne (signed/unsigned ==, != 0)\n//      jg, jge/jns, jle, jl/js (signed  >, >=, <=, < 0)\n// jump if %rbx >= 0\ntestq %rbx, %rbx\njge .L1", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 12, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 7, "chunk_index": 0, "title": "BombLab", "summary": "BombLab .text .LC0: .string \"gandalf\\n\" .LC1: .string \"Success!\" phase1: leaq .LC0(%rip), %rsi callq strcmp testl %eax, %eax je .L1 movl $1, %eax .L1: ret explode_bomb: // ADD BREAKPOINT HERE! //…", "main_text": "BombLab\n.text\n.LC0: .string \"gandalf\\n\"\n.LC1: .string \"Success!\"\nphase1:\nleaq .LC0(%rip), %rsi\ncallq strcmp\ntestl %eax, %eax\nje .L1\nmovl $1, %eax\n.L1:\nret\nexplode_bomb:  // ADD BREAKPOINT HERE!\n// notifies our server\nret\n.globl main\nmain:\n// read string from stdin, save its addr in %rdi\ncall phase1\ntestl %eax, %eax\njne .L6\nleaq .LC1(%rip), %rdi\ncall puts\nmovl $0, %eax\nret\n.L6:\ncall explode_bomb\nmovl $0, %eax\nret\n#include <stdio.h>\n#include <string.h>\nint phase1(char *input) {\nif (strcmp(input, \"gandalf\\n\"))\nreturn 1;  // wrong input\nelse\nreturn 0;  // right input\n}\nvoid explode_bomb() {\n// notifies our server\n}\nint main() {\nchar input[200];\nfgets(input, 200, stdin);\nif (!phase1(input)) {\nputs(\"Success!\");\n} else {\nexplode_bomb();\n}\n}\nSkip next instruction if\nstrcmp returned 0\nGo to .L6 if phase1’s\nreturned nonzero value", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 53, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 8, "chunk_index": 0, "title": "Example 1", "summary": "Example 1 func1: cmpl %esi, %edi jge .L2 movl %edi, (%rdx) ret .L2: movl %esi, (%rdx) ret // x = %edi, y = %esi, res = %rdx void func1(int x,…", "main_text": "Example 1\nfunc1:\ncmpl    %esi, %edi\njge     .L2\nmovl    %edi, (%rdx)\nret\n.L2:\nmovl    %esi, (%rdx)\nret\n// x = %edi, y = %esi, res = %rdx\nvoid func1(int x, int y, int *res)\n{\nif ( x < y )\n*res = x;\nelse\n*res = y;\n}\nfunc2:\ncmpl    $-1, %edi\nje      .L6\ncmpl    $-1, %esi\nje      .L6\ntestl   %edi, %edi\njle     .L5\ncmpl    %esi, %edi\njle     .L5\naddl    $1, %edi\nmovl    %edi, (%rdx)\nret\n.L5:\nmovl    $0, (%rdx)\nret\n.L6:\nsubl    $1, %esi\nmovl    %esi, (%rdx)\nret\n// x = %edi, y = %esi, res = %rdx\nvoid func2(int x, int y, int *res)\n{\nif(x == -1 || y == -1)\n*res = y-1;\nelse if( x > 0 && y < x )\n*res = x+1;\nelse\n*res = 0;\n}\ngcc -S -Og func1.c\ngcc -S –O3 func2.c\nCS:APP 3.6.5", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 49, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 9, "chunk_index": 0, "title": "Example 2", "summary": "Example 2 func3: movl $0, %eax jmp .L2 .L3: addl $1, %eax .L2: movslq %eax, %rdx cmpb $0, (%rdi,%rdx) jne .L3 ret // str = %rdi int func3(char str[]) {…", "main_text": "Example 2\nfunc3:\nmovl    $0, %eax\njmp     .L2\n.L3:\naddl    $1, %eax\n.L2:\nmovslq  %eax, %rdx\ncmpb    $0, (%rdi,%rdx)\njne     .L3\nret\n// str = %rdi\nint func3(char str[])\n{\nint i = 0;\nwhile(str[i] != 0){\ni++;\n}\nreturn i;\n}\nfunc4:\nmovl    (%rdi), %eax\nmovl    $1, %edx\njmp     .L2\n.L4:\nmovslq  %edx, %rcx\nmovl    (%rdi,%rcx,4), %ecx\ncmpl    %ecx, %eax\njle     .L3\nmovl    %ecx, %eax\n.L3:\naddl    $1, %edx\n.L2:\ncmpl    %esi, %edx\njl      .L4\nret\n// dat = %rdi, len = %esi\nint func4(int dat[], int len)\n{\nint min = dat[0];\nfor (int i=1; i < len; i++) {\nif (dat[i] < min) {\nmin = dat[i];\n}\n}\nreturn min;\n}\ngcc -S -Og func3.c\ngcc -S -Og func4.c\nCS:APP 3.6.7", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 50, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 10, "chunk_index": 0, "title": "Condition Codes", "summary": "Condition Codes", "main_text": "Condition Codes", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 11, "chunk_index": 0, "title": "How jumps really work", "summary": "How jumps really work cmpq %rax, %rbx jge .L1 The CPU executes one instruction at a time… how does jge know the result of the previous cmpq instruction? cmp, test,…", "main_text": "How jumps really work\ncmpq %rax, %rbx\njge .L1\nThe CPU executes one instruction at a\ntime… how does jge know the result of\nthe previous cmpq instruction?\ncmp, test, and most instructions (based\non their result) update specific bits of the\nFLAGS register (checked by cond. jumps)\n●\nZF (Zero Flag): Tests if the result is equal to 0\n●\nSF (Sign Flag): Tests if the result is negative\n(copy of MSB of the result of the instruction)\n●\nOF (2’s complement Overflow Flag):\nSet if signed overflow has occurred\n●\nCF (Carry Flag == Unsigned Overflow):\nSet if unsigned overflow has occurred\nEFLAGS Reg\nCPU\nSF ZF\nCF\nOF\nsubl %edx, %eax\n%eax = 0x00000001\n%ecx = 0x80000000\nSF ZF\nCF\nOF\n%edx = 0x00000002\nCS:APP 3.6.1", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 33, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 12, "chunk_index": 0, "title": "cmp[bwql] src1, src2", "summary": "cmp[bwql] src1, src2 Used to compare src2 with src1 (e.g., src2==src1, src2<src1) Real meaning: Calculate ( src2–src1 ) and set the condition codes based on the result ● src1 and…", "main_text": "cmp[bwql] src1, src2\nUsed to compare src2 with src1\n(e.g., src2==src1, src2<src1)\nReal meaning:\nCalculate ( src2–src1 ) and set the\ncondition codes based on the result\n●\nsrc1 and src2 are not changed\n●\nIf (src2 == src1), ZF is set to 1\n●\nIf (src2 – src1) < 0, SF is set to 1\nIf (src2 – src1) overflows, OF = 1\n⇒ src2 < src1 if (SF ^ OF == 1)\nwhen src2 and src2 are signed\n●\nWhen src2 and src1 are unsigned,\nsrc2 < src1 if (CF == 1)\nFlags set by cmp and test\ntest[bwql] src1, src2\ntest[bwql] reg, reg\ntest[bwql]  $3, reg\nUsed to compare reg with 0, or to check\nwhether reg is a multiple of 22 (“3” is\nthe mask 11 in binary)\nReal meaning:\nCalculate ( src1 & src2 ), set ZF/SF\nbased on result, and set OF=CF=0\n●\nsrc1 and src2 are not changed\n●\nIn “test reg,reg”, ZF is set to 1 only\nwhen reg == 0, while SF is set to 1\nonly when reg < 0 (reg&reg == reg)\n●\nIn “test $3,reg”, ZF is set to 1 only\nwhen reg&3 == 0, i.e., multiple of 22", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 37, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 13, "chunk_index": 0, "title": "Conditional Jumps & FLAGS register", "summary": "Conditional Jumps & FLAGS register Instruction Synonym Jump Condition Description jmp label jmp *(Operand) je label jz ZF Equal / zero jne label jnz ~ZF Not equal / not zero…", "main_text": "Conditional Jumps & FLAGS register\nInstruction\nSynonym\nJump Condition\nDescription\njmp label\njmp *(Operand)\nje  label\njz\nZF\nEqual / zero\njne label\njnz\n~ZF\nNot equal / not zero\njs  label\nSF\nNegative\njns label\n~SF\nNon-negative\njg  label\njnle\n~(SF ^ OF) & ~ZF\nGreater (signed >)\njge label\njnl\n~(SF ^ OF)\nGreater or Equal (signed >=)\njl  label\njnge\n(SF ^ OF)\nLess (signed <)\njle label\njng\n(SF ^ OF) | ZF\nLess of equal (signed <=)\nja  label\njnbe\n~CF & ~ZF\nAbove (unsigned >)\njae label\njnb\n~CF\nAbove or equal (unsigned >=)\njb  label\njnae\nCF\nBelow (unsigned <)\njbe label\njna\nCF | ZF\nBelow or equal (unsigned <=)\nReminder:  For all jump instructions other than jmp (which is unconditional), some previous instruction (cmp,\ntest, etc.) is needed to set the condition codes to be examined by the jmp\nCS:APP 3.6.3", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 56, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 14, "chunk_index": 0, "title": "Exercise", "summary": "Exercise – addl $0x7fffffff,%edx – andb %al, %bl – addb $0xff, %al – cmpw $0x7000, %cx 0000 0000 0000 0001 rax 0000 0000 0000 0000 rbx 0000 0000 0000 8801…", "main_text": "Exercise\n– addl $0x7fffffff,%edx\n– andb %al, %bl\n– addb $0xff, %al\n– cmpw $0x7000, %cx\n0000 0000 0000 0001\nrax\n0000 0000 0000 0000\nrbx\n0000 0000 0000 8801\nrcx\n0000 0000 0000 0002\nrdx\nEFLAGS Reg\n?\n?\n?\n?\nSF ZF\nCF\nOF\nSF ZF\nCF\nOF\nSF ZF\nCF\nOF\nSF ZF\nCF\nOF\n0000 0000 8000 0001\nrdx\n0000 0000 0000 0000\nrbx\n0000 0000 0000 0000\nrax\nSF ZF\nCF\nOF\n0000 0000 0000 8801\nrcx\n0000 0000 0000 1801\nresult\nProcessor Registers", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 44, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 15, "chunk_index": 0, "title": "Condition Codes: Move & Bitwise Ops", "summary": "Condition Codes: Move & Bitwise Ops • mov and lea instructions leave the condition codes unaffected • Logical instructions – and, or, xor update only SF and ZF based on…", "main_text": "Condition Codes: Move & Bitwise Ops\n• mov and lea\ninstructions leave the\ncondition codes\nunaffected\n• Logical instructions\n–\nand, or, xor update only\nSF and ZF based on the\nresult and clear CF and\nOF to 0\n–\nnot does not affect\nthe condition codes\nin any way\nEFLAGS\nZF\nSF\n0000 0000 8000 0001\nrdx\n0000 0000 8000 0000\nrdx\nmovw $0,%dx\n0000 0000 7FFF FFFF\nrdx\nleaq -1(%rdx),%rdx\nOF\nZF\nSF\n0000 0000 8000 FF33\nrdx\n0000 0000 8000 FF00\nrdx\nandb $0xcc,%dl\n0000 0000 8000 FF80\nrdx\norb $0x80,%dl\n0000 0000 8000 FF7F\nrdx\nnotb %dl", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 40, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 16, "chunk_index": 0, "title": "Condition Codes: Shifts", "summary": "Condition Codes: Shifts 1 01... • SF = (copy of MSB) • ZF = (1 if and only if result is 0) • CF = (last bit shifted out of…", "main_text": "Condition Codes: Shifts\n1 01...\n•\nSF = (copy of MSB)\n•\nZF = (1 if and only if result is 0)\n•\nCF = (last bit shifted out of the input)\n•\nOF = undef for shifts of more than 1 bit; shifts by 1-bit work as follows…\n–\nLeft shifts (Logical or Arithmetic) by 1-bit\nOF = 1 if MSB (sign bit) changed (i.e. CF ^ MSB(result)); 0, otherwise.\n–\nRight shifts by 1-bit\nLogical: OF is set with the ORIGINAL MSB of the input value\nArithmetic: OF is always set to 0\n10   . . .   001\nSHL & SAL\nSHR\n1 0  . . .   001\nCopies of\nMSB are\nshifted in\nCF\n0 1...\nOF\nSF\nCF\nCF OF SF\n01   . . .    00\n1 1 0\nOF SF\nSAR\n1 1  . . .    00\n1 0 1\nresult\ninput\nMSB\nchanged\ninput\nresult\ninput\nresult", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 44, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 17, "chunk_index": 0, "title": "Exercise", "summary": "Exercise – shlw $1,%dx – shrb $1, %bl – sarb %cl, %al – shrb %cl, %bl 0000 0000 ff00 f0f6 rax 0000 0000 0000 018a rbx Processor Registers 0000 0000…", "main_text": "Exercise\n– shlw $1,%dx\n– shrb $1, %bl\n– sarb %cl, %al\n– shrb %cl, %bl\n0000 0000 ff00 f0f6\nrax\n0000 0000 0000 018a\nrbx\nProcessor Registers\n0000 0000 0000 0002\nrcx\n0000 0000 1234 8000\nrdx\nEFLAGS Reg\n?\n?\n?\n?\nSF ZF\nCF\nOF\nSF ZF\nCF\nOF\nSF ZF\nCF\nOF\n?\nSF ZF\nCF\nOF\n0000 0000 1234 0000\nrdx\n0000 0000 0000 0145\nrbx\n0000 0000 ff00 f0fd\nrax\n?\nSF ZF\nCF\nOF\n0000 0000 0000 0122\nrbx", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 44, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 18, "chunk_index": 0, "title": "Exercise", "summary": "Exercise 0000 0000 0000 0001 rax 0000 0000 0000 0002 rbx 0000 0000 ffff fffe rcx 0000 0000 0000 0000 rdx SF ZF CF OF f1: testl %edx, %edx je…", "main_text": "Exercise\n0000 0000 0000 0001\nrax\n0000 0000 0000 0002\nrbx\n0000 0000 ffff fffe\nrcx\n0000 0000 0000 0000\nrdx\nSF ZF CF\nOF\nf1:\ntestl %edx, %edx\nje    L2\nL1: cmpw  %bx, %ax\njge   L3\nL2: addl  $1,%ecx\njs    L1\nL3: ret\nOrder:\n__1__\n__2__\n__5___\n__6___\n__3,7_\n__4,8_\n____9_\nReminder:  je jumps if ZF, jge jumps if ~(SF ^ OF), js jumps if SF", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 28, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 19, "chunk_index": 0, "title": "Conditional Moves", "summary": "Conditional Moves", "main_text": "Conditional Moves", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 20, "chunk_index": 0, "title": "Pipelining: Modern CPUs overlap the execution", "summary": "Pipelining: Modern CPUs overlap the execution of multiple instructions ● While an instruction is being executed (1) … ● The next one has already been fetched and is being decoded…", "main_text": "Pipelining: Modern CPUs overlap the execution\nof multiple instructions\n●\nWhile an instruction is being executed (1) …\n●\nThe next one has already been fetched and is being\ndecoded (2) …\n●\nAnd the one after that is being fetched from memory\n(3) …\nSpeculative execution: Conditional jumps force\nthe CPU to guess what instruction will be next\n●\nIf the guess is right, we get good performance\n●\nIf the guess is wrong, we have to throw away the\nwrongly fetched/decoded instructions once we\nrealize the jump was mispredicted\nThe Cost of Conditional Jumps\nfunc1:\ncmpl    $-1, %edi\nje      .L6\ncmpl    $-1, %esi\nje      .L6\ntestl   %edi, %edi\njle     .L5\ncmpl    %esi, %edi\njl      .L5\naddl    $1, %edi\nmovl    %edi, (%rdx)\nret\n.L5:\nmovl    $0, (%rdx)\nret\n.L6:\nsubl    $1, %esi\nmovl    %esi, (%rdx)\nret\nCS:APP 3.6.6\ntime\nfetch\ndecode\nexecute\nfetch\ndecode\nexecute\nfetch\ndecode", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 48, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 21, "chunk_index": 0, "title": "Idea of Conditional Moves", "summary": "Idea of Conditional Moves Be more pipelining friendly: ● Compute both results (sequentially, no jumps) ● Keep the correct result when the condition is known Allows for pure sequential execution…", "main_text": "Idea of Conditional Moves\nBe more pipelining friendly:\n●\nCompute both results\n(sequentially, no jumps)\n●\nKeep the correct result\nwhen the condition is known\nAllows for pure sequential execution\n●\nWith jumps, we had to choose\nwhich instruction to fetch next\n●\nWith conditional moves, we only\nneed to choose whether to save\nor discard a computed result\ncmove1:\ncmpl    $5, %edi\njle     .L2\naddl    $1, %edi\nmovl    %edi, (%rsi)\nret\n.L2:\nsubl    $1, %edi\nmovl    %edi, (%rsi)\nret\ncmove1:\nleal    1(%rdi), %edx\nleal    -1(%rdi), %eax\ncmpl    $6, %edi\ncmovge  %edx, %eax\nmovl    %eax, (%rsi)\nret\nint cmove1(int x, int* res) {\nif(x > 5) {\n*res = x+1;\n} else {\n*res = x-1;\n}\nC Code\nWith Jumps (-Og Optimization)\nWith Conditional Moves\n(-O3 Optimization)\nint cmove1(int x) {\nint then_val = x+1;\nint temp = x-1;\nif(x > 5) {\ntemp = then_val;\n}\n*res = temp;\n}\nEquivalent C code", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 52, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 22, "chunk_index": 0, "title": "cmov[condition] src, dst", "summary": "cmov[condition] src, dst Used to copy src to dst if condition is satisfied (otherwise, do nothing) ● condition can be any of the conditions used in jumps: g, ge, le,…", "main_text": "cmov[condition] src, dst\nUsed to copy src to dst if condition is\nsatisfied (otherwise, do nothing)\n●\ncondition can be any of the\nconditions used in jumps:\ng, ge, le, l, s, ns (signed)\na, jae, jbe, jb (unsigned)\ne, ne, z, nz (both)\n●\nTransfer size inferred from\nregister name\n●\nDestination must be a register\nConditional Move Instruction\nv = then-expr;\nres = else-expr;\n// cmov in assembly:\nif (test-expr) {\nres = v;\n}\nif (test-expr)\nres = then-expr;\nelse\nres = else-expr;", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 25, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 23, "chunk_index": 0, "title": "Similar to Conditional Jumps", "summary": "Similar to Conditional Jumps Instruction Synonym Jump Condition Description cmove reg1,reg2 cmovz ZF Equal / zero cmovne reg1,reg2 cmovnz ~ZF Not equal / not zero cmovs reg1,reg2 SF Negative cmovns…", "main_text": "Similar to Conditional Jumps\nInstruction\nSynonym\nJump Condition\nDescription\ncmove  reg1,reg2\ncmovz\nZF\nEqual / zero\ncmovne reg1,reg2\ncmovnz\n~ZF\nNot equal / not zero\ncmovs  reg1,reg2\nSF\nNegative\ncmovns reg1,reg2\n~SF\nNon-negative\ncmovg  reg1,reg2\ncmovnle\n~(SF ^ OF) & ~ZF\nGreater (signed >)\ncmovge reg1,reg2\ncmovnl\n~(SF ^ OF)\nGreater or Equal (signed >=)\ncmovl  reg1,reg2\ncmovnge\n(SF ^ OF)\nLess (signed <)\ncmovle reg1,reg2\ncmovng\n(SF ^ OF) | ZF\nLess of equal (signed <=)\ncmova  reg1,reg2\ncmovnbe\n~CF & ~ZF\nAbove (unsigned >)\ncmovae reg1,reg2\ncmovnb\n~CF\nAbove or equal (unsigned >=)\ncmovb  reg1,reg2\ncmovnae\nCF\nBelow (unsigned <)\ncmovbe reg1,reg2\ncmovna\nCF | ZF\nBelow or equal (unsigned <=)\nReminder: Some previous instruction (cmp, test, etc.) is needed to set the condition codes to be\nexamined by the cmov", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 53, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 24, "chunk_index": 0, "title": "Example", "summary": "Example – cmpl $8,%edx – cmovl %ecx,%edx – testq %rax,%rax – cmove %rcx,%rax 0000 0000 0000 0001 rax 0000 0000 0000 0000 rbx Processor Registers 0000 0000 0000 8801 rcx…", "main_text": "Example\n– cmpl  $8,%edx\n– cmovl %ecx,%edx\n– testq %rax,%rax\n– cmove %rcx,%rax\n0000 0000 0000 0001\nrax\n0000 0000 0000 0000\nrbx\nProcessor Registers\n0000 0000 0000 8801\nrcx\n0000 0000 0000 0002\nrdx\n0000 0000 0000 8801\nrdx\nSF ZF CF\nOF\nImportant Notes:\n•\nNo size modifier is added to cmov, but instead the register names specify the size\n•\nByte-size conditional moves are not supported (only 16-, 32- or 64-bit conditional moves)\n0000 0000 0000 0001\nrax", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 25, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 25, "chunk_index": 0, "title": "●", "summary": "● If code in then and else have side effects then executing both would violate the original intent ● If large amounts of code in then or else branches, then…", "main_text": "●\nIf code in then and else have side\neffects then executing both would\nviolate the original intent\n●\nIf large amounts of code in then\nor else branches, then doing both\nmay be more time consuming\nLimitations of Conditional Moves\nint z = 100;\nint badcmove1(int x, int y) {\nint res;\nif(x > 5) {\nres = z++; // side effect\n} else {\nres = y;\n}\nreturn res;\n}\nvoid badcmove2(int x, int y) {\nint z;\nif(x > 5) {\n/* Lots of code */\n}\nelse {\n/* Lots of code */\n}\n}\nC Code", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 29, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 26, "chunk_index": 0, "title": "Jump Tables", "summary": "Jump Tables", "main_text": "Jump Tables", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 27, "chunk_index": 0, "title": "Switch with many direct jumps", "summary": "Switch with many direct jumps switch1: movl %edi, %eax andl $7, %eax // eax has x%8 cmpl $1, %eax je .L2 // jumps if x%8==1 cmpl $2, %eax je .L3…", "main_text": "Switch with many direct jumps\nswitch1:\nmovl\n%edi, %eax\nandl\n$7, %eax    // eax has x%8\ncmpl\n$1, %eax\nje\n.L2         // jumps if x%8==1\ncmpl\n$2, %eax\nje\n.L3         // jumps if x%8==2\ntestl   %eax, %eax\nje\n.L6         // jumps if x%8==0\naddl\n$7, %edi\nmovl\n%edi, (%rsi)\nret\n.L6:\naddl\n$5, %edi\nmovl\n%edi, (%rsi)\nret\n.L2:\nsubl\n$3, %edi\nmovl\n%edi, (%rsi)\nret\n.L3:\naddl\n$12, %edi\nmovl\n%edi, (%rsi)\nret\nvoid switch1 (unsigned x, int *res)\n{\nswitch (x % 8) {\ncase 0:\n*res = x+5;\nbreak;\ncase 1:\n*res = x-3;\nbreak;\ncase 2:\n*res = x+12;\nbreak;\ndefault:\n*res = x+7;\nbreak;\n}\n}\nCS:APP 3.6.8\nCase 0\nCase 1\nCase 2\nDefault\ngcc -Og -S switch.c", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 63, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 28, "chunk_index": 0, "title": "Switch with Jump Tables", "summary": "Switch with Jump Tables .text // start a code block .globl switch2 switch2: // save x%8 into eax movl %edi, %eax andl $7, %eax // save addr of table to…", "main_text": "Switch with Jump Tables\n.text  // start a code block\n.globl  switch2\nswitch2:\n// save x%8 into eax\nmovl  %edi, %eax\nandl  $7, %eax\n// save addr of table to rdx\nleaq  .L4(%rip), %rdx\n// use eax as an index to read\n// an entry (offset) from table\nmovslq  (%rdx,%rax,4), %rax\n// add entry to addr of table\naddq  %rdx, %rax\n// jump to: table addr + entry\njmp  *%rax\n.section .rodata  // data block\n.align 4\n.align 4\n// table of long words (4 bytes)\n// each entry has an offset from\n// .L4, the addr of the table\n.L4:\n.long  .L11-.L4\n.long  .L10-.L4\n.long  .L9-.L4\n.long  .L8-.L4\n.long  .L7-.L4\n.long  .L6-.L4\n.long  .L5-.L4\n.long  .L3-.L4\n// x = %edi, res = %rsi\nvoid switch2(unsigned x,\nint *res)    {\nswitch(x % 8) {\ncase 0:\n*res = x+5;\nbreak;\ncase 1:\n*res = x-3;\nbreak;\ncase 2:\n*res = x+12;\nbreak;\ncase 3:\n*res = x+7;\nbreak;\ncase 4:\n*res = x+5;\nbreak;\ncase 5:\n*res = x-3;\nbreak;\ncase 6:\n*res = x+12;\nbreak;\ncase 7:\n*res = x+7;\nbreak;\n}\n}\n.text // start a code block\n.L11: // at .L4+table[0]\naddl  $5, %edi\nmovl  %edi, (%rsi)\nret\n.L0:  // at .L4+table[1]\nsubl  $3, %edi\nmovl  %edi, (%rsi)\nret\n.L9:  // at .L4+table[2]\naddl  $12, %edi\nmovl  %edi, (%rsi)\nret\n.L8:  // at .L4+table[3]\naddl  $7, %edi\nmovl  %edi, (%rsi)\nret\n.L7:  // at .L4+table[4]\naddl  $5, %edi\nmovl  %edi, (%rsi)\nret\n.L6:  // at .L4+table[5]\nsubl  $3, %edi\nmovl  %edi, (%rsi)\nret\n.L5:  // at .L4+table[6]\naddl  $12, %edi\nmovl  %edi, (%rsi)\nret\n.L3:  // at .L4+table[7]\naddl  $7, %edi\nmovl  %edi, (%rsi)\nret\n.L11-.L4\n.L10-.L4\n.L9-.L4\n.L8-.L4\n.L7-.L4\n.L6-.L4\n.L5-.L4\n.L3-.L4\n.L4\nJump Tables\njmp  *%rax\nJumps to address\ngiven by rax\n%rax\nEqual to\n“address of table\n+ table[x%8]”\ngcc -Og -S switch.c", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 112, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 29, "chunk_index": 0, "title": ".text  // start a code block", "summary": ".text // start a code block .globl switch2 switch2: // save x%8 into eax movl %edi, %edx sarl $31, %edx shrl $29, %edx leal (%rdi,%rdx), %eax andl $7, %eax subl…", "main_text": ".text  // start a code block\n.globl  switch2\nswitch2:\n// save x%8 into eax\nmovl  %edi, %edx\nsarl  $31, %edx\nshrl  $29, %edx\nleal  (%rdi,%rdx), %eax\nandl  $7, %eax\nsubl  %edx, %eax\n// jump to .L1 if rax is not\n// one of 0, 1, .., 7\ncmpl  $7, %eax\nja .L1\nmovl  %eax, %eax\n// rest is similar\nWhat happens when x is signed?\nWhy all these instructions for x%8?  x%8 could be in -7,..,-1 if x < 0 !!!\n●\nsarl $31, %edx replicates sign bit of edi all over edx\n●\nshrl $29, %edx keeps sign bit only over the last 32-29=3 bits (bias)\n●\nleal (%rdi,%rdx), %eax adds 7 to edi if edi was negative, 0 if positive\n●\nandl  $7, %eax keeps only the last three bits\n●\nsubl  %edx, %eax subtracts the bias (7 if edi was negative, 0 if positive)\nExample: for edi=9, the bias edx is 0, (9+0)&7-0 is 1 => correct! 1 == 9%8\nExample: for edi=-9, the bias edx is 7, (-9+7)&7-7 is -1 => correct! -1 == -9%8\n// x = %edi, res = %rsi\nvoid switch2(int x,\nint *res)    {\nswitch(x%8) {\ncase 0:\n*res = x+5;\nbreak;\ncase 1:\n*res = x-3;\nbreak;\ncase 2:\n*res = x+12;\nbreak;\ncase 3:\n*res = x+7;\nbreak;\ncase 4:\n*res = x+5;\nbreak;\ncase 5:\n*res = x-3;\nbreak;\ncase 6:\n*res = x+12;\nbreak;\ncase 7:\n*res = x+7;\nbreak;\n}\n}\ngcc -Og -S switch.c", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 61, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 30, "chunk_index": 0, "title": "Assembly Directives", "summary": "Assembly Directives", "main_text": "Assembly Directives", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 31, "chunk_index": 0, "title": "Labels", "summary": "Labels ● The optional label in front of an instruction evaluates to the address where the instruction or data starts in memory and can be used in other instructions .text…", "main_text": "Labels\n● The optional label in front of an instruction evaluates to\nthe address where the instruction or data starts in\nmemory and can be used in other instructions\n.text\nfunc4:  movl  %eax,8(%rdx)\n.L1:    add   $1,%eax\njne   .L1\njmp   func4\nmovl\nadd\njne\njmp\n0x400000 = func4\n0x400003 = .L1\n0x400006\n0x400008\nAssembly Source File\n…and replaces the labels with their\ncorresponding address\nAssembler finds what address\neach instruction starts at…\n.text\n0:       movl  %eax,8(%rdx)\n3:       add   $1,%eax\n6:       jne   0x400003 (-5)\n8:       jmp   0x400000 (-10)", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 27, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 32, "chunk_index": 0, "title": "Directives specify:", "summary": "Directives specify: ● Where to place the information (.text, .data, .bss) ● Whether names (symbols) are visible to other files in the program (.globl) ● Global data variables & their…", "main_text": "Directives specify:\n●\nWhere to place the information\n(.text, .data, .bss)\n●\nWhether names (symbols) are visible to\nother files in the program (.globl)\n●\nGlobal data variables & their sizes\n(.byte, .long, .quad, .string)\n●\nAlignment requirements (.align)\nExample of Other Directives\n.text\n.globl  func\nfunc:\nmovl    $1, %eax\nret\n.globl  grades\n.bss\n.align 32\ngrades:\n.zero   80\n.globl  z\n.data\nz:\n.byte   10\n.globl  str\n.section    .rodata\n.LC0:\n.string \"Hello\"\n.section    .data.rel\n.align 8\nstr:\n.quad   .LC0\n.globl  x\n.data\n.align 16\nx:\n.long   1\n.long   2\n.long   3\n.long   4\n// no main(), this will be a .o\ndouble grades[10];\nunsigned char z = 10;\nchar* str = \"Hello\";\nint x[4] = {1,2,3,4};\nint func() {\nreturn 1;\n}\ngcc -Og -S file.c", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 52, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 33, "chunk_index": 0, "title": "Text and Data Segments", "summary": "Text and Data Segments • .text directive indicates the following instructions should be placed in the program area • .data directive indicates the following data declarations will be placed in…", "main_text": "Text and Data Segments\n• .text directive indicates the following\ninstructions should be placed in the\nprogram area\n• .data directive indicates the following\ndata declarations will be placed in the\ndata memory\n• .bss is for “data initialized to 0”\n• .data.rel is for “addresses to be\nadjusted” … only in “.o” not in\nexecutable after linking\nStack is for local vars not in registers\nUnused\n0x0040_0000\nText\n(program code)\nData\n(global variables)\nHeap\n(managed by\nmalloc)\nStack\n(managed with\npushq, popq,\nsubq/addq %rsp)\nI/O Space\n0x1000_0000\n0x8000_0000\n0xFFFF_FFFC\n0x7FFF_FFFC\n0x0000_0000\n%rsp\nbrk (Linux)", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 33, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 34, "chunk_index": 0, "title": "Static Data Directives", "summary": "Static Data Directives Fill memory with specific data when program is loaded • Format: (Label:) .type_id val_0,val_1,…,val_n type_id = {.byte, .word, .long, .quad, .float, .double} • Initial values specified with…", "main_text": "Static Data Directives\nFill memory with specific data when program is loaded\n• Format:\n(Label:)\n.type_id\nval_0,val_1,…,val_n\ntype_id = {.byte, .word, .long, .quad, .float, .double}\n• Initial values specified with a comma-separated list\n– Example:    myval: .long 1,2,3\n• Each value 1, 2, 3 is stored as a word (i.e. 32-bits)\n• Label “myval” evaluates to the start address of the first word\n(i.e. of the value 1)", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 12, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 35, "chunk_index": 0, "title": "Practice Example", "summary": "Practice Example", "main_text": "Practice Example", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit07_AssemblyFlow", "slide_number": 36, "chunk_index": 0, "title": "%rdi = %r12 = idx", "summary": "%rdi = %r12 = idx %rbp = %ebx = int i Note: %rdi must be reused from idx to the arguments for getInt(), thus the use of %r12 to hold…", "main_text": "%rdi = %r12 = idx\n%rbp = %ebx = int i\nNote: %rdi must be reused from idx to the\narguments for getInt(), thus the use of\n%r12 to hold idx\naddress\nsaved\nProcessor\n0000 0000 0000 0001\nrbp\n0000 0000 0000 0002\nr12\n0000 0000 7fff ffa8\nrsp\n0x7ffffff0\n0x7fffffec\nreturn\n0x7ffffff4\nsaved\n%rbp\n0x7fffffe4\n0x7fffffe0\n%r12\n0x7fffffe8\nStack Frame\n0000 0000\n0x7fffffa8\nvoid getInt(int *ptr);\nint f2(int idx)\n{\nint dat[4], min;\ngetInt(&min);\nfor(int i=0; i < 4; i++){\ngetInt(&dat[i]);\nif(dat[i] < min) min = dat[i];\n}\nreturn dat[idx] + min;\n}\nsaved\n0x7fffffdc\ncanary\nvalue\n0x7fffffd4\n0x7fffffd0\n%rbx\n0x7fffffd8\n0000 0000\n0x7fffffcc\ndat[3]\ndat[2]\n0x7fffffc4\n0x7fffffc0\n0000 0000\n0x7fffffc8\ndat[1]\n0x7fffffbc\nmin\n0000 0000\n0x7fffffb4\n0x7fffffb0\ndat[0]\n0x7fffffb8\n0000 0000\n0x7fffffac\nf2:     pushq   %r12\npushq   %rbp\npushq   %rbx\nsubq    $0x30, %rsp\nmovl    %edi, %r12d\nmovq    %fs:0x28, %rax\nmovq    %rax, 0x28(%rsp)\nxorl    %eax, %eax\nleaq    0xc(%rsp), %rdi\ncall    getInt\nmovl    $0, %ebx\njmp     .L4\n.L6:    movslq  %ebx, %rbp\nleaq    0x10(%rsp,%rbp,4), %rdi\ncall    getInt\nmovl    0x10(%rsp,%rbp,4), %eax\ncmpl    0xc(%rsp), %eax\njge     .L5\nmovl    %eax, 0xc(%rsp)\n.L5:    addl    $1, %ebx\n.L4:    cmpl    $3, %ebx\njle     .L6\nmovslq  %r12d, %r12\nmovl    0xc(%rsp), %eax\naddl    0x10(%rsp,%r12,4), %eax\nmovq    0x28(%rsp), %rdx\nxorq    %fs:0x28, %rdx\nje      .L7\ncall    __stack_chk_fail\n.L7:    addq    $0x30, %rsp\npopq    %rbx\npopq    %rbp\npopq    %r12\nret\nidx\ni", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 100, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 7, "topic": "Assembly Control Flow", "importance_score": 5, "file_hash": "53de6d0c1eb18ce4c346174b1e96136f17ed729a2451b6e9a46e419c2dcedf06"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 1, "chunk_index": 0, "title": "Unit 8: Buﬀer Overﬂows", "summary": "Unit 8: Buﬀer Overﬂows With Data Layout of Structs, Unions, Arrays", "main_text": "Unit 8: Buﬀer Overﬂows\nWith Data Layout of Structs, Unions, Arrays", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 2, "chunk_index": 0, "title": "Structs & Unions", "summary": "Structs & Unions", "main_text": "Structs & Unions", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 3, "chunk_index": 0, "title": "Structs", "summary": "Structs Structs are collections of heterogeneous data ● Each member is laid out in consecutive memory locations, with some padding inserted to ensure alignment ● Intel machines don’t require alignment…", "main_text": "Structs\nStructs are collections of\nheterogeneous data\n●\nEach member is laid out in consecutive\nmemory locations, with some padding\ninserted to ensure alignment\n●\nIntel machines don’t require alignment\nbut perform better when it is used\n●\nReordering can reduce size!\nwww.catb.org/esr/structure-packing\n●\nAll compilers follow the ABI rule:\n“Each elementary type should be\naligned at a multiple of its size”\nData1\nstruct Data1 {\nint x;\nchar y;\n};\nstruct Data2 {\nshort w;\nchar *p;\n};\nstruct Data3 {\nstruct Data1 f;\nint g;\n};\nx\noffset:\ny\nData2\n(w/o padding)\nw\noffset:\np\nw\noffset:\np\npadding\nData3\noffset:\nf.x\nf.y padding\ng\nData2\n(w/ padding)", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 49, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 4, "chunk_index": 0, "title": "Struct oﬀsets in assembly", "summary": "Struct oﬀsets in assembly Assume 4-byte int / float, 8-byte long / double. Can you figure out the offsets for %rdi ? struct record_t { char a[2]; int b; long…", "main_text": "Struct oﬀsets in assembly\nAssume 4-byte int / float,\n8-byte long / double.\nCan you figure out the\noffsets for %rdi ?\nstruct record_t {\nchar a[2];\nint b;\nlong c;\nint d[3];\nshort e;\n};\nvoid initialize(struct record_t *x) {\nx->a[1] = 1;\nx->b    = 2;\nx->c    = 3;\nx->d[1] = 4;\nx->e    = 5;\n}\ninitialize:\nmovb    $1, 1(%rdi)\nmovl    $2, 4(%rdi)\nmovq    $3, 8(%rdi)\nmovl    $4, 20(%rdi)\nmovw    $5, 28(%rdi)\nret\na\na\nb\nb\nb\nb\nc\nc\nc\nc\nc\nc\nc\nc\nd0\nd0\nd0\nd0\nd1\nd1\nd1\nd1\nd2\nd2\nd2\nd2\ne\ne\nStarting of struct:", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 55, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 5, "chunk_index": 0, "title": "struct B {   // this struct must start/end at a multiple of 4, because that's required by 'y'", "summary": "struct B { // this struct must start/end at a multiple of 4, because that's required by 'y' char x; // 1 byte int y; // 4 bytes (needs 3…", "main_text": "struct B {   // this struct must start/end at a multiple of 4, because that's required by 'y'\nchar x;  // 1 byte\nint  y;  // 4 bytes (needs 3 bytes of padding before to start at a multiple of 4)\nchar z;  // 1 byte  (needs 3 bytes of padding after to end at a multiple of 4)\n};\nstruct A {\nchar a;     // 1 byte\nstruct B b; // has 4-byte alignment: 3 bytes of padding before 'b'\nchar c;     // also 3 bytes of padding before 'c', so that 'b' ends at a multiple of 4\n};\nvoid init(struct A *a) {\na->a   = 1;\na->b.x = 2;\na->b.y = 3;\na->b.z = 4;\na->c   = 5;\n}\n$ gcc  -fomit-frame-pointer -mno-red-zone -Og -S align.c; cat align.s | grep mov\nmovb    $1, (%rdi)\nmovb    $2, 4(%rdi)\nmovl    $3, 8(%rdi)\nmovb    $4, 12(%rdi)\nmovb    $5, 16(%rdi)\nWe still want each member of the nested struct to start at a multiple of its size,\nbut where should the nested struct itself start?\nIts start/end should have the   largest   alignment required by its members.", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 26, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 6, "chunk_index": 0, "title": "Unions allow you to read/write the same", "summary": "Unions allow you to read/write the same memory region as variables with different types ● All elements start at offset 0 ● The size of the union is simply the…", "main_text": "Unions allow you to read/write the same\nmemory region as variables with different\ntypes\n●\nAll elements start at offset 0\n●\nThe size of the union is simply the size\nof the biggest member\n●\nElements must be POD (plain old data)\nor at least default-constructible\nUnions\nunion Data1 {\nint x;\nchar y;\n};\nunion Data2 {\nshort w;\nchar *p;\n};\nint main() {\nunion Data1 item;\nitem.x = 0x356;\nitem.y = 'a';\n}\np\nData1\nx\noffset:\nData2\n(w/o padding)\nw\noffset:\ny\nitem\n56 03 00 00\noffset:\nitem\n61 03 00 00\nRecall x86 uses\nlittle-endian", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 41, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 7, "chunk_index": 0, "title": "Unions: Hex Encoding of a Float", "summary": "Unions: Hex Encoding of a Float • 4-byte union • i reads/writes an int • f reads/writes a float Endianness not noticeable: members have same size. #include <stdio.h> union float_int…", "main_text": "Unions: Hex Encoding of a Float\n• 4-byte union\n•\ni reads/writes an int\n•\nf reads/writes a float\nEndianness not noticeable:\nmembers have same size.\n#include <stdio.h>\nunion float_int {\nfloat f;\nint i;\n};\nint main() {\nunion float_int fi;\nfi.f = 1.0;\nprintf(\"%.2f is %08X\\n\", fi.f, fi.i);\n}\n// prints:\n// 1.00 is 3F800000", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 20, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 8, "chunk_index": 0, "title": "Arrays & Buﬀer Overﬂows", "summary": "Arrays & Buﬀer Overﬂows", "main_text": "Arrays & Buﬀer Overﬂows", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 9, "chunk_index": 0, "title": "Array Bounds in Java, Python, C", "summary": "Array Bounds in Java, Python, C class Bounds { public static void main(String[] args) { int[] x = new int[10]; for (int i = 0; i <= x.length; i++) {…", "main_text": "Array Bounds in Java, Python, C\nclass Bounds {\npublic static void main(String[] args) {\nint[] x = new int[10];\nfor (int i = 0; i <= x.length; i++) {\nx[i] = i;\n}\n}\n}\nx = [0] * 10\n# not pythonic! but still...\nfor i in range(len(x) + 1):\nx[i] = i\n#include <stdio.h>\nint main() {\nint x[10];\nfor (int i = 0; i <= 10; i++) {\nx[i] = i;\n}\n}\n$ javac Bounds.java\n$ java Bounds\nException in thread \"main\"\njava.lang.ArrayIndexOutOfBoundsException: 10\nat Bounds.main(Bounds.java:7)\n$ python3 bounds.py\nTraceback (most recent call last):\nFile \"bounds.py\", line 5, in <module>\nx[i] = i\nIndexError: list assignment index out of range\n$ gcc bounds.c -o bounds\n$ ./bounds\n$\nNo failure! Why?", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 34, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 10, "chunk_index": 0, "title": "C does not check array bounds", "summary": "C does not check array bounds • Many functions, especially those related to strings, do not check the bounds of an array either • User or other input may overflow…", "main_text": "C does not check array bounds\n• Many functions, especially those related to strings, do not\ncheck the bounds of an array either\n• User or other input may overflow a fixed size array\n– Note: gets() receives input from 'stdin' until the user enters '\\n'\nand places the string in the given array (no bound checks!)\nvoid greet() {\nchar name[10];\ngets(name);\n...\n}\nvoid func1(char *str) {\nchar copy[10];\nstrcpy(copy, str);\n...\n}\nname\n'T'\n0x7fffffef0:\n'o''m''m''y' 00\n...\ncopy\n'T'\n0x7fffffef0:\n'o''m''m''y' 00\n...\nstr\n'T'\n0x7fffffa80:\n'o''m''m''y' 00\n\"Tommy\" = 54 6f 6d 6d 79 00\nCS:APP 3.10.3", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 32, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 11, "chunk_index": 0, "title": "C does not check array bounds", "summary": "C does not check array bounds • Many functions, especially those related to strings, do not check the bounds of an array either • User or other input may overflow…", "main_text": "C does not check array bounds\n• Many functions, especially those related to strings, do not\ncheck the bounds of an array either\n• User or other input may overflow a fixed size array\n– Now suppose the user types or passes \"Bartholomew\"\ninstead of \"Tommy\"\nvoid greet() {\nchar name[10];\ngets(name);\n...\n}\nvoid func1(char *str) {\nchar copy[10];\nstrcpy(copy, str);\n...\n}\nCS:APP 3.10.3\nname\n'B'\n0x7fffffef0:\n'a''r''t''h' 'o'\n...\n'e'\ncopy\n0x7fffffef0:\n...\nstr\n0x7fffffa80:\n'w' 00\n'B''a''r''t''h' 'o'\n...\n'e''w' 00\nWhat are we overwriting on the stack?", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 33, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 12, "chunk_index": 0, "title": "Buﬀer Overﬂows", "summary": "Buﬀer Overﬂows These local arrays are stored on the stack where the return address is also stored! void greet() { char name[12]; gets(name); printf(\"Hello %s\\n\", name); } 0000 0000 0000…", "main_text": "Buﬀer Overﬂows\nThese local arrays are stored on the stack\nwhere the return address is also stored!\nvoid greet() {\nchar name[12];\ngets(name);\nprintf(\"Hello %s\\n\", name);\n}\n0000 0000\n0000 0000\nProcessor\nMemory / RAM\n0000 0000 0000 0000\nrdi\n0000 0000 7fff f0e0\nrsp\n0x7ffff0f0\n0x7ffff0ec\n0000 0000\n0x7ffff0f4\n0000 0000 0004 d8c4\n0000 0079\n6d6d 6f54\n0x7ffff0e4\n0x7ffff0e0\n0000 0000\n0x7ffff0e8\n0004 a048\n0x7ffff0f8\n0x0\n...\n0xfffffffc\nrip\n0000 0000\ngreet:\nsubq    $24, %rsp\nmovq    %rsp, %rdi\nmovl    $0, %eax\ncall    gets\nmovl    $.LC0, %esi\nmovl    $1, %edi\nmovl    $0, %eax\ncall    __printf_chk\naddq    $24, %rsp\nret\nReturn\nAddress\n\"Tommy\" = 54 6f 6d 6d 79 00\nname", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 49, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 13, "chunk_index": 0, "title": "Buﬀer Overﬂows", "summary": "Buﬀer Overﬂows An intelligent user could carefully craft a “long” input array and overwrite the return address with a desired value! void greet() { char name[12]; gets(name); printf(\"Hello %s\\n\", name);…", "main_text": "Buﬀer Overﬂows\nAn intelligent user could carefully craft a “long” input array and\noverwrite the return address with a desired value!\nvoid greet() {\nchar name[12];\ngets(name);\nprintf(\"Hello %s\\n\", name);\n}\nProcessor\nMemory / RAM\n0000 0000 0000 0000\nrdi\n0000 0000 7fff f0e0\nrsp\n0x7ffff0f0\n0x7ffff0ec\n0x7ffff0f4\n0000 0000 0004 d8c4\n0x7ffff0e4\n0x7ffff0e0\n0x7ffff0e8\n0x7ffff0f8\n0x0\n...\n0xfffffffc\nrip\ngreet:\nsubq    $24, %rsp\nmovq    %rsp, %rdi\nmovl    $0, %eax\ncall    gets\nmovl    $.LC0, %esi\nmovl    $1, %edi\nmovl    $0, %eax\ncall    __printf_chk\naddq    $24, %rsp\nret\nUser string:\n54 6f 6d 6d 79 1e ac 5f 47 80 81\n62 37 48 31 92 54 93 61 72 39 72\n41 20 e8 73 32 3c 68 92 14 43\n7261 9354\n9231 4837\n2041 7239\n5fac 1e79\n6d6d 6f54\n6281 8047\n3c32 73e8\n4314 9268\nOverwritten Return\nAddress", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 51, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 14, "chunk_index": 0, "title": "Code Injection Attack", "summary": "Code Injection Attack Fill the buffer with machine code for some instructions we want to execute and overwrite the return address with an address pointing to our code void greet()…", "main_text": "Code Injection Attack\nFill the buffer with machine code for some instructions we want\nto execute and overwrite the return address with an address\npointing to our code\nvoid greet() {\nchar name[12];\ngets(name);\nprintf(\"Hello %s\\n\", name);\n}\n7261 9354\n9231 4837\nProcessor\nMemory / RAM\n0000 0000 0000 0000\nrdi\n0000 0000 7fff f0e0\nrsp\n0x7ffff0f0\n0x7ffff0ec\n2041 7239\n0x7ffff0f4\n0000 0000 0004 d8c4\n5fac 1e79\n6d6d 6f54\n0x7ffff0e4\n0x7ffff0e0\n6281 8047\n0x7ffff0e8\n7fff f0e8\n0x7ffff0f8\n0x0\n...\n0xfffffffc\nrip\n0000 0000\ngreet:\nsubq    $24, %rsp\nmovq    %rsp, %rdi\nmovl    $0, %eax\ncall    gets\nmovl    $.LC0, %esi\nmovl    $1, %edi\nmovl    $0, %eax\ncall    __printf_chk\naddq    $24, %rsp\nret\nOverwritten\nReturn\nAddress\nUser string:\n54 6f 6d 6d 79 1e ac 5f 47 80 81\n62 37 48 31 92 54 93 61 72 39 72\n41 20 e8 f0 ff 7f 00 00 00 00\nCS:APP 3.10.4", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 54, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 15, "chunk_index": 0, "title": "Exploits", "summary": "Exploits • Common code that we try to inject on the stack would start a shell so that we can now execute any other commands • Pass binary input to…", "main_text": "Exploits\n• Common code that we try to inject\non the stack would start a shell so\nthat we can now execute any other\ncommands\n• Pass binary input to a program by\nredirecting the content of a file:\n./program < attack_input.raw\n• Remote attacks\n– Binary data in HTTP requests\nto remote server\n– Binary JPEG header fields in\nWhatsapp message, etc", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 13, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 16, "chunk_index": 0, "title": "Buﬀer Overﬂows in the Wild", "summary": "Buﬀer Overﬂows in the Wild • TLVs (Type, Length, Value) are often used to give structure to data, and parsing a TLV might mean it's coming from somewhere untrusted. Each…", "main_text": "Buﬀer Overﬂows in the Wild\n•\nTLVs (Type, Length, Value) are often used to give structure to data, and\nparsing a TLV might mean it's coming from somewhere untrusted. Each TLV\nhas a single-byte type followed by a two-byte length which is the length of\nthe variable-sized payload in bytes.\n•\nFirst each TLV is passed to IO80211AWDLPeer::tlvCheckBounds. This\nmethod has a hardcoded list of specific minimum and maximum TLV lengths\nfor some of the supported TLV types. [..] Type 0x14 isn't explicitly listed in\ntlvCheckBounds so it gets the default upper length limit of 1024,\nsignificantly larger than the 60 byte buffer allocated for the destination\nbuffer in the IO80211AWDLPeer structure.\n“Unfortunately, it's the same old\nstory. A fairly trivial buffer\noverflow programming error in\nC++ code in the kernel parsing\nuntrusted data, exposed to\nremote attackers.”\n-- Ian Beer, Project Zero\nhttps://bit.ly/3rd576w", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 21, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 17, "chunk_index": 0, "title": "Methods of Prevention", "summary": "Methods of Prevention ● Use languages where bounds are checked ○ Overhead can be high (e.g., in games or OS) ● Use C libraries that write only limited data strcpy…", "main_text": "Methods of Prevention\n● Use languages where bounds are checked\n○ Overhead can be high (e.g., in games or OS)\n● Use C libraries that write only limited data\nstrcpy (char* dest, char* src)\nstrncpy(char* dest, char* src, size_t len)\n● Compiler Protections\n○ Add code to detect overflows (e.g., canary values)\n● OS Protections\n○ Randomize code location (ASLR)\n○ Prevent execution of code in the stack (VM)", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 11, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 18, "chunk_index": 0, "title": "Canary Values", "summary": "Canary Values Compiler inserts code to generate and store a unique value between the return address and the local variables Before returning it will check whether this value has been…", "main_text": "Canary Values\nCompiler inserts code to generate and store a unique\nvalue between the return address and the local variables\nBefore returning it will check whether this value has been\naltered (by a buffer overflow) and raise an error if it has\n5ac3 3ca5\n0000 0000\nProcessor\nMemory / RAM\n0000 0000 0000 0000\nrdi\n0000 0000 7fff f0e0\nrsp\n0x7ffff0f0\n0x7ffff0ec\nfeed bead\n0x7ffff0f4\n0000 0000 0004 d8c4\n0000 0079\n6d6d 6f54\n0x7ffff0e4\n0x7ffff0e0\n0000 0000\n0x7ffff0e8\n0004 a048\n0x7ffff0f8\n0x0\n...\n0xfffffffc\nrip\n0000 0000\nReturn\nAddress\nname\ngreet:\nsubq    $24, %rsp\nmovq    %fs:40, %rax\nmovq    %rax, 16(%rsp)\nmovq    %rsp, %rdi\nmovl    $0, %eax\ncall    gets\nmovl    $.LC0, %esi\nmovl    $1, %edi\nmovl    $0, %eax\ncall    __printf_chk\nmovq    16(%rsp), %rax\nxorq    %fs:40, %rax\nje      .L2\ncall    __stack_chk_fail\n.L2:\naddq    $24, %rsp\nret", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 52, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 19, "chunk_index": 0, "title": "Address Space Layout Randomisation", "summary": "Address Space Layout Randomisation OS randomizes the address where the stack starts ● The attacker doesn’t know the exact address of the injected code (it’s different every time) void greet()…", "main_text": "Address Space Layout Randomisation\nOS randomizes the address where the stack starts\n●\nThe attacker doesn’t know the exact address of the\ninjected code (it’s different every time)\nvoid greet()\n{\nchar name[12];\ngets(name);\nprintf(\"Hello %s\\n\");\n}\n7261 9354\n9231 4837\nProcessor\nMemory / RAM\n0000 0000 0000 0000\nrdi\n0000 0000 7fff f0e0\nrsp\n0x7ffff0f0\n0x7ffff0ec\n2041 7239\n0x7ffff0f4\n0000 0000 0004 d8c4\n5fac 1e79\n6d6d 6f54\n0x7ffff0e4\n0x7ffff0e0\n6281 8047\n0x7ffff0e8\n7fff f0e8\n0x7ffff0f8\n0x0\n...\n0xfffffffc\nrip\n0000 0000\ngreet:\nsubq    $24, %rsp\nmovq    %rsp, %rdi\nmovl    $0, %eax\ncall    gets\nmovl    $.LC0, %esi\nmovl    $1, %edi\nmovl    $0, %eax\ncall    __printf_chk\naddq    $24, %rsp\nret\nOverwritten\nReturn\nAddress\nUser string:\n54 6f 6d 6d 79 1e ac 5f 47 80 81\n62 37 48 31 92 54 93 61 72 39 72\n41 20 e8 f0 ff 7f 00 00 00 00\nname", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 56, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 20, "chunk_index": 0, "title": "How the OS randomizes the layout", "summary": "How the OS randomizes the layout The OS can allocate a random amount of space on the stack each time a program is executed to make it harder for an…", "main_text": "How the OS randomizes the layout\nThe OS can allocate a random amount\nof space on the stack each time a\nprogram is executed to make it harder\nfor an attacker to succeed in an exploit\nThis is referred to as ASLR (Address\nSpace Layout Randomization   )\nOur previous exploit string would now\nhave a return address that does not\nlead to our exploit code and likely\nresult in a crash rather than execution\nof the exploit code\n7261 9354\n9231 4837\nMemory / RAM\n0x7ffb0a10\n0x7ffb0a0c\n2041 7239\n0x7ffb0a14\n5fac 1e79\n6d6d 6f54\n0x7ffb0a04\n0x7ffb0a00\n6281 8047\n0x7ffb0a08\n7fff f0e8\n0x7ffb0a18\n0x0\n...\n0xfffffffc\n0000 0000\nOverwritten\nReturn\nAddress\nname\n0x80000000\nRandom\nAmount\n0x7ffb0a20\n0x7ffb0a1c\nStart of\nexploit code", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 42, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 21, "chunk_index": 0, "title": "Beating ASLR with “nop sleds”", "summary": "Beating ASLR with “nop sleds” Most CPUs have a “ nop ” instruction that is an instruction that does nothing ● Can also just use an instruction that does very…", "main_text": "Beating ASLR with “nop sleds”\nMost CPUs have a “ nop ” instruction that is\nan instruction that does nothing\n●\nCan also just use an instruction that\ndoes very little (e.g., movq %rsp, %rsp)\nIdea: Prepend as many “nop” instructions as\npossible in the buffer before the exploit code\nEffect: Now our guess for the RA does not\nneed to be exact but anywhere in the range\nof nops\nThis yields a higher chance of actually\nlanding in a location that will eventually\ncause the exploit to be executed.\n7261 9354\n9231 4837\nMemory / RAM\n0x7ffb0a10\n0x7ffb0a0c\n2041 7239\n0x7ffb0a14\n90 90 90 90\n90 90 90 90\n0x7ffb0a04\n0x7ffb0a00\n6281 8047\n0x7ffb0a08\n7ffb 09f4\n0x7ffb0a18\n0x0\n0000 0000\nOverwritten\nReturn\nAddress\nname\nRandom\nAmount\n0x7ffb0a20\n0x7ffb0a1c\nExploit\nCode\n90 90 90 90\n90 90 90 90\n90 90 90 90\n90 90 90 90\n0x7ffb09fc\n0x7ffb09f4\n0x7ffb09f0\n0x7ffb09f8\nnop Sled\n(A return address to\nany location in the\nsled will cause us to\nexecute the exploit\ncode)\nnop\nnop\n...\nnop\nexploit code", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 60, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 22, "chunk_index": 0, "title": "Memory Protection (OS & CPU)", "summary": "Memory Protection (OS & CPU) CPUs have hardware to help track areas of memory used by a program (MMU = Memory Management Unit ) & verify appropriate address usage When…", "main_text": "Memory Protection (OS & CPU)\nCPUs have hardware to help track areas of memory used by a\nprogram (MMU =   Memory Management Unit   ) & verify\nappropriate address usage\nWhen performing a memory access the processor will indicate\nthe desired operation: Fetch (eXecute), Read data, Write data\nThis will be compared to the access permissions stored in the\nMMU and catch any violation (to halt execution)\nThe stack area\ncan be set for\nNo-eXecute\n(NX or X=0)\nby the OS\nx86 CPU\nrsp\n0x16000\nrip\nrax\nMMU = Memory Mgmt. Unit\n0x16000\nunused\nStack\nSeg.\nBase: 0x14000\nBase + Bound:\n0x19000\nExploit\nCode\n0x16000\n0x2a000 0x03200\nBase\nBound\nRWX\n0x14000 0x05000\n0x08000\n0x0400\nDescriptor Table\nData\nSeg.\nBase: 0x2a000\nCode\nSeg.\nBase + Bound:\n0x2d200\nBase: 0x08000\nBase + Bound:\n0x80400\nhttp://ece-research.unm.edu/jimp/310/slides/micro_arch2.html\nDesired\nAccess\n(R/W/X)\nMemory\neXecute\nViolation", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 54, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 23, "chunk_index": 0, "title": "Return Oriented Programming", "summary": "Return Oriented Programming Even when the stack is marked as non-executable, and its position randomized, we can still execute arbitrary code! ● Find the attack instructions inside of those that…", "main_text": "Return Oriented Programming\nEven when the stack is marked as\nnon-executable, and its position\nrandomized, we can still execute\narbitrary code!\n●\nFind the attack instructions\ninside of those that already\nexist in the code segment\n(always executable, position\nnot randomized)\n●\nUse attack instructions followed\nby a ret (a gadget) which will\npop the next return address\nthat we prepared on the stack\n●\npop gadgets to save data to regs\nunused\nStack\nSeg.\nBase: 0x14000\nBase + Bound:\n0x19000\n0x16000\nData\nSeg.\nBase: 0x2a000\nCode\nSeg.\nBase + Bound:\n0x2d200\nBase: 0x08000\nBase + Bound:\n0x80400\nMemory\nExploit\nCode\n0040 11a1\n0000 0000\n0000 0000\n0000 0000\n0040 119f\n0000 0010\n0040 117e\n...\n0000 0000\n0000 0000\nAddress of gadget1\n(initial return address,\nnow overwritten)\nAddress of gadget2\n8-byte value 16\n(popped into %rax)\nAddress of touch", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 55, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 24, "chunk_index": 0, "title": "ROP Example 1", "summary": "ROP Example 1", "main_text": "ROP Example 1", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 25, "chunk_index": 0, "title": "ROP Example 2", "summary": "ROP Example 2", "main_text": "ROP Example 2", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 26, "chunk_index": 0, "title": "ROP Example 3", "summary": "ROP Example 3", "main_text": "ROP Example 3", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 27, "chunk_index": 0, "title": "Gadgets", "summary": "Gadgets Often, it is possible to find useful instructions within the byte encoding of other instructions. Gadget: short sequence of instructions followed by ret (0xc3) void setval_210(unsigned *p) { *p…", "main_text": "Gadgets\nOften, it is possible to find useful\ninstructions within the byte encoding\nof other instructions.\nGadget: short sequence of instructions\nfollowed by ret (0xc3)\nvoid setval_210(unsigned *p) {\n*p = 3347663060U;\n}\n0000000000400f15 <setval_210>:\n400f15: c7 07 d4 48 89 c7   movl $0xc78948d4,(%rdi)\n400f1b: c3                  retq\n48 89 c7 encodes the\nx86_64 instruction\nmovq %rax, %rdi\nTo start this gadget, set a\nreturn address to 0x400f18\n(use little-endian format)", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 18, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 28, "chunk_index": 0, "title": "AttackLab: Known Instructions", "summary": "AttackLab: Known Instructions", "main_text": "AttackLab: Known Instructions", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356_Unit08_BufferOverflows", "slide_number": 29, "chunk_index": 0, "title": "AttackLab: Finding Gadgets", "summary": "AttackLab: Finding Gadgets $ objdump -d rtarget | grep -A2 '89 c7' 401380: 48 89 c7 mov %rax,%rdi 401383: e8 36 00 00 00 callq 4013be <scramble> 401388: 89 c3…", "main_text": "AttackLab: Finding Gadgets\n$ objdump -d rtarget | grep -A2 '89 c7'\n401380:\n48 89 c7\nmov\n%rax,%rdi\n401383:\ne8 36 00 00 00\ncallq  4013be <scramble>\n401388:\n89 c3\nmov\n%eax,%ebx\n--\n401394:\n48 89 c7\nmov\n%rax,%rdi\n401397:\ne8 c4 fc ff ff\ncallq  401060 <srandom@plt>\n40139c:\ne8 1f fd ff ff\ncallq  4010c0 <random@plt>\n--\n40191b:\nb8 48 89 c7 91\nmov\n$0x91c78948,%eax\n401920:\nc3\nretq\n--\n40192e:\n8d 87 5c 48 89 c7\nlea\n-0x3876b7a4(%rdi),%eax\n401934:\nc3\nretq\n--\n40193c:\n8d 87 48 89 c7 c7\nlea\n-0x383876b8(%rdi),%eax\n401942:\nc3\nretq\n--\n401943:\n8d 87 48 89 c7 90\nlea\n-0x6f3876b8(%rdi),%eax\n401949:\nc3\nretq", "notes_text": "", "keywords": [], "images": [], "layout": {"num_text_boxes": 56, "num_images": 0, "dominant_visual_type": "text-heavy"}, "metadata": {"course": "CS356", "unit": 8, "topic": "Buffer Overflows and Security", "importance_score": 5, "file_hash": "804957db702cf7cff55fda6d974f59bbb41aaf918098649c86f3bf0b777ea9cb"}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 1, "title": "Unit 9: ARM", "summary": "Title slide introducing ARM64 ISA and Linux ABI.", "main_text": "Unit 9: ARM - ARM64 ISA and its Linux ABI", "notes_text": null, "keywords": ["ARM", "ARM64", "ISA", "Linux ABI", "CS356"], "images": [{"type": "logo", "description": "CS356 Trojan logo and university branding"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "title_slide"}, "section": "Introduction", "metadata": {"course": "CS356", "unit": 9, "topics": ["ARM overview"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 2, "title": "ARM Ltd's Business Model", "summary": "Explains ARM's licensing business model and major customers.", "main_text": "Business Model: Designs CPU cores and instruction sets, sells licenses and designs to other companies. Customers: Broadcom buys licenses to add Cortex-A76 CPU to its BCM2712 System on Chip (Raspberry Pi5), Qualcomm same for Snapdragon SoC, Apple designs ARM CPUs (M2, A15), Amazon designs AWS Graviton CPUs with ARM cores. ARM is everywhere: mobile, desktop, embedded, cloud.", "notes_text": null, "keywords": ["ARM business model", "licensing", "Broadcom", "Qualcomm", "Apple", "Amazon", "Cortex", "Snapdragon", "Graviton"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Introduction", "metadata": {"course": "CS356", "unit": 9, "topics": ["ARM business"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 3, "title": "RISC Architectures", "summary": "Introduces RISC principles and compares with CISC architectures.", "main_text": "ARM = Advanced RISC Machines. RISC = Reduced Instruction Set Computer. With respect to CISC (Complex ISC, used by Intel): Dedicated instructions to load/store to memory (e.g., add can only add between registers), Sometimes more instructions are needed but it's easier to optimize simple instructions.", "notes_text": null, "keywords": ["RISC", "CISC", "load-store architecture", "instruction optimization", "Hennessy", "Patterson"], "images": [{"type": "photo", "description": "John Hennessy and David Patterson, ACM Turing Award Lecture, 2017"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Introduction", "metadata": {"course": "CS356", "unit": 9, "topics": ["RISC architecture"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 4, "title": "ARM64: Registers", "summary": "Describes ARM64 register set including general-purpose and special registers.", "main_text": "31 General-Purpose Integer Registers (two sizes): To use all 64 bits: x0, x1, x2, ..., x30 (extended), Least significant 32 bits: w0, w1, w2, ..., w30 (word) - writing to these sets the most significant 32 bits to 0. Registers with Special Uses: Stack pointer sp (error if not aligned at a multiple of 16 bytes), Virtual zero registers xzr and wzr (give 0 if read, ignore writes), Link register lr (alias for x30) to save the return address.", "notes_text": null, "keywords": ["ARM64 registers", "x registers", "w registers", "stack pointer", "zero register", "link register"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Registers and Instructions", "metadata": {"course": "CS356", "unit": 9, "topics": ["register architecture"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 5, "title": "Operations on Registers", "summary": "Introduces ARM64 instruction syntax with three-operand format.", "main_text": "A different style! Three arguments, destination first: OP dst, src1, src2 (means dst = src1 OP src2). Always operate on registers or immediates, not memory. Example: x1 = 0x1111111111111111, x2 = 0x2200220022002200, x3 = 0x3300330000330033. add x1, x2, x3 is like x1 = x2 + x3 results in x1 = 0x5500550022332233. add w1, w2, w3 is like w1 = w2 + w3 (also sets rest of x1 to 0, like x86 addl) results in x1 = 0x0000000022332233.", "notes_text": null, "keywords": ["three-operand", "instruction syntax", "add instruction", "register operations"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Registers and Instructions", "metadata": {"course": "CS356", "unit": 9, "topics": ["instruction format"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 6, "title": "Flexible Operands", "summary": "Explains optional shift and rotation modifiers for operands.", "main_text": "add, sub, and, orr, eor, mvn, mov allow optional shift or rotation of the last argument by n bits: OP dst, src1, src2, LSL n means dst = src1 OP (src2 << n), OP dst, src1, src2, LSR n means dst = src1 OP (src2 >> n), OP dst, src1, src2, ASR n means dst = src1 OP (src2 s>> n), OP dst, src1, src2, ROR n means dst = src1 OP (src2 >>> n). Example: add x1, x2, x3, lsl 32 gives x1 = 0x2233223322002200, add w1, w2, w3, ror 8 gives x1 = 0x0000000055005500.", "notes_text": null, "keywords": ["flexible operands", "LSL", "LSR", "ASR", "ROR", "barrel shifter", "shift operations"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Registers and Instructions", "metadata": {"course": "CS356", "unit": 9, "topics": ["operand modifiers"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 7, "title": "Arithmetic/Logic Operations", "summary": "Comprehensive table of ARM64 arithmetic and logical instructions.", "main_text": "Instructions include: Add immediate (add x0, x1, 1), Add register (add x0, x1, x2), Add shifted register (add x0, x1, x2, lsl 10), Subtract (sub x0, x1, x2), Subtract shifted (sub x0, x1, x2, lsl 10), Negate (neg x0, x1), Multiply (mul x0, x1, x2), Multiply-add (madd x0, x1, x2, x4), Divide signed (sdiv x0, x1, x2), Divide unsigned (udiv x0, x1, x2), Logical shift left (lsl x0, x1, x2), Logical shift right (lsr x0, x1, x2), Arithmetic shift right (asr x0, x1, x2), Rotate bits (ror x0, x1, x2), Bitwise AND (and x0, x1, x2), Bitwise OR (orr x0, x1, x2), Bitwise XOR (eor x0, x1, x2), Bitwise NOT (mvn x0, x1).", "notes_text": null, "keywords": ["arithmetic operations", "logic operations", "add", "subtract", "multiply", "divide", "shift", "rotate", "bitwise"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "table"}, "section": "Registers and Instructions", "metadata": {"course": "CS356", "unit": 9, "topics": ["instruction reference"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 8, "title": "Compare with x86-64", "summary": "Side-by-side comparison of ARM64 and x86-64 code for three functions.", "main_text": "C functions: add(x,y) returns x+y, multadd(x,y) returns 10+x+8*y, bias(x,k) returns x + (mask & bias) where bias=(1<<k)-1 and mask=x>>31. ARM64 code uses simple three-operand format with shifts. x86-64 code uses lea for address arithmetic and complex addressing modes. ARM64 requires more explicit operations but maintains uniform instruction format.", "notes_text": null, "keywords": ["ARM vs x86", "code comparison", "assembly differences", "lea instruction"], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "code_comparison"}, "section": "Registers and Instructions", "metadata": {"course": "CS356", "unit": 9, "topics": ["ISA comparison"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 9, "title": "Load and Store: Data from/to memory", "summary": "Introduces dedicated load and store instructions for memory access.", "main_text": "mov is only for registers: A64 has dedicated instructions to move from/to memory. ldr x1, [x2] means x1 = 8 bytes from address x2. str x1, [x2] means 8 bytes at address x2 = x1. ldr x0, x1, [x2] means x0 = 8 bytes at x2, x1 = 8 bytes at x2+8. str x0, x1, [x2] means 8 bytes at x2 = x0, 8 bytes at x2+8 = x1.", "notes_text": null, "keywords": ["load instruction", "store instruction", "ldr", "str", "memory access", "load-store architecture"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Memory Operations", "metadata": {"course": "CS356", "unit": 9, "topics": ["load store instructions"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 10, "title": "Comparing with x86-64", "summary": "Shows ARM64 vs x86-64 for pointer dereference operations.", "main_text": "C functions: get(ptr) returns *ptr, set(ptr, x) sets *ptr = x. ARM64 uses ldr w0, [x0] for load and str w1, [x0] for store with dedicated memory instructions. x86-64 uses movl (%rdi), %eax and movl %esi, (%rdi) with mov instruction that can access memory directly.", "notes_text": null, "keywords": ["pointer operations", "memory access comparison", "ldr vs mov"], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "code_comparison"}, "section": "Memory Operations", "metadata": {"course": "CS356", "unit": 9, "topics": ["memory instruction comparison"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 11, "title": "Addressing Modes", "summary": "Comprehensive table of ARM64 addressing modes for load/store.", "main_text": "Addressing modes: Base (ldr x1, [x2] uses x2), Base+Offset (ldr x1, [x2, 16] uses x2+16), Pre-Indexed (ldr x1, [x2, 16]! uses x2+16, side-effect: x2=x2+16), Post-Indexed (ldr x1, [x2], 16 uses x2, side-effect: x2=x2+16), Base+Register (ldr x1, [x2, x3] uses x2+x3), Scaled (ldr x1, [x2, x3, lsl 2] uses x2+(x3<<2)), Sign Extended (ldr x1, [x2, w3, sxtw] uses x2+sign_extend(w3)), Zero Extended (ldr x1, [x2, w3, uxtw] uses x2+zero_extend(w3)), Sign Extended+Scaled (ldr x1, [x2, w3, sxtw 2] uses x2+(sign_extend(w3)<<2)). Note: immediate values can have a # prefix, e.g., ldr x1, [sp], #16.", "notes_text": null, "keywords": ["addressing modes", "pre-indexed", "post-indexed", "scaled addressing", "base offset"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "table"}, "section": "Memory Operations", "metadata": {"course": "CS356", "unit": 9, "topics": ["addressing modes"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 12, "title": "Stack Push and Pop", "summary": "Explains how to implement stack operations without dedicated push/pop instructions.", "main_text": "The reason ldr/str can have a side effect: There is no dedicated push/pop instruction! Push x1 onto the stack: str x1, [sp, -16]! stores x1 at memory address sp-16, and updates sp to sp-16 (to allocate). Pop the 8 bytes at sp into x1: ldr x1, [sp], 16 reads 8 bytes from memory address sp, after that updates sp to sp+16 (to deallocate). x1 is only 8 bytes, why moving sp by 16? For alignment! To avoid wasting 8 bytes on the stack, push/pop register pairs with stp x0, x1, [sp, -16]! and ldp x0, x1, [sp], 16.", "notes_text": null, "keywords": ["stack operations", "push", "pop", "stack alignment", "stp", "ldp", "pre-indexed", "post-indexed"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Memory Operations", "metadata": {"course": "CS356", "unit": 9, "topics": ["stack management"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 13, "title": "Addressing Modes: Stack Variables", "summary": "Example showing stack allocation and access for local array.", "main_text": "C function local_array() creates int a[2] = {1, 2} and returns (long) a (bad practice - never return addr of stack variables). ARM64: sub sp, sp, 16 (allocate), mov w0, 1; str w0, [sp, 8] (store 1), mov w0, 2; str w0, [sp, 12] (store 2), add x0, sp, 8 (compute address), add sp, sp, 16 (deallocate). x86-64 uses similar pattern with subq $0x10, %rsp, movl operations, leaq for address, addq $0x10, %rsp.", "notes_text": null, "keywords": ["stack variables", "local array", "stack allocation", "stack addressing"], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "code_comparison"}, "section": "Memory Operations", "metadata": {"course": "CS356", "unit": 9, "topics": ["stack variable access"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 14, "title": "Addressing Modes: Array Access", "summary": "Demonstrates scaled indexed addressing for array element swapping.", "main_text": "C function array_swap(a, i, j) swaps a[i] and a[j]. ARM64: sxtw x1, w1 (sign-extend i), ldr w3, [x0, x1, lsl 2] (load a[i] with scaled index), sxtw x2, w2 (sign-extend j), ldr w4, [x0, x2, lsl 2] (load a[j]), str w4, [x0, x1, lsl 2] (store to a[i]), str w3, [x0, x2, lsl 2] (store to a[j]). x86-64 uses movslq for sign extension and lea with scaled addressing. Both use lsl/shift by 2 for int array (4 bytes per element).", "notes_text": null, "keywords": ["array access", "scaled addressing", "array indexing", "sign extension"], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "code_comparison"}, "section": "Memory Operations", "metadata": {"course": "CS356", "unit": 9, "topics": ["array operations"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 15, "title": "Control Flow", "summary": "Introduces ARM64 conditional branches and flag-setting instructions.", "main_text": "Similar style, different instruction names. To set flags without changing registers, use instructions with s suffix: subs x0, x1 or its alias cmp x0, x1 computes x0-x1, adds x0, x1 or its alias cmn x0, x1 computes x0+x1, ands x0, x1 or its alias tst x0, x1 computes x0&x1. Similar flags: Z (zero), N (negative), C (carry), V (overflow). Similar conditional jumps: beq (branch equal), bne (not equal), bgt (greater than), bge (greater or equal), ble, blt, bmi (js), bhi (unsigned higher), bhs (unsigned higher or same), bls, blo. Unconditional branch: b label. Examples: cmp x1, x2 followed by blt .L1 jumps to .L1 if x1 < x2. ands x1, x1 followed by beq .L1 jumps to .L1 if x1 == 0.", "notes_text": null, "keywords": ["control flow", "conditional branches", "flags", "cmp", "beq", "blt", "bgt"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Control Flow", "metadata": {"course": "CS356", "unit": 9, "topics": ["branches and conditions"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 16, "title": "ARM64: Adding up array elements", "summary": "Complete ARM64 implementation of array sum with loop.", "main_text": "C function array_sum(a, n) sums array elements. ARM64: mov x4, x0 (move array address to x4), mov w2, 0 (set index w2 to 0), mov w0, 0 (set total w0 to 0), b .L2 (jump to .L2). Loop .L3: ldr w3, [x4, w2, sxtw 2] (w3 = x4[w2 extended to 64, shifted left by 2]), add w0, w0, w3 (w0 = w0 + w3), add w2, w2, 1 (w2 = w2 + 1). Test .L2: cmp w2, w1 (compare index w2 with bound w1), blt .L3 (if w2 < w1, jump to .L3), ret (return, total is in w0).", "notes_text": null, "keywords": ["array sum", "loop implementation", "ARM64 loop", "array traversal"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "code"}, "section": "Control Flow", "metadata": {"course": "CS356", "unit": 9, "topics": ["loop example"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 17, "title": "x86-64: Adding up array elements", "summary": "x86-64 implementation of array sum for comparison.", "main_text": "C function array_sum(a, n) sums array elements. x86-64: movl $0, %eax (set index eax to 0), movl $0, %edx (set total edx to 0), jmp .L2 (jump to comparison). Loop .L3: movslq %eax, %rcx (extend index eax to 8 bytes in rcx), addl (%rdi,%rcx,4), %edx (add 4 bytes at rdi+rcx*4 to total edx), addl $1, %eax (increase index by 1). Test .L2: cmpl %esi, %eax (compare eax and esi), jl .L3 (if eax < esi, jump to .L3), movl %edx, %eax (else move total to eax), ret (return value in eax).", "notes_text": null, "keywords": ["array sum", "x86-64 loop", "loop comparison"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "code"}, "section": "Control Flow", "metadata": {"course": "CS356", "unit": 9, "topics": ["loop comparison"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 18, "title": "Calling Conventions", "summary": "ARM64 procedure calling conventions and call/return mechanism.", "main_text": "Conventions: Arguments in x0 through x7 then on the stack, Return value in x0, Caller-save registers x0 to x18, Callee-save registers x19 to x29, Callee saves link register x30 if it invokes a procedure. Call/Return Mechanism: Branch with link (bl) sets the link register x30 (lr) to PC+4 where PC is the address of the current instruction (each is 4 bytes), Return (ret) jumps to the address in x30 (lr), can also use ret x0 or with any other register.", "notes_text": null, "keywords": ["calling conventions", "ABI", "caller-save", "callee-save", "link register", "branch with link", "bl", "ret"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Procedure Calls", "metadata": {"course": "CS356", "unit": 9, "topics": ["calling conventions"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 19, "title": "ARM64: Procedure Calls in Action", "summary": "Detailed example of function call with 10 arguments in ARM64.", "main_text": "C functions: args_sum takes 10 int arguments and returns their sum, args_sum_example calls it with values 1-10. args_sum: adds w0-w7, then ldr w7, [sp] (get 9th arg), add w0, w0, w7, ldr w1, [sp, 8] (get 10th arg), add w0, w0, w1, ret. args_sum_example: sub sp, sp, #32 (allocate), stp x29, x30, [sp, 16] (save frame pointer and link register), add x29, sp, 16 (set frame pointer), mov w0, 10; str w0, [sp, 8] (push 10th arg), mov w0, 9; str w0, [sp] (push 9th arg), mov w7-w0 with values 8-1, bl args_sum (call), ldp x29, x30, [sp, 16] (restore), add sp, sp, 32 (deallocate), ret.", "notes_text": null, "keywords": ["procedure call", "function arguments", "stack arguments", "stp", "ldp", "frame pointer"], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "code"}, "section": "Procedure Calls", "metadata": {"course": "CS356", "unit": 9, "topics": ["procedure call example"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 20, "title": "x86-64: Procedure Calls in Action", "summary": "x86-64 implementation of same 10-argument function call.", "main_text": "C functions: args_sum takes 10 int arguments, args_sum_example calls it with values 1-10. args_sum: addl %esi, %edi through addl %r8d, %edi for first 5 args, leal (%rdi,%r9), %eax (add 6th), addl 8(%rsp), %eax through addl 32(%rsp), %eax for args 7-10, ret. args_sum_example: pushq $10 through pushq $7 (push args 10-7), movl $6, %r9d through movl $1, %edi (load args 6-1 into registers), call args_sum, addq $32, %rsp (clean up stack), ret.", "notes_text": null, "keywords": ["x86-64 calling", "push arguments", "call instruction"], "images": [], "layout": {"num_text_boxes": 3, "num_images": 0, "dominant_visual_type": "code"}, "section": "Procedure Calls", "metadata": {"course": "CS356", "unit": 9, "topics": ["x86 procedure comparison"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 21, "title": "Try It Yourself in the Assembly Dojo", "summary": "Reference to practice exercises repository.", "main_text": "Try It Yourself in the Assembly Dojo - https://github.com/usc-cs356/assembly-dojo", "notes_text": null, "keywords": ["practice", "assembly dojo", "exercises", "GitHub"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Resources", "metadata": {"course": "CS356", "unit": 9, "topics": ["practice resources"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 22, "title": "A64 Reference Card", "summary": "Link to official ARM documentation reference card.", "main_text": "A64 Reference Card - https://documentation-service.arm.com/static/5ed66080ca06a95ce53f932d", "notes_text": null, "keywords": ["reference card", "ARM documentation", "instruction reference"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Resources", "metadata": {"course": "CS356", "unit": 9, "topics": ["documentation"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 23, "title": "Bonus Material", "summary": "Section divider for additional advanced topics.", "main_text": "Bonus Material", "notes_text": null, "keywords": ["bonus", "advanced topics", "section divider"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_header"}, "section": "Bonus Material", "metadata": {"course": "CS356", "unit": 9, "topics": ["advanced content"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 24, "title": "Extension and Truncation", "summary": "Explains load/store suffixes for different data sizes with extension.", "main_text": "Default: transfer 4 bytes (w registers) or 8 bytes (x registers). To force a smaller load/store, use a suffix: b (zero-extend) or sb (sign-extend) for 1 byte - ldrb w0, [x1] reads 1 byte at address x1, zero extend to 4 bytes (w0), strb w0, [x1] writes least-significant byte of w0 to address x1. h (zero-extend) or sh (sign-extend) for 2 bytes - ldrsh x0, [x1] reads 2 bytes at address x1, sign extend to 8 bytes, strh w0, [x1] writes least-significant 2 bytes of w0 to address x1. sw (sign-extend) for 4 bytes - ldrsw x0, [x1] reads 4 bytes at address x1, sign extend to 8 bytes, str w0, [x1] is used instead of ldrw (which doesn't exist).", "notes_text": null, "keywords": ["extension", "truncation", "ldrb", "ldrh", "ldrsw", "strb", "strh", "sign-extend", "zero-extend"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Bonus Material", "metadata": {"course": "CS356", "unit": 9, "topics": ["data size operations"]}}
{"deck_name": "CS356Unit09_ARM", "slide_number": 25, "title": "Zero/Sign Extension from Memory", "summary": "Table showing all load instruction variants for extension from memory.", "main_text": "Table showing Source (memory) to Destination (w0 or x0 register) mappings: 1 byte at address x1 can use ldrb w0,[x1] (zero to w0), ldrsb w0,[x1] (sign to w0), ldrb w0,[x1] (zero to x0), ldrsb x0,[x1] (sign to x0). 2 bytes at address x1 can use ldrh w0,[x1] (zero to w0), ldrsh w0,[x1] (sign to w0), ldrh w0,[x1] (zero to x0), ldrsh x0,[x1] (sign to x0). 4 bytes at address x1 can use ldr w0,[x1] (to w0 or x0), ldrsw x0,[x1] (sign to x0). 8 bytes at address x1 uses ldr x0,[x1].", "notes_text": null, "keywords": ["load variants", "extension table", "memory to register"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "table"}, "section": "Bonus Material", "metadata": {"course": "CS356", "unit": 9, "topics": ["load instruction reference"]}}
{"deck_name":"CS356Unit09_ARM","slide_number":26,"title":"Zero Extension, Sign Extension, and Truncation","summary":"Explains how ARM64 handles 32-bit to 64-bit register operations including zero/sign extension and truncation.","main_text":"Zero Extension: When writing to w register, the top 32 bits of corresponding x register are set to 0. Sign Extension: Use special instructions like sxtw to sign-extend a 32-bit value to 64 bits. Truncation: Reading from w register only accesses the lower 32 bits of x register, effectively truncating the value.","notes_text":null,"keywords":["zero extension","sign extension","truncation","ARM64","register width"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"section":"Registers and Instructions","metadata":{"course":"CS356","unit":9,"topics":["register operations"]}}
{"deck_name":"CS356Unit09_ARM","slide_number":27,"title":"Compare with x86-64","summary":"Contrasts ARM64 register behavior with x86-64 when writing 32-bit values.","main_text":"In ARM64: writing to w register zero-extends the upper 32 bits automatically. In x86-64: writing to 32-bit register (eax) also zero-extends into rax. However, writing to 16-bit or 8-bit registers may leave upper bits unchanged. ARM behavior is more consistent across sizes.","notes_text":null,"keywords":["ARM64 vs x86-64","register semantics","zero extension"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"section":"Registers and Instructions","metadata":{"course":"CS356","unit":9,"topics":["architecture comparison"]}}
{"deck_name":"CS356Unit09_ARM","slide_number":28,"title":"Saving PC + Offset for Later","summary":"Describes how ARM instructions save PC-relative addresses using the link register.","main_text":"Some instructions store PC + offset into a register for later use. This is important for implementing function calls and position-independent code. The link register (x30) is commonly used to store the return address.","notes_text":null,"keywords":["PC-relative","link register","function calls","ARM64"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"section":"Control Flow","metadata":{"course":"CS356","unit":9,"topics":["program counter"]}}
{"deck_name":"CS356Unit09_ARM","slide_number":29,"title":"Use: PC-Relative References","summary":"Shows practical use of PC-relative addressing in ARM64.","main_text":"PC-relative references are used for accessing constants and global data in a position-independent way. This is common in modern compilers to allow code to be relocatable in memory without modification.","notes_text":null,"keywords":["PC-relative addressing","position independent code","ARM64"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"section":"Control Flow","metadata":{"course":"CS356","unit":9,"topics":["addressing modes"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 1, "title": "Unit 10: Caches", "summary": "Title slide introducing the cache memory unit and its role in improving memory access performance.", "main_text": "Unit 10: Caches\nImproving Memory Access Performance", "notes_text": null, "keywords": ["caches", "memory hierarchy", "CS356"], "images": [{"type": "logo", "description": "CS356 Trojan logo and university branding"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "title_slide"}, "section": "Introduction", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache overview"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 2, "title": "Why Caches?", "summary": "Introduces the motivation for caches by highlighting the growing CPU-memory performance gap.", "main_text": "CPU speed has increased much faster than main memory speed. Caches are used to reduce the average memory access time by storing frequently used data closer to the processor.", "notes_text": null, "keywords": ["cpu-memory gap", "performance", "latency", "cache motivation"], "images": [{"type": "diagram", "description": "Graph showing widening gap between CPU performance and memory speed"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "comparison"}, "section": "Cache Motivation", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache necessity"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 3, "title": "Memory Hierarchy", "summary": "Displays the layered structure of memory with tradeoffs between speed, size, and cost.", "main_text": "The memory hierarchy consists of registers, L1, L2, L3 caches, main memory, and disk storage, each varying in speed, cost, and capacity.", "notes_text": null, "keywords": ["memory hierarchy", "l1 cache", "l2 cache", "latency"], "images": [{"type": "diagram", "description": "Pyramid showing levels of memory hierarchy"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Cache Architecture", "metadata": {"course": "CS356", "unit": 10, "topics": ["memory structure"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 4, "title": "Principle of Locality", "summary": "Explains temporal and spatial locality as foundational reasons caches improve performance.", "main_text": "Temporal locality: recently accessed data is likely to be accessed again soon. Spatial locality: nearby data is likely to be accessed soon. Caches exploit these behaviors to reduce latency.", "notes_text": null, "keywords": ["locality", "temporal locality", "spatial locality"], "images": [{"type": "diagram", "description": "Memory access pattern illustrating clustered and repetitive usage"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Locality", "metadata": {"course": "CS356", "unit": 10, "topics": ["locality principle"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 5, "title": "Cache Terminology", "summary": "Introduces fundamental terms such as hit, miss, and eviction in cache operation.", "main_text": "A cache hit occurs when requested data is found in the cache. A miss occurs when it is not. When the cache is full, an existing block must be evicted.", "notes_text": null, "keywords": ["hit", "miss", "eviction", "cache block"], "images": [{"type": "flowchart", "description": "Flow of memory request through cache system"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Cache Basics", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache terminology"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 6, "title": "Cache Organization", "summary": "Introduces how caches are structured into blocks, lines, and sets.", "main_text": "The cache is divided into blocks (also called lines). Each block holds a fixed-size chunk of data from memory. These blocks are organized into sets depending on the cache mapping technique.", "notes_text": null, "keywords": ["cache organization", "blocks", "cache lines", "sets"], "images": [{"type": "diagram", "description": "Block-based structure of a cache with labeled lines"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Cache Architecture", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache structure"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 7, "title": "Direct-Mapped Cache", "summary": "Explains how each memory block maps to exactly one cache line.", "main_text": "In a direct-mapped cache, each block of main memory maps to exactly one location in the cache. While simple and fast, this can lead to conflict misses.", "notes_text": null, "keywords": ["direct mapped", "cache mapping", "conflict miss"], "images": [{"type": "diagram", "description": "Mapping from memory blocks to single cache lines"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Cache Mapping", "metadata": {"course": "CS356", "unit": 10, "topics": ["direct mapping"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 8, "title": "Fully Associative Cache", "summary": "Shows that any memory block can be placed in any cache line.", "main_text": "Fully associative caches allow any block of main memory to be stored in any cache line. This reduces conflict but increases lookup complexity.", "notes_text": null, "keywords": ["fully associative", "cache", "flexible mapping"], "images": [{"type": "diagram", "description": "Memory blocks mapping to any cache location"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Cache Mapping", "metadata": {"course": "CS356", "unit": 10, "topics": ["associative mapping"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 9, "title": "Set-Associative Cache", "summary": "Combines features of direct-mapped and fully associative caches.", "main_text": "In set-associative caches, each block maps to a specific set but can be stored in any line within that set, balancing speed and flexibility.", "notes_text": null, "keywords": ["set-associative", "mapping", "cache sets"], "images": [{"type": "diagram", "description": "Block mapping into cache sets"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Cache Mapping", "metadata": {"course": "CS356", "unit": 10, "topics": ["set associative"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 10, "title": "Replacement Policies", "summary": "Introduces policies for deciding which cache line to evict.", "main_text": "When a cache is full, replacement policies determine which line to remove. Common strategies include LRU, FIFO, and Random replacement.", "notes_text": null, "keywords": ["replacement policy", "LRU", "FIFO", "random eviction"], "images": [{"type": "chart", "description": "Comparison table of eviction strategies"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "comparison"}, "section": "Replacement", "metadata": {"course": "CS356", "unit": 10, "topics": ["replacement policies"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 11, "title": "Write Policies", "summary": "Explains how writes are handled in cache systems.", "main_text": "Write-through immediately updates main memory, while write-back delays updates until eviction. Each approach has performance and consistency tradeoffs.", "notes_text": null, "keywords": ["write policy", "write-back", "write-through"], "images": [{"type": "diagram", "description": "Write operation flow through cache"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Write Strategies", "metadata": {"course": "CS356", "unit": 10, "topics": ["write policies"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 12, "title": "Cache Performance Metrics", "summary": "Defines how cache effectiveness is measured.", "main_text": "Cache performance is measured using hit rate, miss rate, miss penalty, and average memory access time (AMAT). These metrics evaluate efficiency.", "notes_text": null, "keywords": ["AMAT", "hit rate", "miss rate", "performance"], "images": [{"type": "formula", "description": "AMAT calculation equation"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "text-heavy"}, "section": "Performance", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache performance"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 13, "title": "Cache Optimization Techniques", "summary": "Introduces methods for improving cache efficiency and reducing misses.", "main_text": "Techniques such as blocking, prefetching, and loop transformation improve locality and reduce cache miss rates.", "notes_text": null, "keywords": ["optimization", "prefetching", "blocking", "loop tiling"], "images": [{"type": "diagram", "description": "Illustration of optimized loop access patterns"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Optimization", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache optimization"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 14, "title": "Policies & Metadata", "summary": "Section divider introducing cache policies and metadata management.", "main_text": "Policies & Metadata", "notes_text": null, "keywords": ["policies", "metadata", "section divider"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_header"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache policies"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 15, "title": "Designing a Cache", "summary": "Overview of cache design considerations including write policy, eviction policy, and organization.", "main_text": "Caching is everywhere: L1/L2/L3 caches, paging between MM and disk, file I/O, browser pages/images, DNS. Design issues include: Write Policy (whether to modify data just in cache or persist changes), Eviction Policy (deciding what to remove when full), Organization (keeping track of cache content, selecting block size for spatial locality).", "notes_text": null, "keywords": ["cache design", "write policy", "eviction policy", "organization", "caching applications"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache design principles"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 16, "title": "Terminology: Hit, Miss, Eviction", "summary": "Defines fundamental cache operations: hit, miss, and eviction with visual flow.", "main_text": "When the processor generates a memory read or write, it first checks cache: Yes means hit (get data quickly from cache), No means miss (must load a block and store in cache for future accesses). If cache is full, must evict some other block. Blocks are larger than requested data to exploit spatial locality and start on addresses that are multiples of block size (e.g., 64 B).", "notes_text": null, "keywords": ["hit", "miss", "eviction", "cache operation", "block alignment"], "images": [{"type": "flowchart", "description": "Flow showing request at 0x400028, cache miss, loading cache line 400020-40003f, memory response, cache forwarding, and subsequent cached access"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache terminology"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 17, "title": "Write Policy", "summary": "Introduces write-through and write-back policies for handling write hits.", "main_text": "In case of a write hit, what should we do? Write Through Policy: Update data in the cache and in the following level. Write Back Policy: Update data only in current level, keep track of which blocks have modified data, when a modified block is evicted or cache is flushed, write it back to the next level to persist changes.", "notes_text": null, "keywords": ["write policy", "write-through", "write-back", "write hit"], "images": [{"type": "diagram", "description": "CPU, Registers, Cache, Main Memory hierarchy"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["write policies"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 18, "title": "Write Through", "summary": "Details write-through policy characteristics and performance implications.", "main_text": "We immediately update the block in the current level and the next one. Keeps both versions in synch at all times. Can write just the modified portion. Poor performance when: next level of hierarchy is slow (e.g., L3 to MM, or MM to disk), there are many repeated writes to the same block.", "notes_text": null, "keywords": ["write-through", "synchronization", "performance", "memory hierarchy"], "images": [{"type": "diagram", "description": "CPU, Registers, Cache, Main Memory with immediate update flow"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["write-through policy"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 19, "title": "Write Back", "summary": "Explains write-back policy with delayed persistence and dirty bit tracking.", "main_text": "Update the block only in the current level and write it to the next level only when it is evicted (write only the final version, not intermediate ones). Next level can be out-of-date. Need to track modified blocks and write back entire blocks (instead of just modified portions, which would be too difficult to track). Writing to current level is faster! Write back is definitely faster when the next level is much slower (e.g., in L3 or for memory paging); in practice, used.", "notes_text": null, "keywords": ["write-back", "dirty bit", "delayed write", "eviction"], "images": [{"type": "flowchart", "description": "4-step process: 1) Update only block in cache, 2) Make other updates in cache, 3) When cache full and need to evict modified block, 4) write back block and then evict"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["write-back policy"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 20, "title": "Eviction Policy", "summary": "Compares LRU, FIFO, and Random eviction policies.", "main_text": "What block to evict when the cache is full? LRU: The least recently used block - usually the best policy because it exploits temporal locality, hard to implement, often just approximated with easier policies. FIFO: The oldest block - not always good, the block may be old but we may have accessed it several times, even recently. Random: Pick a block randomly - performs surprisingly well.", "notes_text": null, "keywords": ["eviction policy", "LRU", "FIFO", "random", "temporal locality"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["eviction policies"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 21, "title": "Block Metadata", "summary": "Introduces the metadata needed for each cache block.", "main_text": "Data of cache blocks doesn't tell us: 1) Where in memory (range of addresses) a block comes from - we need to know to check whether it's a hit or miss, 2) Whether the block was modified - if modified, we have to write it back to memory before evicting, 3) When the block was last accessed - to implement LRU or pseudo-LRU.", "notes_text": null, "keywords": ["metadata", "cache block", "address tag", "modified bit", "LRU tracking"], "images": [{"type": "diagram", "description": "CPU with registers, ALU, cache, and main memory showing metadata requirements"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache metadata"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 22, "title": "Block Metadata: Example", "summary": "Concrete example showing valid bit, modified bit, and address range for cache blocks.", "main_text": "Valid Bit: Whether we already have data on this block of cache (valid=0 if line is empty and we ignore remaining metadata). Modified Bit: Whether we have modified this block (i.e., only the cached copy has been updated, we need to write back in case of eviction/flush). Address Range: Where the data is coming from, e.g., memory addresses 0x7c0 to 0x7cf. Do we really need to store the entire address range? No, just the tag 0x7c.", "notes_text": null, "keywords": ["valid bit", "modified bit", "dirty bit", "tag", "address range"], "images": [{"type": "diagram", "description": "Cache showing four blocks with metadata: 0x7c0-7cf (Valid, Modified), 0x470-47f (Valid, Unmodified), two empty blocks"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["metadata example"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 23, "title": "Splitting an Address", "summary": "Explains how memory addresses are split into block number and byte offset.", "main_text": "Given a memory address of m = n + k bits (e.g., m = 12 bits, split into n = 8 plus k = 4): There are 2^m addresses, each pointing to a different byte in memory (e.g., 0x000 to 0xFFF). The most significant n bits (e.g., 0xFD) identify a memory block of 2^k bytes. The block start address has all 0's for the least significant k bits (e.g., 0xFD0). The block end address has all 1's for the least significant k bits (e.g., 0xFDF). All addresses in the block have the same initial n bits, which tell us where the block is in memory.", "notes_text": null, "keywords": ["address splitting", "block number", "byte offset", "address fields"], "images": [{"type": "diagram", "description": "Address breakdown showing A[11:4] as Block # and A[3:0] as Byte # in block, with memory blocks from 0x000 to 0xFF0"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["address structure"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 24, "title": "Different Split", "summary": "Shows address splitting with different bit allocations (7+5 instead of 8+4).", "main_text": "What if we split an address of m = 12 bits into n = 7 plus k = 5 bits? Still 2^12 addresses, 0x000 to 0xFFF. The block is identified by the most significant 7 bits (e.g., 1111 110). The block start address has all 0's for the least significant k bits: 1111 110 0 0000 = 0xFC0. The block end address has all 1's for the least significant k bits: 1111 110 1 1111 = 0xFDF. Blocks include 2^5 = 32 bytes. Note how we convert hex to binary, since the split is not at a multiple of 4 bits.", "notes_text": null, "keywords": ["address splitting", "binary conversion", "block size", "bit allocation"], "images": [{"type": "diagram", "description": "Address showing A[11:5] as Block # and A[4:0] as Byte # in block, with example 0xFD4 = 1111 110 1 0100"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["address splitting variations"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 25, "title": "Analogy: Hotel Rooms", "summary": "Uses hotel room numbering as analogy for address block identification.", "main_text": "To refer to the range of rooms on the second floor, left aisle we would just say rooms 20x. 1st Digit = Floor, 2nd Digit = Aisle, 3rd Digit = Room within aisle. For 4 word (16-byte) blocks: Addr Range 000-00f (Binary: 0000 0000 0000..1111), 010-01f (Binary: 0000 0001 0000..1111). For 8 word (32-byte) blocks: Addr Range 000-01f (Binary: 0000 000 00000..11111), 020-03f (Binary: 0000 001 00000..11111).", "notes_text": null, "keywords": ["analogy", "hotel rooms", "address ranges", "block identification"], "images": [{"type": "diagram", "description": "Hotel layout showing 1st and 2nd floors with rooms numbered 100-109, 120-129, 200-209, 220-229"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["address analogy"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 26, "title": "Revised Metadata", "summary": "Shows complete metadata structure with valid bit, dirty bit, and tag.", "main_text": "Only valid bit, dirty bit, tag (for now, we ignore metadata for eviction policy). Examples shown with cache blocks containing tags like T=0111 1100 (V=1 D=0) for address 0x7c0-7cf, and T=0100 0111 (V=1 D=1) for 0x470-47f. Address splitting shown as A[11:4] for Tag and A[3:0] for Byte offset. Example addresses: 0x7c4 = 01000111 1100 (Byte 4 within block 7c0-7cf), 0xaf1 = 00011010 1111 (Byte 1 within block af0-aff).", "notes_text": null, "keywords": ["metadata structure", "valid bit", "dirty bit", "tag bits", "address fields"], "images": [{"type": "diagram", "description": "Cache and Memory diagram showing cache blocks with metadata (T, V, D bits) and corresponding memory blocks"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Policies & Metadata", "metadata": {"course": "CS356", "unit": 10, "topics": ["metadata implementation"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 27, "title": "Organization & Lookup", "summary": "Section divider introducing fully-associative, set-associative, and direct mapping.", "main_text": "Organization & Lookup: Fully-Associative, Set-Associative, Direct", "notes_text": null, "keywords": ["organization", "lookup", "mapping schemes", "section divider"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_header"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache organization"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 28, "title": "Lookup: Fully Associative Cache", "summary": "Explains fully associative cache where blocks can be placed anywhere.", "main_text": "If we keep all cache blocks in a single set (fully associative cache), to find whether the cache has data of a given address, we need to compare the tag with those of all blocks. Everyday Example: You lost your keys - You think back to where you have been lately: library, class, gym. Where do you have to look to find your keys? If you had been home all day and discovered your keys were missing, where would you look? Key lesson: If something can be anywhere you have to search everywhere. By contrast, if we limit where things can be then our search need only look in those limited places.", "notes_text": null, "keywords": ["fully associative", "cache lookup", "search everywhere", "tag comparison"], "images": [{"type": "diagram", "description": "Cache with four blocks showing tags and valid/dirty bits"}], "layout": {"num_text_boxes": 3, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["fully associative cache"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 29, "title": "Parallel Tag Comparison", "summary": "Shows hardware implementation of parallel tag comparison for fully associative cache.", "main_text": "Requires additional hardware, expensive when caches have many blocks (> 16 or 32). Address = A[11:0] = 0x47c, Tag = A[11:4] = 0x47, Byte Offset A[3:0] = 0xc. When a block can be anywhere you have to search everywhere. Shows parallel comparators checking tag 0100 0111 against all cache block tags simultaneously, with valid bits ANDed, results ORed for HIT/MISS signal.", "notes_text": null, "keywords": ["parallel comparison", "hardware cost", "tag matching", "fully associative"], "images": [{"type": "circuit_diagram", "description": "Logic diagram showing parallel comparators, AND gates with valid bits, OR gate for hit/miss detection"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["parallel tag comparison"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 30, "title": "K-Way Set-Associative Cache", "summary": "Introduces set-associative cache as compromise between fully associative and direct mapped.", "main_text": "Idea: Given a cache of N blocks, split the cache into S = N/K sets, each with K blocks (ways). Split the address into 3 parts: tag | set index | block offset. Search for the tag only inside the set given by the index. On a miss, save inside the set given by the index (if full, evict a line in the set). Lookup is faster (up to K comparisons), but we have to evict from a specific set - other sets may have empty or less recently used lines.", "notes_text": null, "keywords": ["set-associative", "K-way", "sets", "ways", "tag index offset"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["set-associative cache"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 31, "title": "K-Way Set-Associative Cache", "summary": "Detailed example of 2-way set-associative cache with address mapping.", "main_text": "12-bit addresses, B=16 bytes/block, K=2 ways. Offset: B=16 bytes per block, log2(B) = 4 offset bits, determines byte/word within the block. Set: S=N/K=2 sets, log2(S) = 1 set bit, performs hash function (i mod S). Tag: Remaining bits, identifies blocks that map to the same bucket (block 0x00, 0x08, 0x0a, 0x0c). Example shows memory blocks mapping to Set 0 (even) or Set 1 (odd). Write 0x084 maps to Set 0, Block 0x080-0x08f can be placed anywhere in set 0.", "notes_text": null, "keywords": ["2-way set-associative", "address fields", "set mapping", "hash function"], "images": [{"type": "diagram", "description": "Cache with Set 0 and Set 1, showing blocks with tags, memory blocks 08-0c, address field breakdown"}], "layout": {"num_text_boxes": 3, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["set-associative example"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 32, "title": "K-Way Set-Associative Cache", "summary": "Shows cache operation after placing first block in set 0.", "main_text": "Continuing previous example: We picked the first line in set 0 for block 0x080-0x08f with T=0000 100, V=1, D=1. Shows the cache state after write to 0x084 has been processed and stored in Cache Block 0 of Set 0.", "notes_text": null, "keywords": ["cache placement", "set selection", "write operation"], "images": [{"type": "diagram", "description": "Cache showing Set 0 with populated block 0x080-0x08f, Set 1 empty"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache operation sequence"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 33, "title": "K-Way Set-Associative Cache", "summary": "Shows multiple operations filling both sets.", "main_text": "Operations: Write 0x084 (Set 0), Read 0x0b0 (Set 1), Read 0x0c8 (Set 0). Then we picked the first line in set 1 and the second in set 0. Cache now contains: Set 0 with blocks 0x080-0x08f (T=0000 100) and 0x0c0-0x0cf (T=0000 110), Set 1 with block 0x0b0-0x0bf (T=0000 101).", "notes_text": null, "keywords": ["multiple operations", "set filling", "cache state"], "images": [{"type": "diagram", "description": "Cache showing both sets with multiple blocks populated"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache filling sequence"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 34, "title": "K-Way Set-Associative Cache", "summary": "Demonstrates cache conflict requiring eviction.", "main_text": "Operations include Read 0x0a4 which maps to Set 0. Set 0 is full, we can only store this line there (set idx is 0) - must evict! Shows the cache conflict situation where both ways in Set 0 are occupied but a new block needs to be placed there.", "notes_text": null, "keywords": ["cache conflict", "eviction needed", "set full"], "images": [{"type": "diagram", "description": "Cache showing Set 0 full with two blocks, new block 0x0a0-0x0af needs placement"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache conflict"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 35, "title": "K-Way Set-Associative Cache", "summary": "Shows eviction and write-back of dirty block.", "main_text": "We evict the LRU block (first line), but first we write it back (it's dirty). Block 0x080-0x08f is written back to memory before being replaced with 0x0a0-0x0af. Final cache state shows Set 0 with blocks 0x0a0-0x0af and 0x0c0-0x0cf, Set 1 with block 0x0b0-0x0bf.", "notes_text": null, "keywords": ["LRU eviction", "write-back", "dirty block", "replacement"], "images": [{"type": "diagram", "description": "Cache after eviction, showing write-back arrow from cache to memory"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["eviction process"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 36, "title": "Fully Associative Cache", "summary": "Defines fully associative as special case of set-associative.", "main_text": "Fully associative caches are a special case of set-associative caches where: S=1 (a single set), K=N (all lines in that set). Since s = log2(S) = log2(1) = 0, we don't need set index bits - we just split into tag and block offset: b = log2(B) bits for block offset, t = m - b bits for the tag.", "notes_text": null, "keywords": ["fully associative", "single set", "no index bits", "tag and offset"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["fully associative definition"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 37, "title": "Fully Associative Cache", "summary": "Example of fully associative cache operation with first write.", "main_text": "Block 0 can go in any empty cache block, but let's just pick cache block 2. Write 0x004 places block 0x000-0x00f in Cache Block 2 with tag T=0000 0000, V=1, D=1. Address split into Tag (8 bits) and Offset (4 bits), no set index needed.", "notes_text": null, "keywords": ["fully associative example", "flexible placement", "no set constraint"], "images": [{"type": "diagram", "description": "Cache with 4 blocks, memory blocks shown, block 0x000-0x00f placed in cache block 2"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["fully associative example"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 38, "title": "Fully Associative Cache", "summary": "Shows multiple operations filling empty blocks freely.", "main_text": "Blocks can go anywhere so the next 3 accesses will prefer to fill in empty blocks. Operations: Write 0x004, Read 0x018, Read 0xfe0, Read 0xffc. Cache now contains blocks: 0x000-0x00f (T=0000 0000), 0x010-0x01f (T=0000 0001), 0xfe0-0xfef (T=1111 1110), 0xff0-0xfff (T=1111 1111).", "notes_text": null, "keywords": ["multiple operations", "free placement", "filling cache"], "images": [{"type": "diagram", "description": "Cache with all 4 blocks populated from various memory locations"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["fully associative filling"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 39, "title": "Fully Associative Cache", "summary": "Demonstrates LRU eviction with write-back in fully associative cache.", "main_text": "Now cache is full so when we access a new block (0xfc0-0xfcf) we have to evict a block from cache. Let us pick the Least Recently Used (LRU). Since it is dirty/modified we must write 0x000-0x00f back to MM. Operations show Read 0xfc4 causing eviction of block 0x000-0x00f and replacement with 0xfc0-0xfcf (T=1111 1100).", "notes_text": null, "keywords": ["LRU eviction", "fully associative", "write-back", "cache full"], "images": [{"type": "diagram", "description": "Cache showing eviction with write-back arrow, new block 0xfc0-0xfcf replacing 0x000-0x00f"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["fully associative eviction"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 40, "title": "Direct Mapping", "summary": "Defines direct-mapped cache as special case of set-associative.", "main_text": "Direct-mapped caches are a special case of set-associative caches where: S=N (a set for each line), K=1 (obviously, one line per set). The set index is also called block index (since each block is a set). Address fields: b = log2(B) bits for block offset, s = log2(N) bits for block index, t = m - s - b bits for the tag.", "notes_text": null, "keywords": ["direct mapped", "one line per set", "block index", "fixed mapping"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["direct mapping definition"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 41, "title": "Direct Mapping", "summary": "Illustrates direct-mapped cache with modulo-based placement.", "main_text": "Each block from memory can only be put in one location. MM block i maps to cache block 'i mod N'. Each set has only 1 block. Examples show: MM Block 0 maps to Cache Block 0 (0 mod 4), MM Block 1 to Cache Block 1 (1 mod 4), MM Block 2 to Cache Block 2 (2 mod 4), MM Block 3 to Cache Block 3 (3 mod 4), MM Block 4 to Cache Block 0 (0 mod 4), MM Block 5 to Cache Block 1 (1 mod 4), MM Block 6 to Cache Block 2 (2 mod 4).", "notes_text": null, "keywords": ["direct mapped", "modulo mapping", "fixed location", "hash function"], "images": [{"type": "diagram", "description": "Cache with 4 blocks, memory blocks 0-6 with arrows showing mod 4 mapping"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["direct mapping example"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 42, "title": "Direct Mapping", "summary": "Example of placing block 0x080 in direct-mapped cache.", "main_text": "Address = 080, Offset: B=16 bytes per block, log2(B) = 4 offset bits, determines byte/word within the block. Block: N=4 blocks in the cache, log2(N) = 2 block bits, performs hash function (i mod N). Tag: Remaining bits, identifies blocks that map to the same bucket (block 0, 4, 8). Block 0x080-0x08f hashes/maps to cache block 0 and thus must be placed there! Write 0x084 with tag 0000 10, block 00.", "notes_text": null, "keywords": ["direct mapping", "address breakdown", "fixed placement", "block calculation"], "images": [{"type": "diagram", "description": "Cache blocks 0-3, memory blocks 08-0c, address field breakdown showing tag|block|offset"}], "layout": {"num_text_boxes": 3, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["direct mapping operation"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 43, "title": "Direct Mapping", "summary": "Shows multiple operations in direct-mapped cache.", "main_text": "Other blocks must be placed where they hash which is computed by simply using the block bits. Operations: Write 0x084 (block 00, tag 0000 10), Read 0x09c (block 01, tag 0000 10), Read 0x0b8 (block 11, tag 0000 10), Read 0x0c8 (block 00, tag 0000 11). Cache contains blocks at positions determined by block index: 0x080-0x08f at position 0, 0x090-0x09f at position 1, 0x0b0-0x0bf at position 3, 0x0c0-0x0cf at position 0.", "notes_text": null, "keywords": ["direct mapping operations", "block placement", "hash calculation"], "images": [{"type": "diagram", "description": "Cache showing blocks placed according to block index bits"}], "layout": {"num_text_boxes": 3, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["direct mapping sequence"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 44, "title": "Summary of Mapping Schemes", "summary": "Compares fully associative, direct-mapped, and set-associative caches.", "main_text": "Fully associative: Most flexible (fewer evictions), longest search time O(N), no hashing, can be placed anywhere in cache, must search N locations. Direct-mapped cache: Least flexible (more evictions), shortest search time O(1), h(a) = block field, only search 1 location. K-way Set Associative mapping: Compromise - 1-way set associative = Direct, N-way set associative = Fully Assoc., work to search is O(K), for small K search in parallel: O(1), h(a) = set field, only search k locations.", "notes_text": null, "keywords": ["mapping comparison", "flexibility tradeoffs", "search complexity", "cache organization"], "images": [{"type": "diagram", "description": "Three address breakdowns: Fully Associative (Tag|Offset), Direct Mapped (Tag|Block|Offset), Set Associative (Tag|Set|Offset)"}], "layout": {"num_text_boxes": 4, "num_images": 1, "dominant_visual_type": "comparison"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["mapping schemes summary"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 45, "title": "Practice Example", "summary": "Practice problem setup for cache address mapping.", "main_text": "16-bit addresses, 2 kB cache, 32 bytes/block. Find address mapping for: Fully Associative, Direct Mapping, 4-way Set Associative, 8-way Set Associative. Questions: B? N? S, K?", "notes_text": null, "keywords": ["practice problem", "cache parameters", "address mapping"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["practice problem"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 46, "title": "Solution", "summary": "Calculates cache parameters B, N, and S for practice problem.", "main_text": "First find parameters: B = Block size, N = Cache blocks, S = Sets for 4-way and 8-way. B is given as 32 bytes/block. N depends on cache size and block size: N = (2 kB) / (32 bytes/block) = (2^11 / 2^5) = 2^6 = 64 blocks in the cache. S for 4-way & 8-way: S_4-way = N/k = 64/4 = 16 sets, S_8-way = N/k = 64/8 = 8 sets.", "notes_text": null, "keywords": ["parameter calculation", "cache size", "block count", "set count"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["solution derivation"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 47, "title": "Solution: Fully Associative", "summary": "Address breakdown for fully associative cache.", "main_text": "log2(32) = 5 byte/offset bits (A4-A0). Tag = 11 upper bits (A15-A5). Parameters: B = 32, N = 64, S_4-way = 16, S_8-way = 8.", "notes_text": null, "keywords": ["fully associative solution", "address fields", "tag bits", "offset bits"], "images": [{"type": "diagram", "description": "16-bit address showing Tag (bits 15-5) and Offset (bits 4-0)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["fully associative solution"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 48, "title": "Solution: Direct Mapping", "summary": "Address breakdown for direct-mapped cache.", "main_text": "log2(32) = 5 word bits (A4-A0). log2(64) = 6 block bits (A10-A5). Tag = 5 upper bits (A15-A11). Parameters: B = 32, N = 64, S_4-way = 16, S_8-way = 8.", "notes_text": null, "keywords": ["direct mapping solution", "block index", "tag calculation"], "images": [{"type": "diagram", "description": "16-bit address showing Tag (15-11), Block (10-5), Offset (4-0)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["direct mapping solution"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 49, "title": "Solution: Set Associative, K=4", "summary": "Address breakdown for 4-way set-associative cache.", "main_text": "log2(32) = 5 word bits (A4-A0). log2(16) = 4 set bits (A8-A5). Tag = 7 upper bits (A15-A9). Parameters: B = 32, N = 64, S_4-way = 16, S_8-way = 8.", "notes_text": null, "keywords": ["4-way set-associative", "set index", "address solution"], "images": [{"type": "diagram", "description": "16-bit address showing Tag (15-9), Set (8-5), Offset (4-0)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["4-way solution"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 50, "title": "Solution: Set Associative, K=8", "summary": "Address breakdown for 8-way set-associative cache.", "main_text": "log2(32) = 5 word bits (A4-A0). log2(8) = 3 set bits (A7-A5). Tag = 8 upper bits (A15-A8). Parameters: B = 32, N = 64, S_4-way = 16, S_8-way = 8.", "notes_text": null, "keywords": ["8-way set-associative", "set bits", "tag length"], "images": [{"type": "diagram", "description": "16-bit address showing Tag (15-8), Set (7-5), Offset (4-0)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["8-way solution"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 51, "title": "Practice Example", "summary": "Additional practice problem slide.", "main_text": "Practice Example", "notes_text": null, "keywords": ["practice", "additional problem"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_header"}, "section": "Organization & Lookup", "metadata": {"course": "CS356", "unit": 10, "topics": ["practice"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 52, "title": "Access Time & Design Tradeoffs", "summary": "Section divider introducing cache performance analysis.", "main_text": "Access Time & Design Tradeoffs", "notes_text": null, "keywords": ["access time", "design tradeoffs", "performance", "section divider"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_header"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache performance"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 53, "title": "Average Access Time", "summary": "Derives formula for average memory access time (AMAT).", "main_text": "Average Access Time = (Hit Rate) × (Hit Time) + (Miss Rate) × [(Hit Time) + (Miss Penalty)] = [(Hit Rate) + (Miss Rate)] × (Hit Time) + (Miss Rate) × (Miss Penalty) = (Hit Time) + (Miss Rate) × (Miss Penalty). Definitions: Hit Rate = (# hits)/(# accesses), fraction of accesses that find data in the cache. Miss Rate = (# misses)/(# accesses) = 1 – (Hit Rate). Hit Time = Time to lookup and retrieve data from the cache. Miss Penalty = Additional time to load data into the cache after a miss. Intuition: In either case (hit or miss), we still need to lookup and retrieve data from the cache (hit time); in case of a miss, we also pay the miss penalty to load data into the cache.", "notes_text": null, "keywords": ["AMAT", "average access time", "hit rate", "miss rate", "miss penalty"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["AMAT formula"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 54, "title": "Average Access Time: Example", "summary": "Numerical example calculating AMAT.", "main_text": "Average Access Time = (Hit Time) + (Miss Rate) × (Miss Penalty) = (5 ns) + [1 - (Hit Rate)] × (200 ns) = (5 ns) + (0.1) × (200 ns) = 25 ns. Note: The (Miss Penalty) includes the time to obtain the data from the next level of the cache hierarchy. That may also be a hit or a miss.", "notes_text": null, "keywords": ["AMAT calculation", "numerical example", "performance metrics"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["AMAT example"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 55, "title": "Average Access Time: 2-Level Cache", "summary": "Extends AMAT formula to multi-level cache hierarchy.", "main_text": "Average Access Time = (Hit Time of L1) + (Miss Rate of L1) × (Miss Penalty of L1). (Miss Penalty of L1) = average time to access L2 = (Hit Time of L2) + (Miss Rate of L2) × (Miss Penalty of L2). (Miss Penalty of L2) = average time to access Main Memory. Shows hierarchical structure: L1 (hit or miss), L2 (hit or miss), Main Memory (always a hit).", "notes_text": null, "keywords": ["multi-level cache", "L1 L2", "hierarchical AMAT"], "images": [{"type": "flowchart", "description": "Three-level hierarchy showing L1, L2, and Main Memory with hit/miss paths"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["multi-level AMAT"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 56, "title": "Average Access Time: Example", "summary": "Numerical example for 2-level cache AMAT calculation.", "main_text": "Consider a 2-level cache L1/L2/Memory where: (Hit Time of L1) = 5 ns, (Hit Time of L2) = 20 ns, (Hit Rate of L1) = 0.9, (Hit Rate of L2) = 0.8, (Time to Transfer a Cache Line from Memory) = 200 ns. Average Access Time = (Hit Time of L1) + (Miss Rate of L1) × (Miss Penalty of L1) = (5 ns) + (0.1) × [(20 ns) + (0.2) × (200 ns)] = 11 ns.", "notes_text": null, "keywords": ["2-level AMAT", "L1 L2 calculation", "performance example"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["2-level AMAT example"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 57, "title": "Cache Design Tradeoffs", "summary": "Three strategies to reduce average access time.", "main_text": "Average Access Time = (Hit Time) + (Miss Rate) × (Miss Penalty). Three ways to reduce access time: Reduce Hit Time - usually done by starting cache access before the end of virtual address translation (next unit). Reduce Miss Penalty - multi-level caches (previous example), smaller blocks. Reduce Miss Rate - crucial at lower levels of memory hierarchy (high miss penalty).", "notes_text": null, "keywords": ["design tradeoffs", "hit time", "miss penalty", "miss rate", "optimization"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "text-heavy"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache optimization strategies"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 58, "title": "Miss Rate", "summary": "Categorizes types of cache misses: compulsory, capacity, and conflict.", "main_text": "What causes cache misses? Compulsory Misses: First access to some data will always result in a miss. Capacity Misses: Due to cache being too small to hold our working set of data or to exploit spatial locality. Conflict (or Collision) Misses: Due to constraints of mapping scheme (direct or set associative) forcing us to evict data that we later need. Question: Can the cache accommodate the working set of frequently accessed data?", "notes_text": null, "keywords": ["miss types", "compulsory miss", "capacity miss", "conflict miss", "working set"], "images": [{"type": "diagram", "description": "Graph showing accessed memory addresses over time, plus cache showing Set 0 full causing conflict"}], "layout": {"num_text_boxes": 2, "num_images": 2, "dominant_visual_type": "mixed"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["miss classification"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 59, "title": "Block Size", "summary": "Analyzes impact of block size on cache performance.", "main_text": "A larger block size: Increases miss penalty (we have to load larger blocks into the cache), Reduces compulsory misses (we load more nearby data and take better advantage of spatial locality), Increases capacity/conflict misses (for a fixed cache size, we will have fewer blocks if block size is larger - a sparse working set may require many blocks but we can hold fewer, each set has fewer blocks which may result in more conflicts). Graph shows miss rate vs block size with increasing trends after optimal point.", "notes_text": null, "keywords": ["block size", "spatial locality", "miss penalty tradeoff"], "images": [{"type": "graph", "description": "Miss rate vs block size curve from Patterson & Hennessy showing U-shaped relationship"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["block size tradeoffs"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 60, "title": "Cache Size", "summary": "Examines effects of cache size on performance and cost.", "main_text": "A larger cache size: Reduces capacity/conflict misses (we can store more data, and more blocks in each set), Increases hit time (we need to search tags through more blocks), Increases cost and power, Has diminishing returns after we can hold most of the working set. Cache size of a level should be greater than the cache size of previous level. Shows hierarchy: L1 (128kB data & instr), L2 (4 kB to 4 MB). P(Miss in L2, given a miss in L1) is higher due to lower locality given miss in L1. P(Miss in L1 & L2) similar to P(Miss) in single-level with same size as L2 (but latency is lower, L1 is faster!).", "notes_text": null, "keywords": ["cache size", "capacity tradeoff", "diminishing returns", "multi-level sizing"], "images": [{"type": "diagram", "description": "L1 and L2 cache hierarchy"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache size tradeoffs"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 61, "title": "Associativity (# Ways)", "summary": "Analyzes impact of associativity on cache performance.", "main_text": "Higher associativity = more ways (blocks per set): Reduces conflict misses (for a fixed cache size, we will have more blocks in each set, so we will be able to: use invalid blocks that would otherwise be in other sets, evict blocks used less recently - we evict in the set), Increases hit time (we need to search tags through more blocks), Increases HW cost of policies, Diminishing returns. Shows conflict example in Set 0 with 2-way associative cache.", "notes_text": null, "keywords": ["associativity", "ways", "conflict reduction", "search cost"], "images": [{"type": "diagram", "description": "Set-associative cache showing Set 0 full, requiring eviction"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["associativity tradeoffs"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 62, "title": "Other Strategies: Prefetching", "summary": "Introduces prefetching as cache optimization technique.", "main_text": "Prefetching: On miss of block i, fetch block i and i+1. Can be performed in: Software, with prefetch instructions inserted by the compiler. Hardware, when detecting a specific pattern (common).", "notes_text": null, "keywords": ["prefetching", "hardware prefetch", "software prefetch", "optimization"], "images": [{"type": "diagram", "description": "Main Memory blocks showing prefetch pattern"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "mixed"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["prefetching"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 63, "title": "Example: Intel i7 Haswell", "summary": "Real-world example of modern cache architecture.", "main_text": "Quad-core CPU: L1/L2 caches for each core (Separate L1 caches for data and program instruction), L3 cache shared by all cores, Same block size (64 B) on all levels, Levels with increasing cache size: 32 kB (L1), 256 kB (L2), 8 MB (L3), Higher associativity at L3: since miss penalty is larger, we want fewer conflict misses.", "notes_text": null, "keywords": ["Intel i7", "Haswell", "multi-core", "L1 L2 L3", "real architecture"], "images": [{"type": "diagram", "description": "Block diagram showing 4 cores, each with L1/L2, shared L3, and main memory"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["Intel i7 architecture"]}}
{"deck_name": "CS356Unit10_Caches", "slide_number": 64, "title": "Experiment: Measuring Read Tput", "summary": "Introduces experimental measurement of cache read throughput with different stride patterns.", "main_text": "data is an array of long. Adding each item every stride items, over elems items in total. Access pattern examples: stride-1: d[0] d[1] d[2] d[3] d[4] ... d[15] using indices a0 a1 a2 a3 a0 a1 a2 a3 ... stride-2: every other element, stride-4: every fourth element. Demonstrates spatial locality effects on cache performance.", "notes_text": null, "keywords": ["cache measurement", "stride access", "read throughput", "spatial locality experiment"], "images": [{"type": "diagram", "description": "Array access patterns showing stride-1, stride-2, and stride-4 with corresponding index patterns"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "diagram"}, "section": "Access Time & Design Tradeoffs", "metadata": {"course": "CS356", "unit": 10, "topics": ["cache performance measurement"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 1, "title": "Unit 11: Virtual Memory", "summary": "Title slide introducing the virtual memory unit with the tagline 'To Cache and Protect'", "main_text": "Unit 11: Virtual Memory\nTo Cache and Protect", "notes_text": null, "keywords": ["virtual memory", "cache", "protection", "CS356"], "images": [{"type": "logo", "description": "CS356 course shield logo with Trojan mascot"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "title_slide"}, "section": "Introduction", "metadata": {"course": "CS356", "unit": 11, "topics": ["virtual memory overview"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 2, "title": "Virtual Memory Idea", "summary": "Section divider introducing the core concept of virtual memory", "main_text": "Virtual Memory Idea", "notes_text": null, "keywords": ["virtual memory", "concept", "introduction"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "Virtual Memory Idea", "metadata": {"course": "CS356", "unit": 11, "topics": ["virtual memory concept"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 3, "title": "Motivation", "summary": "Explains why virtual memory is needed: multiple processes share CPU and memory, creating two problems - protecting processes from each other and handling insufficient memory by using disk storage", "main_text": "Multiple processes take turns executing instructions on CPU cores and share the main memory. This creates two issues:\n1. What if some process tries to access memory used by another?\n   ⇒ Insulate different processes\n2. What if there is not enough memory for all processes?\n   ⇒ Cache data in memory and store the rest on disk", "notes_text": null, "keywords": ["processes", "CPU", "memory sharing", "protection", "disk storage", "cache", "process isolation"], "images": [{"type": "diagram", "description": "CPU with multiple cores showing Process 1, Process 2, Process 3 with their memory layouts (Code, Data, Heap, Stack) and main memory unable to fit all processes"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "concept_with_diagram"}, "section": "Virtual Memory Idea", "metadata": {"course": "CS356", "unit": 11, "topics": ["process isolation", "memory management", "disk swapping"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 4, "title": "Problems of Physical Addresses", "summary": "Identifies three critical problems when processes use physical addresses directly: cannot reference data moved to disk, can see other processes' data, and prevents OS from rearranging memory", "main_text": "Processes using physical addresses...\n• ...wouldn't be able to reference data moved to disk (only data in memory has physical addr.)\n• ...would see data of other processes in their memory address space\n• ...wouldn't allow the OS to rearrange data in memory\n\nExample: Process A has allocated two pages (4 kB blocks of memory): P1 is in memory but P2 is only on disk", "notes_text": null, "keywords": ["physical addresses", "disk", "process isolation", "memory management", "pages", "security"], "images": [{"type": "system_diagram", "description": "Computer architecture showing CPU with dual cores, L3 cache, main memory (RAM) with pages P0 and P1, and disk storage via I/O bus"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "problem_statement_with_diagram"}, "section": "Virtual Memory Idea", "metadata": {"course": "CS356", "unit": 11, "topics": ["physical addressing problems", "memory pages"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 5, "title": "Idea of Virtual Addresses", "summary": "Introduces the solution: give each process an independent virtual address space that the CPU and OS map to physical addresses", "main_text": "An independent virtual address space for each process ... CPU & OS map virtual to physical addresses", "notes_text": null, "keywords": ["virtual addresses", "virtual address space", "VAS", "address mapping", "MMU", "translation"], "images": [{"type": "diagram", "description": "Shows two processes (A and B) each with their own virtual address space (VP0-VP3) mapping to shared physical memory and disk, with CPU architecture"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "solution_with_mapping_diagram"}, "section": "Virtual Memory Idea", "metadata": {"course": "CS356", "unit": 11, "topics": ["virtual addressing", "address translation", "virtual address space"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 6, "title": "Advantages of Virtual Addresses", "summary": "Lists three key benefits: can reference data not in memory (via page faults), processes cannot see each other's data, and OS can freely rearrange physical memory", "main_text": "Processes using virtual addresses...\n• ...can reference data that is not in memory (e.g., VP1) because it has a virtual address\n  ○ The CPU generates a 'page fault' and the OS loads the data from disk\n• ...cannot see data of other processes, e.g., P0 is not accessible through any of the virtual addresses available to Process A\n• ...don't rely on the exact physical location in memory, so the OS can move data in memory and change the mapping (e.g., VP1 to P1)", "notes_text": null, "keywords": ["virtual addresses", "page fault", "process isolation", "memory flexibility", "OS control", "security"], "images": [{"type": "diagram", "description": "Virtual address space of Process A showing virtual pages mapping to physical memory and disk, with arrows indicating page fault handling"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "benefits_with_diagram"}, "section": "Virtual Memory Idea", "metadata": {"course": "CS356", "unit": 11, "topics": ["virtual addressing benefits", "page faults", "memory protection"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 7, "title": "Virtual Address Space", "summary": "Defines the virtual address space (VAS) as the complete set of addresses available to a process, shared by all threads but protected from other processes. Clarifies that Process = Threads + VAS, and Thread = registers + stack", "main_text": "Everything is easy from the point of view of a process... all memory addresses are available to the process!\n\nThis is the virtual address space (VAS), shared by all threads in the process but protected from other processes:\n\nProcess = Running instance of a program = Threads + VAS\nThread = register values + stack\n\nEach thread has its own stack, but all memory of the process is accessible! (Use locks to avoid data races...)", "notes_text": null, "keywords": ["virtual address space", "VAS", "process", "thread", "stack", "shared memory", "data races", "locks"], "images": [{"type": "memory_layout", "description": "Diagram showing VAS from 0x00000000 to 0xffffffff with sections: Code, Data, Heap, Stack1, Stack2, Mapped I/O"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "definition_with_memory_layout"}, "section": "Virtual Memory Idea", "metadata": {"course": "CS356", "unit": 11, "topics": ["virtual address space", "process model", "thread model", "memory layout"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 8, "title": "Mapping VAS to PAS", "summary": "Explains how virtual address spaces are divided into pages (usually 4 kB) that map to frames in physical memory. Pages can be unallocated, uncached (on disk), or cached (in memory). Introduces page faults as the mechanism for loading uncached pages", "main_text": "Virtual address spaces are broken into blocks called 'pages' (usually 4 kB)\nPhysical address space is broken into blocks called 'frames' (same size)\n\nVAS usually much larger: pages can be\n• Unallocated: Never accessed (yet), e.g., unused portions of stack/heap\n• Uncached: Allocated but on disk\n• Cached: Allocated and in memory\n\nOS keeps LRU pages in memory frames\n\nPage Fault: When a process accesses a page that is not in memory, the OS loads it into a frame", "notes_text": null, "keywords": ["pages", "frames", "page size", "4KB", "unallocated", "uncached", "cached", "LRU", "page fault", "swap file"], "images": [{"type": "complex_diagram", "description": "Shows three virtual address spaces (Process A, B, C) mapping to physical address space with numbered frames (0-6), plus swap file on disk. Color-coded to show cached vs uncached pages"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "technical_diagram"}, "section": "Virtual Memory Idea", "metadata": {"course": "CS356", "unit": 11, "topics": ["pages", "frames", "page tables", "memory mapping", "page replacement"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 9, "title": "Address Translation", "summary": "Section divider introducing the mechanism of translating virtual addresses to physical addresses", "main_text": "Address Translation", "notes_text": null, "keywords": ["address translation", "virtual to physical", "MMU"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "Address Translation", "metadata": {"course": "CS356", "unit": 11, "topics": ["address translation"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 10, "title": "CPU Translates using Page Tables", "summary": "Shows how the CPU uses page tables to translate virtual addresses to physical addresses. Each process has its own page table maintained by the OS and read by the MMU (Memory Management Unit)", "main_text": "CPU Memory Management Unit (MMU) reads page tables prepared by OS (in memory)\n\nExample instruction: movl 0x40008, %eax\nVirtual Address 0x40008 (within a page)\nPhysical Addr 0x6008 (within frame where page was saved)", "notes_text": null, "keywords": ["CPU", "MMU", "Memory Management Unit", "page table", "address translation", "PTE", "page table entry"], "images": [{"type": "complex_diagram", "description": "Shows three process virtual address spaces mapping through their respective page tables to physical memory frames and swap file. Page tables show mappings like 'Page 0 (rx) ⇒ Frame 6', with valid/invalid entries"}], "layout": {"num_text_boxes": 3, "num_images": 1, "dominant_visual_type": "system_architecture"}, "section": "Address Translation", "metadata": {"course": "CS356", "unit": 11, "topics": ["page tables", "MMU", "translation process", "PTEs"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 11, "title": "Address Translation", "summary": "Detailed breakdown of address translation process: Virtual address is split into VPN (Virtual Page Number) and page offset. VPN indexes the page table to get PFN (Physical Frame Number). Physical address = PFN | Page Offset", "main_text": "1. Virtual address 0x000022d8 (VA) is split into virtual page number 0x00002 (VPN) and page offset 0x2d8 (PO, 'offset within page')\n\n2. VPN is used as an index into the page table to read the physical frame number 0x0021b (PFN)\n\n3. Physical address = PFN | PO = 0x0021b2d8\n\nFor 4 kB pages:\npage offset bits = log2(4k) = 12\nVPN bits = VA bits - 12 = 20\n⇒ 2^20 entries ... 4 MB if 4 bytes/entry\n\nPTBR = Page Table Base Register (CR3 on Intel)", "notes_text": null, "keywords": ["VPN", "virtual page number", "PFN", "physical frame number", "page offset", "PTBR", "CR3", "page table entries", "address translation"], "images": [{"type": "technical_diagram", "description": "Bit-level breakdown showing 32-bit virtual address split into 20-bit VPN and 12-bit offset, page table lookup, and resulting 32-bit physical address. Shows PTE structure with rwx permission bits"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "technical_detail"}, "section": "Address Translation", "metadata": {"course": "CS356", "unit": 11, "topics": ["VPN", "PFN", "page offset", "address bits", "PTBR"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 12, "title": "Example", "summary": "Works through a concrete example with 8-bit VAs, 10-bit PAs, and 32-byte pages. Shows the translation process step-by-step", "main_text": "System with 8-bit VAs, 10-bit PAs, and 32-byte pages\n\nVA split into VPN (bits 7-5) and Offset (bits 4-0)\nPage Table lookup uses VPN as index\nPA formed from PFN (from page table) and Offset", "notes_text": null, "keywords": ["example", "address translation", "VPN", "PFN", "page table lookup"], "images": [{"type": "worked_example", "description": "Detailed diagram showing VA 0x2D being translated through page table to PA, with VPN extraction, page table with 8 entries showing valid bits and PFNs, physical memory layout, and virtual address space layout"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "worked_example"}, "section": "Address Translation", "metadata": {"course": "CS356", "unit": 11, "topics": ["address translation example", "page table lookup"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 13, "title": "Exercise", "summary": "Practice exercise for students to work through address translation problems with the same system parameters", "main_text": "• Consider the same system with 8-bit VAs, 10-bit PAs, and 32-byte pages.\n• Fill in the table below showing the corresponding physical or virtual address based on the other. If no translation can be made, indicate 'INVALID'\n\nGiven page table with entries:\nVPN 0: V=0, Entry=0x0E\nVPN 1: V=1, Entry=0x1E\nVPN 2: V=1, Entry=0x16\nVPN 3: V=1, Entry=0x06\nVPN 4: V=0, Entry=0x0B\nVPN 5: V=1, Entry=0x1F\nVPN 6: V=0, Entry=0x15\nVPN 7: V=0, Entry=0x0A\n\nAnswers:\nVA 0x2D = 0010 1101 → PA 0x3CD\nVA 0x7A = 0111 1010 → PA 0x0DA\nVA 0xEF = 1110 1111 → INVALID\nPA 0x3E8 → VA 0xA8", "notes_text": null, "keywords": ["exercise", "practice", "address translation", "valid bit", "invalid pages"], "images": [{"type": "table", "description": "Page table with V bit and Entry columns, plus exercise table with VA and PA columns to fill in"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "exercise"}, "section": "Address Translation", "metadata": {"course": "CS356", "unit": 11, "topics": ["address translation practice"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 14, "title": "Context Switch & PT", "summary": "Explains that each process needs its own page table, and on context switch the OS updates the PTBR to point to the new process's page table. Example shows how Linux stores PTBR in task_struct", "main_text": "Each process has its own VAS ⇒ needs its own PT\n\n• On context switch to new process, the OS modifies the PTBR to point to a new PT\n• E.g., Linux saves the PTBR in the 'task_struct' of the process", "notes_text": null, "keywords": ["context switch", "page table", "PTBR", "task_struct", "process switching", "multiple page tables"], "images": [{"type": "system_diagram", "description": "Complex diagram showing CPU with MMU performing address translation, two process page tables (PT1 and PT2) in OS kernel memory, physical memory with code/data/stack pages, and task structs containing PTBR values"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "system_architecture"}, "section": "Address Translation", "metadata": {"course": "CS356", "unit": 11, "topics": ["context switching", "page table management", "PTBR"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 15, "title": "Additional data in PTEs", "summary": "Describes the additional bits stored in page table entries beyond just the PFN: Valid/Present bit, Protection bits (rwx), Modified/Dirty bit, Referenced bit for LRU, and Cacheable bit", "main_text": "PTEs usually require 4 or 8 bytes each\n\n• Valid bit: Whether the page is in memory. If 0, the rest of the PTE is used by OS to store the location of the page on disk\n\n• Modified/Dirty: Whether it was modified. To implement a write back policy\n\n• Referenced Bit: used for pseudo-LRU, e.g.\n  ○ If a page is referenced, set referenced bit to 1\n  ○ Keep a counter in an OS data structure for each page ... periodically:\n    - If bit is 0 (not accessed), add 1 to counter\n    - If bit is 1 (accessed) set counter to 0\n    - Do this for all pages, then set bits to 0\n  ○ Evict page with max counter\n\n• Protection: Read/Write/eXecute", "notes_text": null, "keywords": ["PTE", "page table entry", "valid bit", "dirty bit", "modified bit", "referenced bit", "LRU", "protection bits", "rwx", "write back"], "images": [{"type": "diagram", "description": "PTE structure showing Page Frame Number field and control bits: Valid/Present, Modified/Dirty, Referenced, Cacheable, Protection Bits (rwx)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "data_structure"}, "section": "Address Translation", "metadata": {"course": "CS356", "unit": 11, "topics": ["PTE structure", "page replacement", "memory protection", "LRU algorithm"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 16, "title": "Saving Space with Multilevel Page Tables", "summary": "Section divider introducing multilevel page tables as a space-saving technique", "main_text": "Saving Space with Multilevel Page Tables", "notes_text": null, "keywords": ["multilevel page tables", "space optimization", "hierarchical page tables"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "Multilevel Page Tables", "metadata": {"course": "CS356", "unit": 11, "topics": ["multilevel page tables"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 17, "title": "Single-Level Page Tables", "summary": "Identifies the problem with single-level page tables: one row for each possible virtual page means mostly invalid entries and wasted memory, especially with one table per process. Uses phone book analogy", "main_text": "One row (PTE) for each virtual page\n\n• Most entries are usually invalid, since a process uses just small fraction of their VA spaces\n• Lots of wasted memory, especially since the OS must allocate a page table for each process\n\nAnalogy of phone number to user id translation: we are allocating an entry for each possible phone number!", "notes_text": null, "keywords": ["single-level page tables", "memory waste", "sparse address space", "page table size problem"], "images": [{"type": "diagram", "description": "Simple page table showing all possible VPNs with mostly invalid entries (v=0), plus phone book analogy showing entries for all possible phone numbers"}], "layout": {"num_text_boxes": 2, "num_images": 2, "dominant_visual_type": "problem_statement"}, "section": "Multilevel Page Tables", "metadata": {"course": "CS356", "unit": 11, "topics": ["single-level page tables", "memory overhead"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 18, "title": "2-Level Page Tables: Analogy", "summary": "Introduces two-level page tables using phone number analogy: 1st level indexes by area code, 2nd level by local number. Only allocate 2nd level tables for area codes actually used. Dramatically reduces space: 10^3 + 2×10^7 entries instead of 10^10", "main_text": "1st Level Table (Page Directory)\n• One row for each area code\n• Each row can either be NULL or point to 2nd level table\n\n2nd Level Tables\n• Allocated if at least one contact has a specific area code\n• Local phone number used as index\n• Rows have the actual contact ids\n\nAdvantage: If only 2 area codes are used, then only 10^3 + 2×10^7 entries are allocated, instead of 10^10", "notes_text": null, "keywords": ["two-level page tables", "page directory", "hierarchical structure", "space savings", "sparse allocation"], "images": [{"type": "diagram", "description": "Phone book analogy showing 1st level indexed by area code (000, 213, 323, etc.) with pointers to 2nd level tables indexed by local phone number, with NULL entries for unused area codes"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "analogy_diagram"}, "section": "Multilevel Page Tables", "metadata": {"course": "CS356", "unit": 11, "topics": ["two-level page tables", "page directory", "memory efficiency"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 19, "title": "3-Level Page Tables: Analogy", "summary": "Extends to three levels: area code, first 3 digits of local number, last 4 digits. For sparse usage (only 213-740-xxxx and 323-821-xxxx), further reduces space to 10^3 + 2×10^3 + 2×10^4 entries", "main_text": "We split phone numbers into 3 fields\n\nAdvantage: If numbers are only in 213-740-xxxx and 323-821-xxxx, we allocate 10^3 + 2×10^3 + 2×10^4 entries", "notes_text": null, "keywords": ["three-level page tables", "hierarchical indexing", "extreme space savings", "sparse address spaces"], "images": [{"type": "diagram", "description": "Three-level hierarchy: 1st level by area code, 2nd level by first 3 digits, 3rd level by last 4 digits. Shows only allocated paths for 213-740 and 323-821"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "hierarchical_diagram"}, "section": "Multilevel Page Tables", "metadata": {"course": "CS356", "unit": 11, "topics": ["three-level page tables", "hierarchical page tables"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 20, "title": "3-Level Page Table", "summary": "Shows actual implementation: VPN is split into 3 indices (Idx1, Idx2, Idx3). PTEs of intermediate levels contain addresses of next-level tables. Only the final level PTEs contain actual PFNs", "main_text": "• PTEs of intermediate levels have address of next table\n• PTEs of last level have PFNs\n\nExample with VPN split into:\n- Idx1: 6 bits (bits 31-26) → Page Directory\n- Idx2: 7 bits (bits 25-19) → Level 2 table\n- Idx3: 7 bits (bits 18-12) → Level 3 table\n- Offset: 12 bits (bits 11-0)\n\nPTBR/CR3 points to Page Directory start address", "notes_text": null, "keywords": ["three-level page tables", "page directory", "intermediate tables", "hierarchical lookup", "PFN", "PTBR"], "images": [{"type": "technical_diagram", "description": "Shows 3-level page table walk: PTBR points to Page Directory, which has entry PD[0x3f] pointing to PT2, which has entry PT2[0x40] pointing to PT3, which has entry PT3[0x35] containing the final PFN. Only allocated subtrees shown"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "technical_architecture"}, "section": "Multilevel Page Tables", "metadata": {"course": "CS356", "unit": 11, "topics": ["page table walk", "hierarchical translation", "page directory"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 21, "title": "Example: Intel i7", "summary": "Real-world example: Intel i7 uses 4 levels of page tables for 48-bit virtual addresses (out of 64-bit architecture), with 9 bits per level and 12-bit page offset for 4KB pages", "main_text": "• 64-bit architecture but addresses use only 48 bits\n• 4 levels of page tables\n\nVirtual Address (48 bits):\n- Index 1: 9 bits → First Level\n- Index 2: 9 bits → Second Level  \n- Index 3: 9 bits → Third Level\n- Index 4: 9 bits → Fourth Level\n- Offset: 12 bits → Page (4 kB)\n\nEach level has 2^9 = 512 PTEs", "notes_text": null, "keywords": ["Intel i7", "4-level page tables", "48-bit addressing", "x86-64", "CR3", "page walk"], "images": [{"type": "technical_diagram", "description": "Intel i7 page table structure showing CR3 register pointing to 4-level hierarchy, each level with 512 entries, culminating in 4KB page with referenced byte"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "real_world_example"}, "section": "Multilevel Page Tables", "metadata": {"course": "CS356", "unit": 11, "topics": ["Intel architecture", "x86-64 paging", "4-level page tables"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 22, "title": "Saving Time with TLBs", "summary": "Section divider introducing Translation Lookaside Buffers (TLBs) as a solution to speed up address translation", "main_text": "Saving Time with TLBs", "notes_text": null, "keywords": ["TLB", "Translation Lookaside Buffer", "performance", "caching"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "TLBs", "metadata": {"course": "CS356", "unit": 11, "topics": ["TLB"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 23, "title": "Cost of a Memory Access", "summary": "Identifies the performance problem: with k-level page tables, every memory access requires k+1 memory accesses (k for page table walk, 1 for actual data). Need caching to speed this up", "main_text": "With k-level page tables, the MMU has to access k PTEs, one in each page table level...\n\n• To perform 1 memory access, we need k + 1 accesses (page tables + actual data with physical address)\n• Some of the PTEs (and the data itself) may be in the cache\n\nHow can we speed up the translation process?", "notes_text": null, "keywords": ["performance", "memory access cost", "page table walk", "latency", "multiple accesses"], "images": [{"type": "diagram", "description": "Flow diagram showing Processor → MMU → multiple Page Table accesses → Cache/memory → Data"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "problem_statement"}, "section": "TLBs", "metadata": {"course": "CS356", "unit": 11, "topics": ["address translation performance", "page table access overhead"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 24, "title": "Translation Lookaside Buffer", "summary": "Introduces TLB as a cache of VPN→PFN translations. On hit, get PFN immediately. On miss, perform full page table walk, then add entry to TLB using LRU eviction if full", "main_text": "TLB Solution: A cache of VPN ⇒ PFN translations! We use the VPN to search...\n\n• On hit, we have the PFN of the physical address\n\n• On miss:\n  ○ We access all page table levels in memory (some may be hits in cache)\n  ○ Once we have the PFN, we add a PTE entry 'VPN ⇒ PFN' to the TLB\n  ○ If the TLB is full, we evict the LRU entry (to make translation of addresses within frequently accessed pages faster)", "notes_text": null, "keywords": ["TLB", "Translation Lookaside Buffer", "VPN to PFN", "cache", "LRU eviction", "translation cache"], "images": [{"type": "system_diagram", "description": "Two diagrams showing TLB hit path (Processor → Translation → PA → Cache/memory → Data) and TLB miss path (with additional Page Table access then adding entry to TLB)"}], "layout": {"num_text_boxes": 2, "num_images": 2, "dominant_visual_type": "solution_diagram"}, "section": "TLBs", "metadata": {"course": "CS356", "unit": 11, "topics": ["TLB", "address translation caching", "TLB hit", "TLB miss"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 25, "title": "TLB Organization", "summary": "TLBs are organized like caches (fully associative, set-associative, or direct mapped) but with no block offset since they cache individual PTEs. VPN used for tag and set index", "main_text": "Similar organization to caches\n\n• Fully associative, set-associative, direct mapped\n  ○ VPN used for tag and set index\n\n• But no block offset: we read an entire PTE (a line of the TLB)\n  ○ The PTE gives us the PFN and permission bits\n\nExample: Virtual Address 7ffe1 6d8\n- VPN = 0x7ffe1 → used for TLB lookup\n- Page Offset = 0x6d8 → copied to physical address\n\nOn TLB hit: PTE contains PFN = 0x308ac\nPhysical Address = 308ac 6d8", "notes_text": null, "keywords": ["TLB organization", "fully associative", "set associative", "VPN", "tag", "PTE", "no block offset"], "images": [{"type": "technical_diagram", "description": "Shows virtual address split into VPN and offset, fully associative TLB with tag comparisons, and resulting physical address formation"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "technical_detail"}, "section": "TLBs", "metadata": {"course": "CS356", "unit": 11, "topics": ["TLB structure", "associativity", "TLB lookup"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 26, "title": "Set Associative TLB", "summary": "Example of 4-way set-associative TLB with 64 entries (16 sets, 4 ways). VPN split into tag and set index. Shows concrete example with VPN=0x7ffe1, set=1, tag=0x7ffe matching in way 2", "main_text": "Example:\nN = 64 entries\nK = 4 ways\nS = N/4 = 16 sets\n\nVPN split into:\n- Tag (upper bits): 0x7ffe\n- Set index (lower bits): 1\n\nLookup searches all 4 ways in set 1 for matching tag\nHit in Way 2 returns PFN 0x308ac", "notes_text": null, "keywords": ["set associative TLB", "4-way", "sets", "ways", "tag comparison", "parallel lookup"], "images": [{"type": "technical_diagram", "description": "Detailed 4-way set-associative TLB diagram showing 16 sets, with VPN breakdown into tag and set index, parallel tag comparisons in all 4 ways, with hit indicated in way 2"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "detailed_example"}, "section": "TLBs", "metadata": {"course": "CS356", "unit": 11, "topics": ["set-associative TLB", "TLB lookup process"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 27, "title": "TLB Exercise", "summary": "Practice exercise: given 2-way set-associative TLB with 4 sets, 256-byte pages, 16-bit addresses, translate virtual address 0x7E85 to physical address, and reverse translation for 0x3020", "main_text": "• 2-way set associative TLB (K = 2)\n• S = 4 sets\n• Page size is 256 bytes\n• 16-bit VAs and PAs\n\nWhat is the physical address of virtual address 0x7E85?\nAnswer: 0x9585\n\nWhat is the virtual address of physical address 0x3020?\nAnswer: 0xA920", "notes_text": null, "keywords": ["TLB exercise", "address translation", "set associative", "practice problem"], "images": [{"type": "table", "description": "TLB table showing Index (0-3), V bit, Tag, and PPFN for 2-way set-associative TLB with 8 total entries"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "exercise"}, "section": "TLBs", "metadata": {"course": "CS356", "unit": 11, "topics": ["TLB practice", "bidirectional translation"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 28, "title": "Context Switch & TLB", "summary": "Each process needs different TLB entries since they have different page tables. Two solutions: (1) Flush TLB on context switch, or (2) Tag TLB entries with ASID (Address Space ID / Process ID) so multiple processes can coexist in TLB", "main_text": "Each process has a different VAS and a different page table (mapping its VAS to portions of the PAS)...\n\n• Since the TLB caches entries of PT, each process must use different TLB entries!\n\n• Two alternative solutions:\n  ○ Flush/empty the TLB at each context switch (new entries will be added after misses)\n  ○ Store the process ID in the TLB: for a hit, we need both the tag and process ID to match (processes compete for TLB entries)\n\nASID = Address Space ID (aka PID = Process ID) is UNIQUE per process.", "notes_text": null, "keywords": ["context switch", "TLB flush", "ASID", "process ID", "TLB coherence", "process isolation"], "images": [{"type": "diagram", "description": "TLB structure with ASID field showing how multiple processes can have entries in TLB simultaneously, with ASID used in tag comparison"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "technical_solution"}, "section": "TLBs", "metadata": {"course": "CS356", "unit": 11, "topics": ["TLB and context switching", "ASID", "TLB tagging"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 29, "title": "Advanced Uses of VM Mappings", "summary": "Section divider introducing advanced virtual memory techniques", "main_text": "Advanced Uses of VM Mappings", "notes_text": null, "keywords": ["virtual memory", "advanced techniques", "memory mapping"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "Advanced VM Uses", "metadata": {"course": "CS356", "unit": 11, "topics": ["advanced virtual memory"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 30, "title": "Sharing Memory Between Processes", "summary": "OS can map virtual pages of different processes to the same physical frame to share memory. Used for shared libraries, cooperating processes, and fork() with copy-on-write", "main_text": "Using page tables, OS can map virtual pages of different processes to the same physical frame!\n\nUsed to share memory among processes (threads always share the entire VAS):\n• Load shared library code only once\n• Cooperating processes, e.g., Python's multiprocessing.shared_memory\n• fork(): duplicate PT & copy-on-write", "notes_text": null, "keywords": ["shared memory", "memory sharing", "shared libraries", "fork", "copy-on-write", "IPC", "interprocess communication"], "images": [{"type": "diagram", "description": "Shows Process A and Process B virtual address spaces with different virtual pages mapping to the same physical frame 6, demonstrating code sharing (both have 1-Code mapped to frame 6)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "application_diagram"}, "section": "Advanced VM Uses", "metadata": {"course": "CS356", "unit": 11, "topics": ["memory sharing", "shared libraries", "copy-on-write"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 31, "title": "Memory Mapped Files", "summary": "OS can map virtual pages directly to file blocks. Pages loaded on demand from file instead of swap space. Enables file caching in memory, private copy-on-write mappings, and zero-demand allocation", "main_text": "Using page tables, OS can map virtual pages to contiguous 4 kB blocks of a file!\n\n• As soon as the process tries to access a page mapped to FileX, OS loads the page into physical memory\n• File pages cached in memory/caches\n• Private mapping for copy-on-write\n• Zero-demand mapping: allocate pages of 0's on demand", "notes_text": null, "keywords": ["memory mapped files", "mmap", "file caching", "copy-on-write", "zero-demand", "page cache"], "images": [{"type": "diagram", "description": "Shows process virtual address space with pages 3 and 4 mapped to File X on disk, with pages cached in physical memory frames 0 and 3. File X also shown in swap file area"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "application_diagram"}, "section": "Advanced VM Uses", "metadata": {"course": "CS356", "unit": 11, "topics": ["memory mapped files", "file I/O", "demand paging"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 32, "title": "Example: ./a.out", "summary": "When starting a program, .text and .data are mapped as private (copy-on-write) from executable file, shared library code/data mapped as shared, and heap/stack are demand-zero regions", "main_text": "When we start a program with ./a.out\n\n• .text (binary code) and .data (global vars) parts of executable file mapped as 'private'\n  ○ If the process makes changes, a copy of the affected pages is made (and used in the VAS of the process)\n  ○ Otherwise, all processes started by ./a.out share the same pages\n\n• Heap and stack are demand-zero\n\n• .text and .data of library files (e.g., libc.so) are 'shared' (changes visible to other proc.)", "notes_text": null, "keywords": ["program loading", "executable", ".text", ".data", "shared libraries", "demand-zero", "copy-on-write", "libc"], "images": [{"type": "memory_layout", "description": "Process memory layout showing regions from low to high addresses: Code (.text), Initialized data (.data), Uninitialized data (.bss), Runtime heap, Memory-mapped shared libraries, User stack, with annotations for private/shared and demand-zero/file-backed"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "real_world_application"}, "section": "Advanced VM Uses", "metadata": {"course": "CS356", "unit": 11, "topics": ["program loading", "executable format", "memory layout"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 33, "title": "Virtual Memory & CPU Caches", "summary": "Section divider introducing the interaction between virtual memory and CPU caches", "main_text": "Virtual Memory & CPU Caches", "notes_text": null, "keywords": ["virtual memory", "CPU caches", "cache coherence", "address translation"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["virtual memory and caching"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 34, "title": "Cache are Physically Addressed", "summary": "CPU caches typically sit between MMU and memory, using physical addresses. Must translate VA to PA before cache lookup. Page offset unchanged allows VIPT optimization", "main_text": "CPU caches (L1, L2, L3) usually sit between MMU and memory\n\nThe CPU needs to translate the VA to a PA (and check permission bits) before searching into the cache...\n\nProcess:\n1. Virtual Address with VPN and Page Offset\n2. TLB lookup or page table walk to get PFN\n3. Form Physical Address = PFN + Page Offset\n4. Search L1/L2/L3 caches with Physical Address\n5. On cache miss, access main memory", "notes_text": null, "keywords": ["physically addressed cache", "cache lookup", "MMU", "physical address", "cache access"], "images": [{"type": "flow_diagram", "description": "Shows address translation flow: VA → MMU (with TLB and page table) → PA → Caches → Memory. Includes bit-level breakdown of virtual and physical addresses"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "system_flow"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["cache addressing", "physical caches", "memory hierarchy"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 35, "title": "Optimization: VIPT Caches", "summary": "Virtually Indexed Physically Tagged caches allow parallel cache set selection and address translation. Page offset determines cache set index, but cache tag still uses physical address. Works when page offset bits ≥ cache set index + block offset bits", "main_text": "Virtually Indexed: Since the page offset doesn't change, we know the cache set idx already from the VA when:\n(Page Offset bits) ≥ (Cache Set Idx bits) + (Block Offset Bits)\n\nPhysically Tagged: The cache tag to search still depends on the PFN, so we have to wait for MMU translation to search the cache...\n\n⇒ In parallel (thus faster):\n• Activation of cache set\n• MMU translation", "notes_text": null, "keywords": ["VIPT", "virtually indexed physically tagged", "parallel lookup", "cache optimization", "performance"], "images": [{"type": "technical_diagram", "description": "Shows how page offset bits from VA can be used for cache set selection while MMU translates VPN to PFN in parallel. Highlights the overlap region allowing simultaneous operations"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "optimization_technique"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["VIPT caches", "cache performance", "parallel operations"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 36, "title": "VIPT: Limitations", "summary": "VIPT requires page size ≥ (# cache sets) × (block size). For direct-mapped cache, size ≤ page size. For k-way set-associative, size ≤ k × page size. Limits cache size (typically 4KB to 32KB for L1)", "main_text": "(Page Offset bits) ≥ (Cache Set Idx bits) + (Block Offset Bits)\nimplies\n2^(Page Offset bits) ≥ 2^(Cache Set Idx bits + Block Offset Bits)\ni.e.,\n(Page Size) ≥ (# Cache Sets) * (Block Size)\n\n⇒ The size of a direct-mapped VIPT (1 block per set) cache cannot be greater than the size of a page (usually 4 kB)\n\n⇒ The size of a 2-way set-associative VIPT (2 blocks per set) cache cannot be greater than twice size of a page, and so on...\n\n(But having more than ≈ 8 ways is usually not desirable)", "notes_text": null, "keywords": ["VIPT limitations", "cache size", "page size", "set associativity", "design constraints"], "images": [{"type": "diagram", "description": "Bit-level diagram showing physical address with PFN and page offset, and how cache set index must fit within page offset for VIPT to work"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "constraint_analysis"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["VIPT constraints", "cache sizing"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 37, "title": "VIPT: Aliasing Problem", "summary": "VIPT has aliasing problem when page offset bits < cache set index + block offset bits. Different virtual addresses mapping to same physical address can select different cache sets, creating multiple inconsistent copies", "main_text": "VIPT has an aliasing problem when\n(Page Offset bits) < (Cache Set Idx bits) + (Block Offset Bits)\n\n• Different virtual addresses, e.g., 7ffe16d8 and 8dda26d8, can map to the same physical addr. 308ac6d8 (e.g., when processes share memory)\n\n• But using the VA for the cache index (16 or 26 instead of c6) selects different cache sets\n  ○ Multiple copies (alias) of data from same physical address\n  ○ One copy could be dirty & the other out-of-date!", "notes_text": null, "keywords": ["aliasing problem", "VIPT", "cache coherence", "shared memory", "multiple copies", "inconsistency"], "images": [{"type": "technical_diagram", "description": "Shows two different virtual addresses with different VPN low bits (16 vs 26) but mapping to same PFN (308ac), demonstrating how they would select different cache sets (set 16 vs set 26)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "problem_illustration"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["cache aliasing", "VIPT problems", "cache coherence issues"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 38, "title": "VIPT: Page Coloring", "summary": "Solution to aliasing: OS ensures that when different virtual pages map to the same physical frame, the VPN bits used by cache set index are identical. Called page coloring", "main_text": "The aliasing problem can be resolved by making sure that:\n\n• When different virtual pages are mapped to the same memory frame...\n• ...the VPN bits used by the cache set index are the same\n\nPrevious Example: VIPT uses cache set indices 16 for 7ffe16d8 and 26 for 8dda26d8...\n\nThe OS can avoid this by assigning a different VA to the region in the VAS of the second process, e.g., 8dda16d8 (now the same set idx 16 is used)", "notes_text": null, "keywords": ["page coloring", "aliasing solution", "cache set selection", "OS memory management", "VIPT fix"], "images": [{"type": "diagram", "description": "Shows how OS assigns virtual addresses such that the low-order VPN bits match when pages map to same physical frame, ensuring consistent cache set selection"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "solution_technique"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["page coloring", "cache management", "aliasing prevention"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 39, "title": "Core i7 Address Translation", "summary": "Complete diagram showing Intel Core i7 address translation flow from 48-bit VA through 4-level page tables and L1 TLB (16 sets, 4 entries/set) to physical address, then through L1 cache (64 sets, 8 lines/set) to L2/L3/memory", "main_text": "Complete Intel Core i7 memory access flow:\n1. 48-bit Virtual Address split into VPN1-VPN4 (9 bits each) and VPO (12 bits)\n2. L1 TLB lookup (16 sets, 4-way) using TLBT and TLBI from VPN\n3. On TLB hit: get PPN directly\n4. On TLB miss: walk 4-level page tables (CR3 → PTE → PTE → PTE → PTE → PPN)\n5. Form physical address from PPN + PPO\n6. L1 cache lookup (64 sets, 8-way) using CT and CI from PA\n7. On L1 miss: access L2, L3, and main memory", "notes_text": null, "keywords": ["Intel i7", "Core i7", "complete translation", "TLB", "cache hierarchy", "page table walk", "x86-64"], "images": [{"type": "comprehensive_diagram", "description": "Complex system diagram showing complete address translation and memory hierarchy: VA breakdown, TLB structure, page table hierarchy, PA formation, L1 cache structure, and connection to L2/L3/memory"}], "layout": {"num_text_boxes": 1, "num_images": 1, "dominant_visual_type": "comprehensive_system_diagram"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["complete translation flow", "Intel architecture", "memory hierarchy"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 40, "title": "Addresses of Page Tables", "summary": "Page tables themselves are accessed using physical addresses (PTBR and intermediate PTE pointers) to avoid chicken-and-egg problem. PTEs can be cached like any other data, causing cache hits/misses during page table walks", "main_text": "Physical addresses are used in\n• PTBR register (address of page directory)\n• PTEs of multilevel page tables (addresses of next-level PT)\n\nsince page tables are necessary for VA translation (otherwise, a 'Catch-22').\n\nPTEs are cached like any other data from memory... we can have cache hits/misses during a page table walk.", "notes_text": null, "keywords": ["page table addresses", "physical addressing", "PTBR", "PTE caching", "page table walk", "cache hits"], "images": [{"type": "technical_diagram", "description": "Shows multilevel page table walk with PTBR containing physical address, intermediate PTEs containing physical addresses of next level, with annotations showing where cache hits/misses can occur"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "technical_detail"}, "section": "VM and Caches", "metadata": {"course": "CS356", "unit": 11, "topics": ["page table storage", "physical addressing", "PTE caching"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 41, "title": "Page Faults", "summary": "Section divider introducing page fault handling", "main_text": "Page Faults", "notes_text": null, "keywords": ["page fault", "exception handling", "disk I/O"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "Page Faults", "metadata": {"course": "CS356", "unit": 11, "topics": ["page faults"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 42, "title": "Page Fault Scenarios", "summary": "CPU raises page fault when: (1) PTE valid bit = 0 (page not allocated or not in memory), or (2) incompatible permission bits. OS handler kills process if invalid access, otherwise allocates new page or loads from disk", "main_text": "When the MMU finds...\n• a PTE with valid bit = 0 (page not allocated or currently not in memory)\n• a PTE with valid bit = 1 but incompatible permission bits (e.g., PTE has 'rx' but instruction writes)\n\n...the CPU (HW) raises a 'page fault.' The OS page fault handler (SW) is invoked to:\n1. Kill the process if the page was not mapped in the VAS (e.g., with mmap)\n2. Kill the process if op was not allowed\n3. Allocate a new page on demand, or load the existing page from disk", "notes_text": null, "keywords": ["page fault", "invalid page", "protection exception", "segmentation fault", "permission bits", "OS handler"], "images": [{"type": "diagram", "description": "Process virtual memory diagram showing three scenarios: (1) Segmentation fault - accessing nonexistent page, (2) Protection exception - writing to read-only page, (3) Normal page fault - accessing valid but uncached page"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "scenario_analysis"}, "section": "Page Faults", "metadata": {"course": "CS356", "unit": 11, "topics": ["page fault types", "exception handling", "memory protection"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 43, "title": "Loading a Page from Disk", "summary": "Page fault handling process: (1-2) TLB miss and invalid PTE trigger page fault, (3) If memory full, evict LRU page with writeback, invalidate its TLB entry, (4) Load needed page from disk, update PTE, (5) Restart instruction, (6) Load new PTE into TLB", "main_text": "• Page fault: TLB miss (1) & invalid PTE in PT (2)\n• If there is no empty frame: select a victim (LRU), write back (if dirty or never swapped), set its PTE to invalid, invalidate its TLB entry if present (3)\n• Load the desired page into the memory frame (overwriting cached data of old page), set its PTE with valid=1 and VPN of memory frame (4)\n• Restart the instruction (5), which will load PTE into TLB (6)", "notes_text": null, "keywords": ["page fault handling", "page replacement", "LRU", "writeback", "disk I/O", "TLB invalidation"], "images": [{"type": "flow_diagram", "description": "Detailed system diagram showing: CPU → MMU → TLB miss (1) → Page Table with invalid PTE (2) → OS Page Fault Handler → Evict victim page (3) → Disk Driver loading new page (4) → Memory updated → Restart instruction (5) → TLB updated (6)"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "process_flow"}, "section": "Page Faults", "metadata": {"course": "CS356", "unit": 11, "topics": ["page fault handling", "page replacement", "disk I/O"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 44, "title": "Page Eviction Bookkeeping", "summary": "Critical coherence requirement: when evicting a page, must invalidate its TLB entry and remove/overwrite its cached data. OS provides software coherence by invalidating TLB entries after modifying PTEs (invlpg instruction)", "main_text": "Copies (TLB entry & cached data) must be removed when the actual data change (PTE & page frame data)!\n\n• If we didn't invalidate the TLB entry of the evicted page, the MMU would find it at the old page frame (where the new page was saved)\n\n• If we didn't remove the data of the evicted page from the cache (or overwrite it when loading the new page), we would retrieve it when accessing the new page\n\n'Software coherence of TLB':\n• After modifying a PTE, the OS invalidates its TLB entry: invlpg 0x123\n• If TLB entries are not tagged with process ID, the OS invalidates all the TLB entries on context switch\n• TLB is usually write-through: changes to the modified bit are propagated to page table", "notes_text": null, "keywords": ["cache coherence", "TLB coherence", "TLB invalidation", "invlpg", "software coherence", "bookkeeping"], "images": [], "layout": {"num_text_boxes": 2, "num_images": 0, "dominant_visual_type": "technical_explanation"}, "section": "Page Faults", "metadata": {"course": "CS356", "unit": 11, "topics": ["TLB coherence", "cache coherence", "OS responsibilities"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 45, "title": "Page Hits in TLB, PT, CPU Cache", "summary": "Analysis of possible hit/miss combinations: TLB hit + PT invalid = impossible (incoherent), TLB miss + PT invalid + cache hit = impossible (data not in memory), other combinations explained", "main_text": "During a memory access (to a specific page) can we observe the following combinations of hit/miss?\n\nTLB Hit + PT Hit (valid) + Cache Hit/Miss → YES (page in memory, PTE in TLB, data may/may not be in cache)\n\nTLB Hit + PT Miss (invalid) + Cache Hit/Miss → NO (TLB cannot have valid entry if PT doesn't)\n\nTLB Miss + PT Hit (valid) + Cache Hit/Miss → YES (page in memory, PTE will be loaded into TLB)\n\nTLB Miss + PT Miss (invalid) + Cache Hit → NO (cannot have cache hit if data not in memory)\n\nTLB Miss + PT Miss (invalid) + Cache Miss → YES (page fault will load page into memory)", "notes_text": null, "keywords": ["TLB hit", "cache hit", "page table", "consistency", "coherence", "hit combinations"], "images": [{"type": "table", "description": "Table analyzing all combinations of TLB hit/miss, Page Table hit/miss (valid/invalid), and CPU Cache hit/miss, with explanations of which are possible and why"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "analysis_table"}, "section": "Page Faults", "metadata": {"course": "CS356", "unit": 11, "topics": ["cache coherence", "TLB consistency", "memory hierarchy"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 46, "title": "Remarks on Memory Hierarchy", "summary": "Section divider introducing discussion of memory hierarchy design considerations", "main_text": "Remarks on Memory Hierarchy", "notes_text": null, "keywords": ["memory hierarchy", "design", "performance"], "images": [], "layout": {"num_text_boxes": 1, "num_images": 0, "dominant_visual_type": "section_divider"}, "section": "Memory Hierarchy", "metadata": {"course": "CS356", "unit": 11, "topics": ["memory hierarchy"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 47, "title": "Memory Hierarchy", "summary": "Complete memory hierarchy overview: Registers (fastest) → L1 Cache (≈1ns, 32KB) → L2 Cache (≈4ns, 256KB) → L3 Cache (≈10-20ns, 8MB) → Main Memory (≈100ns, 16GB) → Secondary Storage (≈2ms HDD / 16μs SSD, 2TB). Unit of transfer increases at each level", "main_text": "Memory Hierarchy from top to bottom:\n\nRegisters (fastest, smallest, most expensive)\nL1 Cache (32 kB, ≈ 1 ns)\nL2 Cache (256 KB, ≈ 4 ns)\nL3 Cache (8 MB, ≈ 10-20 ns)\nMain Memory (16 GB, ≈ 100 ns)\nSecondary Storage (2 TB, ≈ 2 ms HDD seek / ≈ 16 μs SSD random read)\n\nUnit of Transfer:\n• Registers to Cache: Word or Byte (1-8 bytes)\n• Cache to Memory: Cache Block/Line (64 bytes) - spatial locality\n• Memory to Disk: Page (4 kB) - spatial locality\n\nReference: Network RTT: 0.5 ms same datacenter, 60 ms CA to NY, 150 ms CA to EU", "notes_text": null, "keywords": ["memory hierarchy", "latency", "capacity", "cache block", "page", "spatial locality"], "images": [{"type": "hierarchy_diagram", "description": "Pyramid diagram showing memory hierarchy with size, latency, and cost increasing in opposite directions. Shows physical components at each level"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "hierarchy_diagram"}, "section": "Memory Hierarchy", "metadata": {"course": "CS356", "unit": 11, "topics": ["memory hierarchy", "latency", "capacity", "transfer units"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 48, "title": "Virtual Memory vs Other Caches", "summary": "Disk latency is drastically worse (20,000x HDD or 160x SSD vs L3-RAM 10x), driving design decisions: larger page size (4KB vs 64B), fully associative, SW fault handling, pseudo-LRU replacement", "main_text": "Disk latency is much higher than memory: 20,000x (HDD) or 160x (SSD)...\n• Drastically worse than L3 vs RAM (10x)\n\nThis determines many design decisions:\n• Page size larger than cache block size (e.g., 4 kB vs 64 B) to amortize latency\n• VM uses physical memory as a fully associative (lower miss rate) cache of pages, while caches/TLBs are set-associative\n• VM handles page faults in SW, while caches/TLBs handle misses in HW\n• VM implements pseudo-LRU, caches/TLB use full LRU for low associativity, or random", "notes_text": null, "keywords": ["disk latency", "page size", "fully associative", "miss handling", "LRU", "design tradeoffs"], "images": [{"type": "diagram", "description": "Hard disk drive cross-section showing read/write head, platters, and mechanical components illustrating why disk access is slow"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "comparison"}, "section": "Memory Hierarchy", "metadata": {"course": "CS356", "unit": 11, "topics": ["VM design", "latency impact", "cache vs VM"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 49, "title": "Disk Cache", "summary": "OS maintains disk cache (page cache in Linux) to minimize disk I/O. File pages cached in memory frames. Multiple processes reading same file hit cache. Writes go to memory (write back) with periodic sync. Cache size reduced when memory scarce", "main_text": "OS implements a disk cache ('page cache' in Linux) to minimize disk IO:\n\n• Files are also split into pages\n• When a process reads from a file, the corresponding pages are saved to memory frames\n  ○ If another process reads the same portions of the file, we only access physical memory!\n• If any process writes to the file, we only make changes in memory (write back) and sync periodically\n\nThe amount of disk cache is reduced when physical memory is scarce", "notes_text": null, "keywords": ["disk cache", "page cache", "file I/O", "write back", "buffer cache", "memory pressure"], "images": [{"type": "diagram", "description": "Shows file X pages (0-FileX, 1-FileX) both on disk and cached in physical memory frames, with multiple process virtual address spaces mapping to the same cached pages"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "system_optimization"}, "section": "Memory Hierarchy", "metadata": {"course": "CS356", "unit": 11, "topics": ["disk caching", "file system cache", "I/O optimization"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 50, "title": "Monitoring VM Usage: Linux", "summary": "Explains Linux 'free' and 'lsblk' command outputs showing total/used/free physical memory, shared memory, buffer/cache memory (reclaimable), available memory, and swap partition on disk", "main_text": "Linux memory monitoring:\n\ntotal: Total physical memory (62Gi)\nused: Physical memory used by processes (392Mi)\nfree: Physical memory not used at all (46Gi)\nshared: Physical memory shared between processes (13Mi)\nbuff/cache: Physical memory used by disk caches - can be reclaimed if processes need more memory (16Gi)\navailable: Physical memory available after reclaiming caches (61Gi)\n\nSwap: Total swap area on disk (29Gi)\nlsblk shows swap partition: /dev/sda5 (29.8G, type SWAP)", "notes_text": null, "keywords": ["Linux", "free command", "memory monitoring", "swap", "buffer cache", "available memory"], "images": [{"type": "screenshot", "description": "Terminal output showing 'free -h' command with memory statistics and 'lsblk' showing disk partitions including swap partition"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "practical_example"}, "section": "Memory Hierarchy", "metadata": {"course": "CS356", "unit": 11, "topics": ["Linux memory", "monitoring tools", "swap space"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 51, "title": "Monitoring VM Usage: macOS", "summary": "macOS Activity Monitor showing memory usage: Physical Memory 16GB, Memory Used 13.05GB, Cached Files 2.27GB, Swap Used 538.9MB", "main_text": "macOS Activity Monitor Memory tab shows:\n\nPhysical Memory: 16.00 GB\nMemory Used: 13.05 GB\nCached Files: 2.27 GB\nSwap Used: 538.9 MB\nApp Memory: 8.97 GB\nWired Memory: 1.66 GB\nCompressed: 1.57 GB\n\nLists processes sorted by memory usage with columns for Process Name, Memory, Threads, Ports, PID, User", "notes_text": null, "keywords": ["macOS", "Activity Monitor", "memory pressure", "swap", "compressed memory", "wired memory"], "images": [{"type": "screenshot", "description": "macOS Activity Monitor window showing Memory tab with process list and memory statistics at bottom"}], "layout": {"num_text_boxes": 1, "num_images": 1, "dominant_visual_type": "practical_example"}, "section": "Memory Hierarchy", "metadata": {"course": "CS356", "unit": 11, "topics": ["macOS memory", "monitoring tools", "memory compression"]}}
{"deck_name": "CS356_Unit11_VirtualMemory", "slide_number": 52, "title": "Monitoring VM Usage: Windows", "summary": "Windows Task Manager Performance tab memory metrics explained: In use (physical memory used), Current committed (virtual memory allocated), Max committed (max allocatable virtual memory), Cached (disk cache), Paged/non-paged pools (kernel memory)", "main_text": "Windows Task Manager memory metrics:\n\n• In use: physical memory used (total-available on Linux)\n• Current committed: virtual memory allocated by processes (used on Linux)\n• Max committed: virtual memory that can be allocated by processes (used+available on Linux)\n• Cached: used for disk caches\n• Paged/non-paged pools: kernel and driver memory that can/cannot be swapped to disk\n\nExample shown: 2.8 GB in use (124 MB compressed), 1.1 GB available, 5.1/7.8 GB committed, 1.1 GB cached", "notes_text": null, "keywords": ["Windows", "Task Manager", "memory usage", "committed memory", "paged pool", "non-paged pool"], "images": [{"type": "screenshot", "description": "Windows Task Manager Performance tab showing Memory panel with usage graph and detailed memory statistics"}], "layout": {"num_text_boxes": 2, "num_images": 1, "dominant_visual_type": "practical_example"}, "section": "Memory Hierarchy", "metadata": {"course": "CS356", "unit": 11, "topics": ["Windows memory", "monitoring tools", "committed memory"]}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":1,"chunk_index":0,"title":"Unit 12: Heap Management Overview","summary":"Introduces heap management in CS356, focusing on allocators, smart pointers, and garbage collection, and why heap behavior matters for program correctness and performance.","main_text":"This unit covers how dynamic memory on the heap is managed, contrasting manual allocators with automated garbage collection. The lecture frames three major themes: (1) explicit heap allocation/deallocation via library allocators such as malloc/free (or new/delete), (2) allocator design challenges like fragmentation, free-block tracking, and placement policies, and (3) implicit deallocation via garbage collection, including reference counting and tracing collectors. Students should understand the tradeoffs between control and safety: manual allocation enables predictable performance but risks dangling pointers and leaks, while GC reduces programmer burden but introduces runtime overhead and pause behavior. The rest of the deck builds from memory layout fundamentals to concrete allocator mechanisms and finally to GC strategies used in modern languages.","notes_text":"Goal: connect low-level allocator mechanisms to high-level language memory models and to MallocLab implementation choices.","keywords":["heap management","allocators","malloc","free","new/delete","garbage collection","smart pointers","fragmentation","CS356","dynamic memory"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Heap Management Overview","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":2,"chunk_index":0,"title":"Stack vs Heap","summary":"Explains how stack memory differs from heap memory in purpose, lifetime, performance, and limitations.","main_text":"The stack is used by the function call mechanism. Each active function call in a thread has a stack frame that stores local variables. Stack allocation and deallocation are extremely fast because they typically only adjust the stack pointer register (e.g., %rsp). When a function returns, its locals are automatically deallocated, which is convenient but means the data disappears—functions must not return pointers to stack data. The stack is size-limited by the OS (e.g., ~8MB per thread on Linux), so deep recursion or large locals can cause stack overflow.\n\nThe heap is used for long-lived or large data shared across functions and threads. The OS provides a heap region for the entire process, allowing large allocations and lifetimes that extend beyond a function return. Heap memory must be explicitly reused and deallocated when no longer needed. Unlike the stack, heap allocation has overhead because the allocator must track free blocks and manage fragmentation.","notes_text":"Key exam point: lifetime + ownership. Stack is automatic and scoped; heap is manual/shared and can outlive callers.","keywords":["stack","heap","stack frame","automatic deallocation","long-lived objects","stack overflow","heap overhead","sharing","lifetime"],"images":[{"description":"Memory layout diagram showing code/data, heap growing upward, and stack growing downward in virtual address space.","labels":[".text",".data",".bss","heap","stack","memory-mapped regions","brk"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":12,"topic":"Memory Regions: Stack and Heap","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":3,"chunk_index":0,"title":"Heap Management: Explicit vs Implicit Deallocation","summary":"Contrasts manual freeing of heap blocks with garbage collection and highlights typical bugs.","main_text":"Heap memory can be reclaimed in two ways. With explicit deallocation, programmers manually free heap blocks—free() in C, delete in C++. This provides control but creates two classic failure modes. First, dangling pointers occur if a block is freed while still referenced elsewhere, leading to use-after-free bugs. Second, memory leaks occur if a block is never freed, permanently reducing available heap space.\n\nWith implicit deallocation (garbage collection), the runtime automatically detects and frees blocks that are no longer in use. Detection can be done by reference counting or by exploring the object graph to find unreachable objects. GC reduces dangling pointer risk but leaks are still possible if references to unneeded objects remain. A C example illustrates explicit allocation and freeing: a function mallocs an array of Points, uses it, and must free it once no longer needed.","notes_text":"Remember: GC solves many safety issues but not logical leaks (keeping references).","keywords":["explicit deallocation","implicit deallocation","garbage collection","dangling pointer","memory leak","free","delete","reference counting"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Explicit vs Garbage-Collected Heap","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":4,"chunk_index":0,"title":"Extending the Heap with sbrk","summary":"Shows how allocators request more heap space from the OS when free space is insufficient.","main_text":"When the heap lacks space to satisfy an allocation, the allocator must extend it by requesting more memory from the OS. In Linux, a traditional interface is sbrk(intptr_t increment), which moves the program break (brk) pointer up or down and returns the old brk on success. If increment > 0, the heap grows upward and the returned address is where the new heap region starts. sbrk(0) returns the current heap end. Newly obtained memory is zero-initialized.\n\nA process memory map places code (.text), initialized data (.data), uninitialized data (.bss), the heap, memory-mapped regions (e.g., shared libraries), and the user stack in distinct virtual ranges. The heap starts near a low address and grows upward toward higher addresses, while the stack grows downward. Understanding brk movement is essential for implementing custom allocators and for diagnosing allocation failures.","notes_text":"Modern allocators may also use mmap for large requests, but sbrk explains the core model.","keywords":["sbrk","brk pointer","heap extension","program break","virtual address space","memory map","Linux heap"],"images":[{"description":"Virtual memory layout figure labeling text/data/bss/heap/brk/mmap regions and user stack.","labels":["brk","heap","stack","mmap","0x10000000","0x80000000"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Heap Growth and OS Interface","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":5,"chunk_index":0,"title":"Libc Allocator API and Example","summary":"Defines malloc/calloc/free semantics and illustrates stack vs heap addresses with a runtime example.","main_text":"Libc exposes the standard heap allocator interface. malloc(size) allocates size bytes and returns a pointer to the start of a usable payload region. calloc(nmemb, size) allocates nmemb*size bytes and initializes the payload to zero. free(ptr) releases a block previously returned by malloc/calloc, returning it to the allocator’s pool for reuse; ptr must be exactly the pointer originally returned.\n\nAn example program reads a runtime-known number of students into a stack variable num, then allocates an int array scores on the heap of length num. The diagram emphasizes that stack variables reside in the user stack region, while heap blocks occupy addresses in the heap region; malloc returns the payload pointer. Correctness requires calling free(scores) when finished. The example also motivates why heap blocks cannot be relocated easily: programs store and use their addresses directly.","notes_text":"calloc is just malloc + memset(0) from a client perspective.","keywords":["malloc","calloc","free","payload pointer","heap addresses","stack variable","runtime-sized array"],"images":[{"description":"Diagram showing stack frame with num/scores pointer and heap array scores[0..n-1] addresses.","labels":["scores","num","stack frame","heap"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":12,"topic":"Allocator API","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":6,"chunk_index":0,"title":"Allocator Requirements and Performance Goals","summary":"Lists constraints on heap allocators and defines throughput/utilization metrics.","main_text":"An allocator must handle an arbitrary interleaving of malloc/free requests and respond immediately—it cannot delay allocations to optimize globally. It can only use heap space (not stack/globals) for its metadata, and it cannot move already allocated blocks because user code holds pointers to their addresses. All blocks must be aligned to 8 or 16 bytes to preserve struct alignment on 64-bit systems.\n\nPerformance goals emphasize high throughput (many malloc/free operations per second) and high memory utilization. Utilization measures how much of the allocated heap is actually in use rather than trapped in unusable fragments. Peak utilization after k operations is U(k) = max{P(i)}/H(k), where P(i) is heap memory currently allocated (not freed) after i requests, and H(k) is total heap size obtained from the OS after k requests. Fragmentation is the main enemy of utilization.","notes_text":"These goals trade off: faster placement may worsen fragmentation, hurting utilization.","keywords":["allocator requirements","alignment","no moving blocks","throughput","utilization","peak memory utilization","P(i)","H(k)"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Allocator Design Constraints","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":7,"chunk_index":0,"title":"Fragmentation: External vs Internal","summary":"Defines fragmentation types and why they reduce effective heap capacity.","main_text":"Fragmentation occurs when free heap memory exists but cannot be used effectively. External fragmentation arises when there are many small free gaps between allocated blocks. Even if total free memory is large, a request may fail because no single contiguous free block is big enough. Internal fragmentation happens when the allocator gives a block larger than the payload the user requested, so unused bytes inside allocated blocks are wasted. Internal fragmentation is common with fixed-size chunk allocators or when rounding payloads up for alignment.\n\nAllocator strategies such as coalescing adjacent free blocks combat external fragmentation, while careful block sizing and splitting help reduce internal fragmentation. Since allocated blocks cannot be moved, fragmentation accumulates over time in long-running programs, making management policies crucial to sustaining high utilization.","notes_text":"Think of external as ‘between blocks’ and internal as ‘within blocks’.","keywords":["fragmentation","external fragmentation","internal fragmentation","contiguous free memory","alignment waste","heap utilization"],"images":[{"description":"Illustrations showing scattered free gaps (external) and oversized allocated blocks with unused interior (internal).","labels":["allocated","free"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Fragmentation","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":8,"chunk_index":0,"title":"Implementation Issues in Heap Allocators","summary":"Outlines core allocator subproblems: tracking free blocks, placement, splitting, and coalescing.","main_text":"Implementing a heap allocator requires solving several related subproblems. First, free block management: the allocator must track which heap regions are free vs allocated to enable reuse. Second, placement algorithms: when a request arrives, the allocator must choose which free block to allocate (e.g., first-fit, next-fit, best-fit). Third, splitting: if a chosen free block is larger than needed, the allocator may split it into an allocated block plus a smaller free remainder. Fourth, coalescing: when blocks are freed, adjacent free blocks should be merged to reduce external fragmentation. Each feature introduces metadata overhead and affects throughput and utilization.\n\nThe lecture uses these issues as a roadmap for the remaining allocator designs, starting with implicit lists, then explicit lists, then segregated lists.","notes_text":"MallocLab asks you to implement find_fit, place, mm_malloc, and coalesce correctly.","keywords":["free block management","placement algorithm","first-fit","best-fit","splitting","coalescing","allocator metadata"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Allocator Components","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":9,"chunk_index":0,"title":"Implicit vs Explicit Free Lists","summary":"Compares scanning-based implicit lists to pointer-based explicit free lists.","main_text":"Allocated blocks are owned by the programmer and do not need explicit tracking. Instead, allocators manage free blocks. With an implicit free list, all blocks (allocated + free) are laid out in a linear sequence in the heap. To satisfy malloc, the allocator scans through the sequence, checking each block’s header to find a suitable free block. This is simple but can be slow because allocated blocks are also scanned.\n\nWith an explicit free list, the allocator maintains a separate linked list containing only free blocks. Each free block stores pointers to the next (and possibly previous) free block, allowing faster searches because allocated blocks are skipped. The tradeoff is additional metadata inside free blocks and more complex insertion/removal logic.","notes_text":"Implicit lists are baseline; explicit lists improve throughput at some utilization cost.","keywords":["implicit free list","explicit free list","linear scan","free list pointers","metadata overhead"],"images":[{"description":"Side-by-side heap diagrams: implicit list scanning all blocks; explicit list linking only free blocks.","labels":["header","allocated/free bit","free_list","next ptr"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"comparison"},"metadata":{"course":"CS356","unit":12,"topic":"Free List Structures","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":10,"chunk_index":0,"title":"Implicit Free List Block Format and Alignment","summary":"Explains heap block headers, alignment, and epilogue in an implicit list allocator.","main_text":"Implicit free lists require a way to locate block boundaries and allocation status. Blocks are aligned to the largest type—8-byte boundaries on 64-bit systems. The slide uses “word” (4 bytes) and “double word” (8 bytes) terminology. Each block begins with a header storing the block size (a multiple of 8) plus a low-bit flag indicating allocated (1) or free (0). Payloads follow the header and are also aligned. The heap ends with an epilogue header: an always-allocated block of size 0 that marks heap termination.\n\nPointer arithmetic is subtle: adding 1 to a typed pointer advances by sizeof(type), so allocator helper functions must compute next/prev headers carefully. A suggested debugging loop iterates from the first block header while mm_block_size(b) != 0, printing sizes and allocation bits to verify list integrity.","notes_text":"Block size includes header/footer; payload size is block size minus overhead.","keywords":["block header","allocated bit","alignment","double word","epilogue block","pointer arithmetic","implicit list"],"images":[{"description":"Block layout diagram showing header, payload, padding, and size/allocated-bit encoding.","labels":["header","payload","padding","size","allocated bit"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Implicit List Mechanics","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":11,"chunk_index":0,"title":"Coalescing Motivation and Boundary Tags","summary":"Shows why footers enable constant-time coalescing of adjacent free blocks.","main_text":"When a block is freed, the allocator may want to merge it with adjacent free blocks to reduce external fragmentation. Without extra metadata, a freed block’s header provides no direct way to find the previous block; determining whether the previous block is free would require scanning from heap start, costing O(n).\n\nBoundary tags solve this by adding a footer to each block. The footer is a copy of the header placed at the end of the block, immediately before the next block’s header. When freeing a block, the allocator can read the previous block’s footer to learn its size and allocation status, compute the previous header’s address in O(1), and coalesce if needed. This enables fast, local coalescing during free operations.","notes_text":"Boundary tags are required for segregated fit too.","keywords":["coalescing","boundary tags","footer","previous block","O(1) coalescing","free operation"],"images":[{"description":"Heap sequence illustrating headers and footers (boundary tags) and how freeing triggers merge.","labels":["footer","header","free","allocated"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Coalescing with Footers","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":12,"chunk_index":0,"title":"Coalescing Procedure, Timing, and Cases","summary":"Details steps for coalescing, when to do it, and the four possible neighbor cases.","main_text":"To coalesce a freed block, the allocator starts from the payload pointer passed to free, steps back one word to the block header, and obtains its size. It then checks the footer of the previous block (one word before the header) to see if that block is free; if so, it computes the previous header address using the previous size and merges the blocks by updating header and footer sizes. It also checks the next block’s header to see if the next block is free.\n\nCoalescing may be immediate (done on every free) or deferred (done later when searching for space). Immediate coalescing is simpler but can waste work if blocks would be reallocated soon. With immediate coalescing there are four neighbor cases: (1) prev alloc, next alloc; (2) prev alloc, next free; (3) prev free, next alloc; (4) prev free, next free. Each case updates sizes and free-list links accordingly.","notes_text":"MallocLab typically uses immediate coalescing with these four cases.","keywords":["coalescing cases","immediate vs deferred","prev_alloc","next_alloc","header/footer update","free"],"images":[{"description":"Step-numbered coalescing example showing merge of adjacent free blocks into a larger free block.","labels":["case 1-4","prev","next","free"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":12,"topic":"Coalescing Logic","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":13,"chunk_index":0,"title":"Placement Algorithms: First/Next/Best Fit","summary":"Defines three classic policies for choosing a free block and their tradeoffs.","main_text":"Placement algorithms decide which free block to allocate for a request. First fit scans from the beginning of the heap on each malloc and chooses the first free block large enough. It is fast on average but may leave small fragments near the front, increasing external fragmentation. Next fit continues scanning from where the last allocation ended, reducing repeated work in the front of the heap but sometimes worsening fragmentation patterns. Best fit searches the entire free list to find the smallest free block that satisfies the request, which tends to reduce leftover space and fragmentation but increases search time.\n\nMallocLab’s find_fit(size) function is where these policies are implemented. The choice affects throughput and utilization: faster scans raise throughput, while more selective fits can improve utilization.","notes_text":"Best fit approximations appear later in segregated fit.","keywords":["placement algorithm","first fit","next fit","best fit","find_fit","tradeoff throughput/utilization"],"images":[{"description":"Three heap illustrations showing how first/next/best fit pick different free blocks for alloc(4) and alloc(2).","labels":["First Fit","Next Fit","Best Fit"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"comparison"},"metadata":{"course":"CS356","unit":12,"topic":"Placement Policies","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":14,"chunk_index":0,"title":"Splitting and mm_malloc Integration","summary":"Shows how allocators split oversized blocks and combine find_fit/place in mm_malloc.","main_text":"A free block chosen by a placement policy may be larger than required. If the leftover portion after satisfying the request is large enough to form a valid free block (with its own header/footer and minimum payload), the allocator splits it. Example: malloc(12) requires header + footer overhead and rounds payload up to the next multiple of 8, yielding a required block size of 24 bytes. If a 40-byte free block is selected, it can be split into a 24-byte allocated block and a 16-byte free block.\n\nMallocLab’s place(bp, size) handles splitting and returns the allocated block header. mm_malloc(size) computes required_block_size, calls find_fit; if none is found, it extends the heap; then it calls place and returns a payload pointer. Correct splitting reduces internal fragmentation while enabling reuse of the remaining free space.","notes_text":"Remember minimum free block size must hold header/footer and free pointers if explicit lists are used.","keywords":["splitting","place","mm_malloc","required_block_size","payload rounding","internal fragmentation"],"images":[{"description":"Before/after splitting diagram showing a large free block divided into allocated + smaller free block.","labels":["allocated","free","24","16","40"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Splitting and Allocation Flow","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":15,"chunk_index":0,"title":"Explicit Free Lists","summary":"Introduces storing next/prev pointers inside free blocks and flexible ordering of free lists.","main_text":"Explicit free lists store linking pointers directly inside free blocks. When a block is free, part of its payload area is repurposed to hold pointers to other free blocks, typically forming a doubly-linked list. This speeds up allocation searches because only free blocks are traversed. The tradeoff is that free blocks must be large enough to contain the pointers plus header/footer, increasing the minimum block size and possibly internal fragmentation for tiny allocations.\n\nFree blocks can be inserted into the list in any order. If coalescing is deferred, the free list may contain multiple blocks out of address order (e.g., blocks 3, 1, 4). Alternatives include maintaining lists sorted by address (simplifies coalescing) or by size (approximates best fit).","notes_text":"Insertion/removal correctness is the main complexity in explicit list MallocLab variants.","keywords":["explicit free list","doubly linked list","prev/next free block","minimum block size","free list ordering"],"images":[{"description":"Example heap showing how freeing blocks updates an explicit free list with prev/next pointers.","labels":["Prev Free Blk","Next Free Blk","free_list"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Explicit Free List Allocators","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":16,"chunk_index":0,"title":"Segregated Free Lists: High-Level Idea","summary":"Presents segregated lists as multiple free lists partitioned by block size.","main_text":"Segregated free lists improve search efficiency by maintaining several separate free lists, each responsible for a size class of free blocks. On allocation, the allocator chooses the appropriate size class based on the request and searches only within that class; if none is found, it may consult larger classes. Two main variants are introduced: segregated storage and segregated fit. Both aim to combine higher throughput (shorter searches) with good utilization (better-matched blocks), compared to a single global explicit list.\n\nThe subsequent slides detail how fixed-size chunk lists (segregated storage) eliminate headers and coalescing at the cost of internal fragmentation, while variable-size classed lists (segregated fit) retain boundary tags and coalescing to reduce external fragmentation.","notes_text":"Conceptually: ‘bins’ by size.","keywords":["segregated free lists","size classes","bins","segregated storage","segregated fit","throughput"],"images":[{"description":"Diagram showing multiple free lists (free16, free32, free64, free4k) mapped to size ranges.","labels":["free16","free32","free64","free4k"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Segregated Lists","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":17,"chunk_index":0,"title":"Segregated Storage","summary":"Explains fixed-size chunk free lists and how allocation/free work in this scheme.","main_text":"Segregated storage breaks the heap into multiple lists of fixed-size blocks (chunks). Each list corresponds to one block size, such as 16-byte, 32-byte, 64-byte, or 4KB blocks. On malloc, the allocator selects the smallest fixed size that can hold the requested payload and returns a block from that list. If the list is empty, it extends the heap, splits the new region into blocks of that size, and adds them to the list. On free, the block is returned to its size-appropriate list; list order need not match address order.\n\nBecause all blocks in a list have identical size, headers are unnecessary and coalescing/footers are not used. Metadata overhead is low and lists can be singly linked. The minimum block size is one pointer (for the next free pointer). The downside is internal fragmentation: a request for 33 bytes must use a 64-byte block.","notes_text":"Fast and simple; used for small object allocators and slab allocators.","keywords":["segregated storage","fixed-size blocks","no headers","no coalescing","internal fragmentation","minimum block size"],"images":[{"description":"Heap partition with multiple fixed-size free lists and overlays labeling next pointer and payload.","labels":["next","padding","payload","free16","free32"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Segregated Storage Allocators","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":18,"chunk_index":0,"title":"Segregated Fit","summary":"Describes variable-size classed lists with boundary tags and coalescing.","main_text":"Segregated fit maintains multiple explicit free lists by size ranges, but blocks within a class are variable sized. Because blocks can be split and coalesced, headers and footers (boundary tags) are required. Each list covers a size interval, such as [16–31], [32–63], and ≥64 bytes. A free block placed in list n satisfies n ≤ size < m, where m is the next larger class.\n\nOn initialization, the heap may start as a single large free block in the biggest class. As allocations occur, blocks split and remaining fragments move to smaller classes. If no block exists in the target class, the allocator searches progressively larger classes; if even the largest is empty, it extends the heap. On free, the allocator coalesces with neighbors, removes merged blocks from their lists, and inserts the combined block into the correct size class. This approximates best fit while skipping overly small blocks.","notes_text":"This is the dominant design in high-performance general-purpose allocators.","keywords":["segregated fit","size classes","boundary tags","coalescing","split blocks","approximate best fit"],"images":[{"description":"Before/after example showing malloc from size class lists and coalescing after free(a2).","labels":["free16","free32","free64","ptr1","before/after"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":12,"topic":"Segregated Fit Allocators","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":19,"chunk_index":0,"title":"Garbage Collection Motivation and Approaches","summary":"Introduces GC as an alternative to manual memory management and previews GC families.","main_text":"Manual heap management is powerful but error-prone. Tools like valgrind can detect leaks, but automated garbage collection aims to reclaim unreachable heap blocks without explicit free calls. Two broad GC approaches are introduced. Reference counting keeps a count of active references per object and frees an object when the count drops to zero. Tracing garbage collectors periodically explore the object graph from a set of roots (globals, static members, and locals in active stack frames) and free any object not reachable from those roots. Tracing can collect cycles, unlike pure reference counting.\n\nGC changes the programming model by shifting responsibility to the runtime, but it adds overhead and can impact latency depending on the collector design. The remaining slides dive into these approaches in detail.","notes_text":"Good framing for languages like Python (RC + cycle GC) vs Java/Go/JS (tracing).","keywords":["garbage collection","valgrind","reference counting","tracing GC","roots","object graph","unreachable objects"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Garbage Collection Overview","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":20,"chunk_index":0,"title":"Reference Counting","summary":"Explains reference counting GC, its basic operations, and its weakness with cycles.","main_text":"In reference counting, each heap object stores a reference count. When a new reference to the object is created, the count increments; when a reference is destroyed or reassigned, the count decrements. If the count reaches zero, the object is deallocated immediately. This may cascade: freeing an object can reduce counts of objects it referenced, potentially freeing them too. Python uses reference counting as a primary GC mechanism, illustrated by getrefcount showing counts increasing with new aliases and decreasing with del.\n\nThe major limitation is cycles. If objects reference each other in a closed loop, their counts never drop to zero even when the cycle is unreachable from program roots. As a result, reference counting alone cannot reclaim cyclic garbage without additional cycle-detection or tracing support.","notes_text":"Reference counting provides prompt reclamation but adds overhead to pointer updates.","keywords":["reference counting","refcount increment/decrement","Python GC","cascade deallocation","cycles","uncollectable garbage"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Reference Counting GC","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":21,"chunk_index":0,"title":"Tracing GC: Mark & Sweep","summary":"Describes mark-and-sweep tracing GC and its tradeoffs.","main_text":"Tracing garbage collectors run when available heap memory is low. The collector identifies roots—global variables, static fields, and locals of active functions—and explores the heap object graph from those roots (often depth-first). In the mark phase, each reachable object is visited and marked via a bit in its header. In the sweep phase, the collector scans all heap objects and frees those not marked.\n\nMark-and-sweep can reclaim cycles because reachability, not reference count, determines liveness. However, it introduces overhead: the sweep checks every object even though only some are reachable. It can also leave memory fragmented because freed objects create holes; a compacting phase may be added to relocate reachable objects and reduce fragmentation. Many systems still require a stop-the-world pause during tracing.","notes_text":"Key contrast: tracing cost is periodic + global; RC cost is incremental + local.","keywords":["tracing GC","mark and sweep","roots","reachable objects","mark bit","sweep phase","compaction","fragmentation"],"images":[{"description":"Mark-and-sweep diagram showing reachable objects from a global root and unreachable objects freed during sweep.","labels":["mark","sweep","global var","reachable bit"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"flowchart"},"metadata":{"course":"CS356","unit":12,"topic":"Tracing Garbage Collectors","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":22,"chunk_index":0,"title":"Stop & Copy and Generational GC","summary":"Explains stop-and-copy tracing and generational hypothesis with Eden/Survivor/Tenured regions.","main_text":"Stop-and-copy tracing addresses fragmentation and sweep overhead by dividing the heap into two equal regions. The program allocates only in the active region. During GC, reachable objects are copied into the inactive region, compacted tightly, and allocation switches to that region. Unreachable objects are never touched, so sweep is avoided. The downside is that long-lived objects are copied repeatedly.\n\nGenerational collectors exploit the weak generational hypothesis: most objects die young. The heap is partitioned into Eden (new objects), two Survivor spaces S0/S1, and a Tenured (Old Gen) region. When Eden fills, live objects are copied to a Survivor space; each GC cycle copies survivors between S0 and S1 and increments a survival counter. Objects that survive many cycles are promoted to Tenured, which is collected less frequently and may use compaction. This reduces copying of old objects while keeping young-gen GC fast.","notes_text":"Java’s collectors are generational; Eden GC pauses are usually small but frequent.","keywords":["stop and copy","generational GC","weak generational hypothesis","Eden","Survivor spaces","S0","S1","Tenured/Old Gen","promotion"],"images":[{"description":"Heap region diagram labeling Eden, S0, S1, and Tenured with copying paths during GC.","labels":["Eden","S0","S1","Tenured","brk"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Generational Tracing GC","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":23,"chunk_index":0,"title":"GC Variations and Remaining Issues","summary":"Summarizes parallel and concurrent collectors and notes that logical leaks can persist under GC.","main_text":"Tracing collectors come in several practical variants. Parallel GC uses multiple threads to explore the object graph, especially in young generations, reducing pause time by leveraging multicore hardware. Concurrent mark-and-sweep performs some marking or sweeping while the program continues running, improving latency. However, it must track concurrent mutations to already-visited objects; some stop-the-world steps remain, such as when Eden fills or when Tenured space requires compaction or promotion.\n\nEven with GC, memory leaks can occur if the program retains references to objects that are no longer semantically needed. Such objects remain reachable from roots and will not be reclaimed. Therefore, GC improves safety but cannot fix poor object-lifecycle logic.","notes_text":"Modern low-pause collectors (e.g., ZGC, Shenandoah) are concurrent + region-based.","keywords":["parallel GC","concurrent mark and sweep","stop-the-world","latency","promotion","GC memory leaks"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"GC Variants and Limitations","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":24,"chunk_index":0,"title":"Coalescing Cases (Immediate Coalescing)","summary":"Summarizes the four neighbor-allocation cases an allocator checks when coalescing a newly freed block immediately.","main_text":"With immediate coalescing, the allocator merges a freed block with adjacent free blocks right away to reduce external fragmentation and keep the free structure consistent. Using boundary tags, only four cases matter, based on whether the previous and next physical neighbors are allocated or free. Case 1: prev allocated, next allocated → no coalescing; the freed block becomes a standalone free block. Case 2: prev allocated, next free → merge with the next block; update header/footer to combined size. Case 3: prev free, next allocated → merge with the previous block; update combined size and return pointer to the previous header. Case 4: prev free, next free → merge all three into one larger free block; remove neighboring free blocks from any free lists and write the new size into the outer header/footer. These four cases are enough to guarantee correctness when coalescing is done at free-time.","notes_text":"Boundary tags let you find prev block in O(1), making these checks cheap.","keywords":["coalescing","immediate coalescing","boundary tags","prev/next allocation","four cases","external fragmentation"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Coalescing Cases","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":25,"chunk_index":0,"title":"Coalescing in MallocLab: free_coalesce()","summary":"Shows the structure of the MallocLab coalescing helper and how it returns the correct merged block.","main_text":"The slide provides the MallocLab skeleton for `static BlockHeader *free_coalesce(BlockHeader *bp)`. After a block is freed, this helper reads neighbor allocation bits (prev via footer, next via header), then branches into the four immediate-coalescing cases. Each branch computes the new merged size and rewrites the header/footer tags to reflect the combined free block. If the previous block is free (cases 3 or 4), the function typically returns a pointer to the previous block’s header, because that becomes the start of the merged block. If the previous block is allocated but the next is free (case 2), it returns `bp` after merging with next. This helper is invoked by `mm_free` and also by `extend_heap` to ensure newly created free space is coalesced with neighbors.","notes_text":"Your implementation must also update free-list links if using explicit/segregated lists.","keywords":["free_coalesce","MallocLab","mm_free","extend_heap","header/footer update","neighbor cases"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Coalescing in MallocLab","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":26,"chunk_index":0,"title":"Placement Algorithms: First Fit, Next Fit, Best Fit","summary":"Defines three classic policies for selecting a free block and illustrates how each behaves.","main_text":"Placement policies determine which free block an allocator chooses for a request. **First fit** scans from the heap start each time and picks the first free block that is large enough. It is usually fast, but tends to leave small fragments near the front of the heap, increasing external fragmentation over time. **Next fit** is similar but continues scanning from where the previous search ended, rather than restarting at the heap beginning. This reduces repeated scanning of early blocks but can produce more uneven fragmentation patterns depending on workloads. **Best fit** searches the entire free list or heap to find the smallest free block that satisfies the request, minimizing leftover space and often improving utilization, but at a higher search cost. The diagrams on the slide show that the same allocation requests (e.g., alloc(4), alloc(2)) can land in different places depending on the policy.","notes_text":"In MallocLab, this choice lives in `find_fit`.","keywords":["placement algorithm","first fit","next fit","best fit","find_fit","throughput vs utilization"],"images":[],"layout":{"num_text_boxes":12,"num_images":0,"dominant_visual_type":"comparison"},"metadata":{"course":"CS356","unit":12,"topic":"Placement Algorithms","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":27,"chunk_index":0,"title":"Placement in MallocLab: find_fit() Stub","summary":"Gives the `find_fit` skeleton and clarifies what it should return to support allocation.","main_text":"This slide shows the required MallocLab stub for `BlockHeader* find_fit(int size)`. The function must search the allocator’s representation of free space (implicit list, explicit list, or size-class list) to find a free block whose block size is at least the requested `size`. If a suitable block is found, return a pointer to that block (typically its header), enabling `mm_malloc` to call `place` there. If no block fits, return `NULL`, signaling `mm_malloc` to extend the heap. The exact traversal and stopping rule implement your chosen placement policy (first-fit, next-fit, best-fit approximation, etc.), so correctness plus reasonable performance depends heavily on this function.","notes_text":"If you return a payload pointer instead, stay consistent with helper macros.","keywords":["find_fit","MallocLab","placement policy","free block search","return NULL","allocator traversal"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"find_fit","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":28,"chunk_index":0,"title":"Splitting During Placement","summary":"Explains how allocators split an oversized free block when placing an allocation.","main_text":"Once a free block is chosen, the allocator checks if it is significantly larger than the required size. If so, it **splits** the block into two: an allocated block of `required_size` and a remaining free block containing the leftover bytes. Splitting is only done when the remainder is large enough to form a valid free block (including header/footer and any minimum payload/pointer space). The slide’s numeric example illustrates a chosen free block being divided so the front portion becomes allocated (allocated bit set), while the tail becomes a smaller free block with its own header/footer tags. Splitting reduces internal fragmentation by avoiding handing out more space than necessary, and it increases future reuse by keeping leftover bytes in the free pool.","notes_text":"Minimum free block size depends on list type (explicit lists need next/prev pointers).","keywords":["splitting","place","required_size","minimum block size","internal fragmentation","header/footer"],"images":[],"layout":{"num_text_boxes":7,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Splitting During Placement","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":29,"chunk_index":0,"title":"Splitting in MallocLab: place() Stub","summary":"Shows the `place` helper skeleton that allocates a block and optionally splits it.","main_text":"The slide provides the MallocLab stub for `static BlockHeader* place(BlockHeader *bp, int size)`. This helper takes a free block `bp` found by `find_fit` and a required block size `size`. It marks `bp` allocated, writes correct header/footer tags, and if `bp` is larger than `size` by at least the minimum free-block size, splits the block: the first part stays allocated, and the remainder becomes a new free block inserted into the free structure. The function then returns a pointer to the allocated block’s header (or payload, depending on your macro convention). Correct splitting logic is essential for utilization and for later coalescing correctness.","notes_text":"Remember to update free-list pointers if you remove/split a free node.","keywords":["place","MallocLab","splitting logic","allocated bit","free remainder","block tags"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"place()","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":30,"chunk_index":0,"title":"Putting It Together: mm_malloc() Flow","summary":"Presents the overall allocator pipeline used in `mm_malloc`.","main_text":"`mm_malloc(size_t size)` is the top-level allocation routine. First, it ignores zero-byte requests. Next, it computes `required_size` by rounding the payload up for alignment and adding allocator overhead (headers/footers). It then calls `find_fit(required_size)` to locate a suitable free block. If a fit is found, `place` is called to mark the block allocated and split if needed, and `mm_malloc` returns a pointer to the payload. If no fit exists, `mm_malloc` extends the heap via `extend_heap`, coalesces the new space with neighbors, then calls `place` on the resulting free block. This sequence connects all prior components—size computation, placement policy, splitting, coalescing, and heap extension—into a working allocator.","notes_text":"This is the core you implement for MallocLab.","keywords":["mm_malloc","required_size","find_fit","extend_heap","place","allocation pipeline"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"mm_malloc Flow","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":31,"chunk_index":0,"title":"Explicit Free Lists (Structure)","summary":"Introduces explicit free lists and the metadata stored inside free blocks.","main_text":"Explicit free lists track only free blocks in a dedicated linked list. When a block is free, the allocator uses some of its payload space to store pointers to other free blocks—commonly `prev` and `next` in a doubly linked list. Allocated blocks omit these pointers and contain only user payload. Because searches traverse only free blocks, `find_fit` becomes faster than with implicit lists that must scan both allocated and free blocks. The cost is extra internal fragmentation for small blocks (free blocks must be big enough to hold the pointers) and more complex list maintenance: upon allocation, the block must be removed from the free list; upon freeing, it must be inserted back, possibly after coalescing.","notes_text":"Ordering can be by LIFO, address, or size; each affects fragmentation/perf.","keywords":["explicit free list","free block pointers","doubly linked list","find_fit speedup","minimum block size","metadata overhead"],"images":[],"layout":{"num_text_boxes":7,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Explicit Free Lists","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":32,"chunk_index":0,"title":"Explicit Free Lists: Example and Ordering","summary":"Walks through how frees update the explicit list and discusses ordering choices.","main_text":"The example traces a sequence of allocations and frees to show how explicit free lists evolve. When blocks are freed, they are inserted into the free list and may later be coalesced with adjacent free blocks, requiring removal of the old nodes and insertion of the merged node. If coalescing is deferred, the free list can contain multiple free blocks in a time-based order rather than address order (e.g., blocks 3, 1, 4). The slide notes two alternative policies: maintaining the free list **sorted by address**, which simplifies coalescing and can reduce external fragmentation, or **sorted by size**, which approximates best-fit and can improve utilization. Each ordering affects throughput and fragmentation differently.","notes_text":"Key skill: correct prev/next pointer updates under all cases.","keywords":["explicit list example","free insertion","remove on alloc","list ordering","address-ordered","size-ordered"],"images":[],"layout":{"num_text_boxes":10,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":12,"topic":"Explicit Free List Example","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":33,"chunk_index":0,"title":"Explicit Free Lists: Exercise","summary":"Exercise prompt to practice explicit free list updates and coalescing.","main_text":"This slide is an exercise checkpoint. You are expected to apply the explicit-free-list rules to a short allocation/free trace: remove allocated blocks from the free list, insert freed blocks, and coalesce adjacent free blocks when required. The purpose is to reinforce the invariant that the free list must remain well-formed and that coalescing must update both heap tags and list pointers consistently.","notes_text":"Work the trace and verify list invariants after each step.","keywords":["exercise","explicit free list","coalescing practice","list invariant","pointer updates"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Explicit Free List Exercise","importance_score":6,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":34,"chunk_index":0,"title":"Segregated Free Lists Overview","summary":"Introduces multiple free lists by size class and names segregated storage and segregated fit.","main_text":"Segregated free lists speed up allocation by keeping **separate free lists for different block sizes**. When malloc requests `size`, the allocator maps the request to a size class and searches only that list first; if empty, it searches progressively larger classes. This reduces search time and tends to match requests to closer-sized blocks, improving utilization compared to a single global list. Two main variants are presented: **segregated storage**, which uses fixed-size blocks per class and removes headers/coalescing, and **segregated fit**, which keeps variable-size blocks with boundary tags, splitting, and coalescing. The rest of the section expands each variant.","notes_text":"Think “bins by size.”","keywords":["segregated free lists","size classes","bins","segregated storage","segregated fit","fast search"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Segregated Free Lists","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":35,"chunk_index":0,"title":"Segregated Storage (Mechanism)","summary":"Describes fixed-size class lists and how allocation and free work.","main_text":"Segregated storage splits the heap into multiple free lists of **fixed-size blocks**, such as 16-byte, 32-byte, 64-byte, and 4KB classes. For an allocation, the allocator chooses the smallest block size that can hold the requested payload and returns one block from that class list. If that list is empty, it extends the heap, partitions the new region into blocks of that size, and pushes them into the class list. Freeing is trivial: return the block to its corresponding class list, not necessarily in address order. Because all blocks in a class have identical size, headers/footers and coalescing are unnecessary, keeping metadata tiny and operations fast. The tradeoff is internal fragmentation when requests don’t align well with class sizes.","notes_text":"Very common for small-object allocators and slab allocators.","keywords":["segregated storage","fixed-size blocks","class lists","no coalescing","internal fragmentation","fast malloc/free"],"images":[],"layout":{"num_text_boxes":9,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Segregated Storage","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":36,"chunk_index":0,"title":"Segregated Storage: Properties and Overlay","summary":"Lists key properties and shows how a free block overlays metadata onto payload space.","main_text":"Because distributed class lists use fixed block sizes, segregated storage can avoid boundary tags entirely. A free block is overlaid with a small struct that stores only the **next free pointer**; an allocated block uses the same bytes as payload. This allows singly linked lists, constant-time push/pop in each class, and very low overhead. The minimum block size equals one pointer, since a free block must store at least its `next` link. However, the simplicity comes at a cost: rounded-up allocations can waste space inside allocated blocks (internal fragmentation) and blocks in different classes can’t be coalesced. The overlay figure for a 16-byte class highlights the dual use: (free) `next + padding` versus (allocated) `payload`.","notes_text":"Overlay is why headers aren’t needed here.","keywords":["overlay","next pointer","fixed-size class","no headers/footers","minimum block size","internal fragmentation"],"images":[],"layout":{"num_text_boxes":8,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":12,"topic":"Segregated Storage Properties","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":37,"chunk_index":0,"title":"Segregated Storage: Example","summary":"Example trace demonstrating class selection, reuse, and simple frees.","main_text":"This example shows a heap managed with segregated storage and multiple fixed-size free lists. A sequence of allocations maps each request to the smallest sufficient class, removes a block from that class list, and uses it as payload. When blocks are freed, they are returned to the correct class list (often LIFO), enabling quick reuse on later allocations of similar size. The example reinforces two crucial behaviors: (1) lists are independent, so allocation searches are short and skip irrelevant sizes, and (2) because blocks are fixed size, there is no splitting or coalescing—free operations are constant-time list insertions.","notes_text":"Focus on how internal fragmentation appears in fixed class systems.","keywords":["segregated storage example","size class mapping","constant-time free","reuse","no splitting","no coalescing"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Segregated Storage Example","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":38,"chunk_index":0,"title":"Segregated Fit (Idea + First Example)","summary":"Introduces variable-size size classes with boundary tags and a small worked allocation example.","main_text":"Segregated fit keeps **multiple free lists by size range**, but unlike segregated storage, blocks within each class are variable sized and can be split/coalesced. This requires boundary tags (header/footer). Each list covers an interval such as [16–31], [32–63], and ≥64 bytes, and a free block belongs to the smallest interval that can contain its size. On malloc, the allocator searches the list matching the request first; if empty, it checks larger lists. When it finds a fit, it may split off a remainder and place that remainder back into the correct smaller class. The small example on the slide shows an initial state of size-class lists, then how a `malloc(32)` request consumes from a larger class and leaves a fragment inserted into a smaller class.","notes_text":"This approximates best-fit while avoiding tiny blocks.","keywords":["segregated fit","variable-size blocks","size intervals","boundary tags","split remainder","best-fit approximation"],"images":[],"layout":{"num_text_boxes":7,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":12,"topic":"Segregated Fit Intro","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":39,"chunk_index":0,"title":"Segregated Fit (Algorithm Details)","summary":"Explains initialization, cross-class searching, splitting, and coalescing on free.","main_text":"At initialization, segregated fit may start with one large free block in the largest size class. As allocations proceed, blocks split and leftover fragments move down into smaller size classes. If the requested size class has no fitting block, the allocator searches larger classes in increasing order until a fit is found. If even the largest class is empty, the heap is extended. On free, the allocator coalesces the freed block with adjacent free neighbors (removing those neighbors from their current lists) and inserts the merged block into the size class matching its new size. This design improves throughput by shrinking search spaces while preserving utilization via splitting and coalescing.","notes_text":"Correctly moving blocks between classes is essential.","keywords":["segregated fit algorithm","search larger classes","heap extension","coalescing","class insertion","splitting"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Segregated Fit Algorithm","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":40,"chunk_index":0,"title":"Segregated Fit: Extended Example","summary":"Step-by-step example showing searches across classes, splitting, and coalescing.","main_text":"The extended example traces segregated fit with several size-class free lists (e.g., free16, free32, free4k). For an allocation like `ptr1 = malloc(48)`, the allocator first searches the 32-class list and finds it empty, then moves to the next larger class. It allocates from an 80-byte free block, splits the block, and inserts the leftover fragment into the appropriate smaller class. Later, when a block such as `a2` is freed, the allocator coalesces it with neighboring free blocks, removes those neighbors from their lists, forms a larger combined free block, and inserts that combined block into the size class matching its new size. The “before/after malloc” and “before/after free(a2)” states illustrate how blocks migrate between classes and how segregated fit approximates best-fit without scanning tiny blocks.","notes_text":"Use this to sanity-check your own segregated fit logic.","keywords":["segregated fit example","class search","split block","insert fragment","coalesce neighbors","move between lists"],"images":[],"layout":{"num_text_boxes":10,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":12,"topic":"Segregated Fit Example","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":41,"chunk_index":0,"title":"Garbage Collection (Section Header)","summary":"Transitions from explicit allocators to garbage collection.","main_text":"This slide marks the start of the garbage collection part of the unit. The focus shifts from manual allocation policies (implicit/explicit/segregated lists) to automatic memory reclamation strategies used by modern runtimes. The upcoming slides introduce GC motivations, main approaches, and performance tradeoffs.","notes_text":"","keywords":["garbage collection","section transition","automatic deallocation"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Garbage Collection Header","importance_score":6,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":42,"chunk_index":0,"title":"GC Approaches Overview","summary":"Motivates GC and identifies reference counting and tracing as two major families.","main_text":"Manual memory management is difficult and error-prone: programmers must free everything exactly once, or risk dangling pointers and memory leaks. Tools like Valgrind can detect leaks, but many languages adopt automated garbage collection. The slide highlights two broad GC approaches. **Reference counting** (including smart pointers) keeps a per-object count of active references and frees objects when counts hit zero. **Object graph analysis** (tracing GC) periodically traverses pointers from program roots to identify reachable objects and reclaims the rest. These two families trade incremental overhead (reference counting updates on every pointer change) against periodic global overhead and pauses (tracing).","notes_text":"Sets up the next deep-dive slides.","keywords":["GC approaches","valgrind","reference counting","smart pointers","tracing GC","object graph"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"GC Approaches","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":43,"chunk_index":0,"title":"Reference Counting Example","summary":"Shows how refcounts change with aliasing and deletion, and notes cycles as the key limitation.","main_text":"The Python REPL example demonstrates reference counting in practice. Creating `x = object()` establishes a baseline reference count. Assigning `y = x` creates a new alias, incrementing the count; deleting `y` decrements it. When an object’s count reaches zero, the runtime reclaims it immediately, which can cascade to other objects whose counts drop as a result. Reference counting provides prompt reclamation and tends to keep pauses small. However, the major limitation is **reference cycles**: if objects reference each other in a closed loop, their counts never fall to zero, even when the cycle is unreachable from roots. Languages using reference counting often add an extra tracing phase to collect cyclic garbage.","notes_text":"Python uses RC + periodic cycle detection.","keywords":["reference counting","Python example","getrefcount","aliasing","immediate reclamation","cycles"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Reference Counting","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":44,"chunk_index":0,"title":"Tracing Garbage Collectors (Concept)","summary":"Defines tracing GC, roots, reachability, and stop-the-world behavior.","main_text":"Tracing garbage collectors reclaim memory based on reachability. When heap memory becomes scarce, the runtime identifies a set of **roots**: global variables, static class members, and local variables in active stack frames. Starting from these roots, it traverses the object graph by following pointers. Any object reached is considered live and kept; anything not reached is garbage and can be deallocated, including cyclic structures. A central drawback is that tracing typically requires pausing the program during traversal, a **stop-the-world (STW)** event. These pauses can be fast in modern collectors but occur at unpredictable times, affecting latency.","notes_text":"Used by Java, Go, JavaScript, etc.","keywords":["tracing GC","roots","reachability","object graph traversal","cycles collected","stop-the-world"],"images":[],"layout":{"num_text_boxes":6,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Tracing GC Concept","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":45,"chunk_index":0,"title":"Tracing GC: Mark and Sweep","summary":"Explains the mark phase, sweep phase, and why fragmentation/scan cost arise.","main_text":"Mark-and-sweep GC runs in two phases. **Mark phase:** traverse from roots (often depth-first) and set a “reachable” mark bit in each visited object header. **Sweep phase:** scan all heap objects linearly; any object without its mark bit set is unreachable and is freed, while reachable objects have their mark bits cleared for the next cycle. This approach collects cyclic garbage because it uses reachability rather than counts. Its costs are (1) sweeping touches every object even if most are garbage, and (2) freeing objects in place can leave holes, causing fragmentation. To mitigate fragmentation, some implementations add a compacting step that relocates reachable objects together.","notes_text":"Contrast with stop-and-copy on next slide.","keywords":["mark and sweep","mark bit","sweep scan","reachability","fragmentation","compaction"],"images":[],"layout":{"num_text_boxes":12,"num_images":0,"dominant_visual_type":"flowchart"},"metadata":{"course":"CS356","unit":12,"topic":"Mark and Sweep GC","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":46,"chunk_index":0,"title":"Tracing GC: Stop and Copy","summary":"Describes stop-and-copy collection to avoid sweep overhead and reduce fragmentation.","main_text":"Stop-and-copy GC divides the heap into two equal regions. The program allocates only in the active region. When GC runs, the runtime stops the program and traverses from roots; each reachable object is **copied** into the inactive region, leaving them packed contiguously. After traversal, the inactive region becomes the new active heap, and the old active region is considered entirely free—unreachable objects were never copied, so they are reclaimed implicitly. This eliminates the sweep scan and compacts memory automatically, preventing fragmentation. The drawback is that long-lived objects may be copied repeatedly each cycle, creating overhead for persistent data.","notes_text":"Motivates generational GC next.","keywords":["stop and copy","two-space heap","copy reachable objects","implicit sweep","compaction","long-lived overhead"],"images":[],"layout":{"num_text_boxes":11,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Stop and Copy GC","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":47,"chunk_index":0,"title":"Generational GC and Heap Regions","summary":"Introduces the weak generational hypothesis and Eden/Survivor/Tenured regions.","main_text":"Generational GC relies on the **weak generational hypothesis**: most objects die young. The heap is partitioned into regions. New objects are allocated in **Eden**. When Eden fills, a young-generation collection copies live Eden objects into one of two **Survivor** spaces (S0 or S1). On subsequent young-gen collections, live objects are copied between S0 and S1, and a survival counter increments. Objects that survive enough cycles are **promoted** to the **Tenured (Old Gen)** region, which is collected less frequently and may use compaction to avoid fragmentation. By collecting young objects often and old objects rarely, generational GC greatly reduces the repeated copying cost of long-lived objects while keeping most collections fast.","notes_text":"This is the standard design in Java-like runtimes.","keywords":["generational GC","weak generational hypothesis","Eden","Survivor spaces","tenured/old gen","promotion"],"images":[],"layout":{"num_text_boxes":12,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Generational GC","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":48,"chunk_index":0,"title":"Visual GC in Java","summary":"Visual illustration reinforcing object movement across Eden, Survivors, and Tenured.","main_text":"This slide provides a visual summary of Java’s generational garbage collection. It depicts allocations in Eden, copying of surviving objects into Survivor spaces during minor GCs, and promotion of older survivors into Tenured after multiple cycles. The illustration emphasizes how Java avoids scanning or copying long-lived objects too frequently and why young-gen collections are usually short but happen often.","notes_text":"Diagram-only reinforcement slide.","keywords":["Java GC","Eden to Survivor","minor GC","promotion","tenured region","diagram"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":12,"topic":"Java GC Visualization","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":49,"chunk_index":0,"title":"GC Variations: Parallel and Concurrent","summary":"Summarizes parallel tracing and concurrent mark-and-sweep to reduce pauses.","main_text":"Two practical tracing-GC variations reduce stop-the-world time. **Parallel GC** uses multiple threads to traverse and copy/mark objects (especially in Eden and Survivor spaces), speeding up collections on multicore CPUs. **Concurrent mark-and-sweep** performs some marking and sweeping while the program continues running, improving responsiveness. However, concurrent collectors must monitor mutations to already-visited objects, because these changes can make previously unreachable objects reachable again. Even concurrent schemes retain some STW steps, such as when Eden fills or when Tenured promotion/compaction is required. The net effect is lower average pause time and better latency at the cost of more complex runtime bookkeeping.","notes_text":"Modern low-pause collectors combine both ideas.","keywords":["parallel GC","concurrent GC","concurrent mark and sweep","latency","STW steps","runtime bookkeeping"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"GC Variations","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit12_HeapManagement","slide_number":50,"chunk_index":0,"title":"Memory Leaks with GC","summary":"Explains why logical leaks still happen under garbage collection.","main_text":"Garbage collection prevents many low-level memory errors, but it cannot stop **logical memory leaks**. A GC frees only objects that are unreachable from roots. If the program retains references to objects it no longer needs—such as entries left in global caches, long-lived lists, or closures attached to event handlers—those objects remain reachable and will not be collected. Over time, this increases the live set and can exhaust memory even in a garbage-collected language. Preventing these leaks requires correct application-level ownership and lifecycle design, not just a GC runtime.","notes_text":"Key takeaway: reachability ≠ usefulness.","keywords":["GC memory leaks","logical leak","reachable objects","roots","caches","lifecycle management"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":12,"topic":"Memory Leaks with GC","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":1,"chunk_index":0,"title":"Unit 13: Linking — Binary Executables & Libraries","summary":"Introduces Unit 13 and frames linking as the step that produces final executables and libraries from compiled modules.","main_text":"This opening slide names the unit topic: Linking. It sets the scope around how binary executables and libraries are produced from separately compiled code. The unit will focus on why linking is required after compilation, what information object files contain for linking, and how final runnable binaries are built. The slide signals that students should think of linking as part of the standard build pipeline that turns modular source code into a complete program.","notes_text":"","keywords":["Unit 13","linking","executables","libraries","object files","binary programs"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Linking Overview","importance_score":6,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":2,"chunk_index":0,"title":"Binary Programs & Linking","summary":"Motivates linking by introducing binary programs and the need to combine compiled modules.","main_text":"This slide introduces binary programs and sets up linking as the mechanism that combines independently compiled pieces into a final executable or library. It emphasizes that real programs are not compiled as a single file; they are split into modules that must later be unified. Linking is the stage that connects those parts into one coherent binary image that can run.","notes_text":"","keywords":["binary programs","linking motivation","modules","executables","toolchain"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Motivation for Linking","importance_score":6,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":3,"chunk_index":0,"title":"Compilation Process","summary":"Shows the full pipeline from high-level source to assembly, object code, executable, and loading.","main_text":"The slide lays out the compilation pipeline. High-level source (.c/.cpp) is preprocessed and compiled into assembly (.s/.asm), assembled into relocatable object/machine code (.o), then linked into an executable binary image, and finally loaded by the OS for execution. It also illustrates that tools like gcc/clang wrap multiple steps: preprocessing (cpp/cc1), assembling (as), linking (ld), and loading/runtime support. The slide includes example source code, corresponding assembly, and binary-looking instruction bytes to show how representations evolve from human-readable to machine-executable.","notes_text":"Main takeaway: compilation yields .o files, but linking is required to create a runnable executable.","keywords":["compilation pipeline","assembly",".s",".o","linker","loader","gcc","clang"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":13,"topic":"Compilation Pipeline","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":4,"chunk_index":0,"title":"Why Many .o Files? Modularity","summary":"Explains that splitting programs into multiple object files enables modular, reusable code and independent recompilation.","main_text":"This slide states the advantage of using many .o files: modular and reusable code, often packaged as libraries. Each module/compilation unit can be recompiled independently, improving build speed and software organization. The slide reinforces how separate compilation interacts with linking: modules are built separately, then linked together into a final binary.","notes_text":"","keywords":["modularity","separate compilation","object files",".o","libraries","reusability"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Separate Compilation","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":5,"chunk_index":0,"title":"Example: Toolchain Stages","summary":"Walks through a concrete example of preprocessing, compiling, assembling, and the role of linking.","main_text":"This example slide shows how a real build proceeds through toolchain stages to produce an object file. It illustrates that compilation outputs binary .o code but not a full program. The example reinforces the existence of distinct stages (preprocess → compile → assemble → link) and the artifacts produced at each step.","notes_text":"","keywords":["toolchain example","preprocess","compile","assemble","object file","link stage"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":13,"topic":"Compilation Example","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":6,"chunk_index":0,"title":"Example: Compilation Units","summary":"Defines compilation units and shows how multiple source files become separate .o files.","main_text":"This slide presents an example of compilation units: each .c file (plus headers) is compiled to its own .o. It emphasizes separate compilation: files are compiled independently, producing multiple object files that later require linking. The goal is independence during compile-time with integration at link-time.","notes_text":"","keywords":["compilation units","separate compilation","multiple .o files","headers","modular build"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":13,"topic":"Compilation Units","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":7,"chunk_index":0,"title":"callq sum Placeholder","summary":"Shows that external calls compile to machine-code placeholders before linking.","main_text":"The slide focuses on a `callq sum` instruction whose machine-code encoding contains a placeholder displacement (e.g., e8 00 00 00 00). Because sum is defined in another compilation unit, the compiler/assembler cannot fill in the true target offset. The linker is responsible for patching this field later during relocation.","notes_text":"","keywords":["callq","external function","placeholder","machine code","relocation site","linker patch"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":13,"topic":"Need for Relocation","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":8,"chunk_index":0,"title":"Unknown External Addresses in .o Files","summary":"States that object files don’t know final addresses of external functions/globals, leaving zeros for the linker.","main_text":"This slide explains that in relocatable object files, addresses of external functions or global variables are not yet known. As a result, the .o file contains blanks or zero offsets where those references will go. These are tracked in relocation information so the linker can fill them after it chooses final layout.","notes_text":"","keywords":["relocatable object",".o file","external symbols","unknown addresses","relocation"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Object Files Before Linking","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":9,"chunk_index":0,"title":"RIP-Relative Offset After Linking","summary":"Computes a real PC-relative call displacement once addresses are assigned by the linker.","main_text":"The slide shows how a final call displacement is computed after linking: address(sum) minus RIP at the call yields a signed offset (example values shown such as 0x401106 − 0x401137 = 0xffffffcf). This demonstrates that x86-64 uses PC/RIP-relative addressing, so the linker patches offsets rather than absolute pointers.","notes_text":"","keywords":["RIP-relative","PC-relative call","displacement","link-time addresses","relocation math"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"PC-Relative Relocation Example","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":10,"chunk_index":0,"title":"Linking Tasks","summary":"Defines the two fundamental linker jobs: symbol resolution and relocation.","main_text":"This slide states the two main tasks of linking. **Symbol resolution** matches every symbol reference to exactly one definition across all input object files and libraries. **Relocation** merges code/data sections, assigns final addresses, and patches machine code/data to reflect those addresses. Together these tasks convert multiple .o files into a runnable executable.","notes_text":"","keywords":["linker tasks","symbol resolution","relocation","section merging","address assignment"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Symbol Resolution and Relocation","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":11,"chunk_index":0,"title":"Object Files (.o) Contents","summary":"Explains what .o files must provide to enable separate compilation and linking.","main_text":"Object files must provide enough information for separate compilation to work. Each .o includes machine code, data, a symbol table describing exported/needed symbols, and relocation entries marking where external addresses must be patched. It may also include bookkeeping sections (debugging, metadata) that assist a linker or loader. The slide emphasizes that the .o format is structured to support modular builds.","notes_text":"","keywords":["object file contents","symbol table","relocation table",".text",".data",".bss","separate compilation"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Object File Structure","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":12,"chunk_index":0,"title":"ELF File Types","summary":"Introduces ELF as the Linux format for .o files, shared libraries, and executables.","main_text":"ELF (Executable and Linkable Format) is the structured Linux file format used for all key binary products: relocatable object files (.o), shared libraries (.so), and executables (e.g., prog). The slide also contrasts relocatable vs executable ELF: executables don’t need relocation sections like .rel.text or .rel.data because they are already fully relocated. ELF provides consistent structure so linkers and loaders can interpret code, data, symbols, and runtime metadata.","notes_text":"","keywords":["ELF","Executable and Linkable Format",".o",".so","executables","Linux binaries"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"ELF Overview","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":13,"chunk_index":0,"title":"Symbol Types and extern Declarations","summary":"Shows extern function/global declarations and classifies symbol types used during linking.","main_text":"The slide gives extern examples such as `int sum(int *a, int n); // extern` and `extern int sum_error;` to show cross-file declarations. It then defines symbol categories relevant to the linker: **global symbols** (functions/globals defined in the module and visible externally), **external/undefined symbols** (referenced here but defined elsewhere), and **local symbols** (private to the module). Automatic local variables live on the stack and are not handled by the linker.","notes_text":"","keywords":["extern","symbol types","global symbols","undefined symbols","local symbols","headers","cross-file references"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Symbol Types","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":14,"chunk_index":0,"title":"Symbol Resolution Process","summary":"Describes how the linker scans .o files and libraries to resolve symbols.","main_text":"This slide explains symbol resolution: the linker checks input .o files and libraries in order, building a global view of defined and undefined symbols. When it sees a definition, it records it; when it sees a reference without a definition, it adds it to undefined symbols to be resolved later. The slide uses the main.c / sum.c example and gcc commands to illustrate that undefined calls in main.o are matched to definitions in sum.o during linking.","notes_text":"","keywords":["symbol resolution","link order","undefined symbols","global symbol table","main.o","sum.o"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Symbol Resolution","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":15,"chunk_index":0,"title":"Duplicate Symbols and Weak/Strong Rules","summary":"Explains older GCC behavior with duplicate globals and weak vs strong symbol resolution.","main_text":"Older compilers (GCC < 10) allowed duplicate global symbols using weak/strong rules. The slide contrasts **weak** definitions (e.g., `int x;`) and **strong** definitions (e.g., `int x = 1;` or `int x = 0;`). Resolution rules: if one strong and multiple weak exist, the strong one wins; if multiple strong exist, it is an error; if only weak exist, they resolve to one shared variable. This slide highlights when duplicates resolve vs when linking fails.","notes_text":"","keywords":["duplicate symbols","weak symbols","strong symbols","GCC <10","multiple definition error","global variables"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"comparison"},"metadata":{"course":"CS356","unit":13,"topic":"Weak/Strong Symbols","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":16,"chunk_index":0,"title":"Resolution Example (res2)","summary":"Shows a concrete linking example that triggers a multiple definition/conflict error.","main_text":"This code example (res2a.c / res2b.c) demonstrates symbol resolution outcomes in practice. When compiled and linked together (gcc res2a.c res2b.c -o res2), the linker reports an error, illustrating what happens when symbol definitions conflict under the rules. The slide reinforces that symbol resolution is enforced at link time.","notes_text":"","keywords":["resolution example","linker error","multiple definitions","gcc link","res2"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":13,"topic":"Resolution Example","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":17,"chunk_index":0,"title":"Resolution Example (res3)","summary":"Another conflict example showing how undefined/duplicate global symbols lead to link errors.","main_text":"This second example (res3a.c / res3b.c) again compiles and links two modules together, producing a linker error. The slide’s purpose is to make the weak/strong and duplicate-definition rules feel concrete: certain combinations of globals produce valid bindings; others lead to immediate link-time failure.","notes_text":"","keywords":["resolution example","res3","link-time error","duplicate globals","symbol conflict"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":13,"topic":"Resolution Example","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":18,"chunk_index":0,"title":"Global Variables Summary","summary":"Gives best-practice advice about using globals and extern declarations safely.","main_text":"The slide summarizes guidance on global variables. It discourages unnecessary globals when alternatives exist. If globals must be shared, use correct `extern` declarations and ensure consistent types across modules. The best practice is to declare shared globals/functions in a header provided by the defining module so every compilation unit sees the same interface and types.","notes_text":"","keywords":["global variables","extern","headers","best practices","type consistency","linking safety"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Globals and extern","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":19,"chunk_index":0,"title":"Wrong Extern Types","summary":"Shows that mismatched extern types across files cause warnings/errors and unsafe linkage.","main_text":"This slide demonstrates a common bug: different compilation units declare the same extern symbol with different types (example shown with swap signature and short/int mismatch). Compiling with strict flags (e.g., -Wall -Wextra -std=c99 -fcommon) surfaces these inconsistencies. The mismatch can lead to wrong code generation or runtime bugs even if linking succeeds, so extern types must match exactly.","notes_text":"","keywords":["wrong extern types","type mismatch","linker warnings","swap example","C headers"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":13,"topic":"Extern Type Mismatch","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":20,"chunk_index":0,"title":"Solution: Compile with -flto","summary":"Shows link-time optimization as a solution to catching cross-module type issues.","main_text":"The slide presents a fix for cross-file inconsistencies: compile and link with **-flto** (link-time optimization). With LTO enabled, the linker has access to richer type/IR information across modules, allowing it to detect mismatches and optimize globally. The example uses the same swap/global mismatch scenario to show that LTO helps catch or prevent the issue.","notes_text":"","keywords":["-flto","link-time optimization","LTO","cross-module checks","type safety"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Link-Time Optimization","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":21,"chunk_index":0,"title":"Solution: Use a Header + gcc Example","summary":"Demonstrates correct compilation using shared headers for extern declarations.","main_text":"The slide gives a gcc command example compiling multiple files together and shows a correct header (e.g., swap.h) guarded by include guards. The message is that extern declarations should live in a shared header so all modules agree on types. Compiling main.c and swap.c together with correct includes yields safe, consistent linking.","notes_text":"","keywords":["headers","include guards","swap.h","gcc command","extern consistency"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":13,"topic":"Header-Based Interfaces","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":22,"chunk_index":0,"title":"Relocation (Concept + Example)","summary":"Introduces relocation and shows how linker produces an executable by patching placeholder references.","main_text":"This slide introduces relocation: when the linker runs, it creates an executable by combining input object files (main.o and sum.o) and patching all unresolved references. It illustrates the before/after idea using a small executable object view: code from main.o and sum.o is merged, and call targets/addresses are rewritten to match final placement. Relocation is required because compilation assumed unknown external addresses.","notes_text":"","keywords":["relocation","linker patching","main.o","sum.o","executable object","address fixups"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":13,"topic":"Relocation","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":23,"chunk_index":0,"title":"Relocation and Memory Layout","summary":"Explains that each output executable section has a run-time address and is built from input sections.","main_text":"The slide explains that each section in the executable (.text, .data, etc.) receives a run-time memory address. The linker merges like sections from object files into final executable sections (system code in .text, system data in .data). Relocation adjusts section-relative references so that they point to the correct final addresses in this merged layout.","notes_text":"","keywords":["executable sections","run-time address",".text",".data","section merging","relocation"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Relocation Layout","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":24,"chunk_index":0,"title":"Relocation Review: Placeholders","summary":"Reviews that object files leave relocation sites for global symbols and shows code with unresolved references.","main_text":"This review slide restates that object files leave links/placeholders to global symbols. It shows an example implementation of sum with globals and makes clear that references to external functions or variables are not final until relocation. The linker later fixes those placeholder fields based on final addresses.","notes_text":"","keywords":["relocation review","placeholders","global symbols","sum example","object file fixups"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":13,"topic":"Relocation Review","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":25,"chunk_index":0,"title":"Relocation Review: Relocation Entries","summary":"Shows actual relocation entries and how they reference symbols and patch locations.","main_text":"This slide shows relocation entries produced by the compiler/assembler. Each relocation lists a location to patch and a symbol it depends on (example entry shown like R_X86_64_PC32 sum - 4). The point is that relocation is data-driven: the linker reads a relocation table and applies the specified patches to emit a correct executable.","notes_text":"","keywords":["relocation entries","R_X86_64_PC32","symbol index","patch location","ELF relocation"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":13,"topic":"Relocation Entries","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":26,"chunk_index":0,"title":"Static Libraries","summary":"Introduces static libraries as archives of object files used during linking.","main_text":"This slide introduces static libraries. A static library is a packaged collection of .o files that can be linked into executables. Instead of linking many .o files manually, developers bundle reusable code into a library and link against it as needed.","notes_text":"","keywords":["static libraries",".a","archives","reusable code","linking"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Static Libraries","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":27,"chunk_index":0,"title":"The Need for Libraries","summary":"Explains why reusable modules are packaged as libraries rather than copied into many programs.","main_text":"This slide motivates libraries: many programs need the same functions, so shared reusable code should live in libraries rather than being duplicated across projects. Libraries support modular development and reduce code repetition and maintenance burden.","notes_text":"","keywords":["need for libraries","code reuse","modularity","avoid duplication","shared code"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Library Motivation","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":28,"chunk_index":0,"title":"Benefits of a Library","summary":"Lists practical advantages of libraries for maintainability and distribution.","main_text":"This slide summarizes benefits of using libraries: code reuse, simpler builds, improved maintainability, and clearer separation between application logic and reusable modules. Updating a library improves all dependent programs without rewriting each one.","notes_text":"","keywords":["library benefits","maintainability","reuse","modular build","distribution"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Benefits of Libraries","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":29,"chunk_index":0,"title":"Static Libraries on Linux: ar and lib Naming","summary":"Explains Linux static library creation/search using ar and naming conventions like libX.a.","main_text":"The slide explains how static libraries work on Linux. The `ar` archiver packs multiple .o files into a single **libX.a** file. The linker searches libraries using naming conventions, assuming a `lib` prefix and `.a` suffix. Example objects like io.o are bundled into the archive. This provides a standard way for ld/gcc to locate and include static code.","notes_text":"","keywords":["static libraries Linux","ar","libX.a","archiver","library naming","io.o"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Linux Static Libraries","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":30,"chunk_index":0,"title":"Static Libraries on Linux: gcc Flags","summary":"Shows gcc include/library flags (-I, -L, -l) used when linking static libraries.","main_text":"This slide demonstrates how to compile and link with static libraries using gcc flags. `-I` adds header include paths, `-L` adds library search paths, and `-lname` links against libname.a (or libname.so if dynamic). A sample command line is shown combining multiple -I and -L options and linking several libraries to produce the program.","notes_text":"","keywords":["gcc flags","-I include path","-L library path","-l library","static linking","command line"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":13,"topic":"Linking with Static Libraries","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":31,"chunk_index":0,"title":"Library Example: Multiple Modules","summary":"Provides a multi-file example showing how functions/objects get organized into a library plus apps.","main_text":"The slide shows a worked library example with several modules (f1a.c, f1b.c, f2.c) and an application (app1.c) using a shared header f.h. Example functions like f11/f12 manipulate a global x and return values. The purpose is to illustrate how a reusable library is structured across multiple .c/.o files and linked into an app.","notes_text":"","keywords":["library example","multiple modules","f1a.c","f1b.c","f2.c","app1.c","header f.h"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":13,"topic":"Library Example","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":32,"chunk_index":0,"title":"Exercise: Build libf.a","summary":"Exercise slide asking students to compile objects and archive them into a static library.","main_text":"This exercise references building a static library named libf.a from module object files and using f.h. Students are expected to compile constituent .c files to .o files, archive them with ar, and link an application against the resulting library.","notes_text":"","keywords":["exercise","libf.a","ar","static library build","f.h","gcc"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Static Library Exercise","importance_score":6,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":33,"chunk_index":0,"title":"Static Library Issues","summary":"Explains drawbacks of static libraries and motivates shared/dynamic libraries.","main_text":"This slide asks what happens if a static library must be changed. Because static libraries are copied into executables, every dependent application must be re-linked (and redistributed) to pick up fixes. The slide motivates a better approach: shared libraries and dynamic linking, which allow updates without rebuilding every program.","notes_text":"","keywords":["static library issues","re-linking cost","update problem","motivation for dynamic linking","shared libraries"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Static Library Drawbacks","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":34,"chunk_index":0,"title":"Dynamic Libraries","summary":"Introduces dynamic/shared libraries and dynamic linking as an alternative to static libraries.","main_text":"This slide introduces dynamic libraries (shared libraries) as the solution to static library update and duplication issues. Instead of copying code into each executable, dynamic linking allows programs to reference library code that is loaded at run time.","notes_text":"","keywords":["dynamic libraries","shared libraries","dynamic linking","runtime loading","vs static"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Dynamic Libraries Intro","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":35,"chunk_index":0,"title":"Motivation for Dynamic Linking","summary":"Explains why dynamic linking avoids memory duplication and enables flexible runtime resolution.","main_text":"This motivation slide notes that static linking duplicates common code in physical memory across processes. Dynamic linking avoids that duplication by allowing multiple processes to share one physical copy of a library’s code pages. The slide also says dynamic linking enables applications to search for and use a library’s data or code at run time, using whatever address is found then. This provides flexibility and lower memory footprint.","notes_text":"","keywords":["dynamic linking motivation","avoid duplication","shared code pages","runtime resolution","memory savings"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":13,"topic":"Dynamic Linking Motivation","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":36,"chunk_index":0,"title":"Shared Libraries in Memory","summary":"Illustrates how two processes map shared library code to the same physical memory.","main_text":"The slide depicts two program address spaces (Prog. 1 and Prog. 2) showing that their code (.text) regions for a shared library can map to identical physical pages, while each retains private data regions. Example virtual/physical addresses are shown to reinforce that shared library code is read-only and shareable, whereas data is per-process. This explains why shared libraries reduce overall memory use.","notes_text":"","keywords":["shared libraries","address space","shared .text","private data","memory mapping","processes"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":13,"topic":"Shared Library Mapping","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":37,"chunk_index":0,"title":"Simplified View of Dynamic Linking","summary":"Shows static vs dynamic code references via PLT/GOT indirection for shared libraries.","main_text":"This slide gives a simplified dynamic linking picture. Statically linked code references targets directly. Dynamically linked code uses an indirection layer: calls go through structures like the **PLT (Procedure Linkage Table)** and **GOT (Global Offset Table)** so the actual library address can be filled at run time. The key point is that dynamic linking inserts a level of lookup/patching between the program and shared-library code.","notes_text":"","keywords":["dynamic linking","PLT","GOT","indirection","static vs dynamic linking","runtime binding"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"comparison"},"metadata":{"course":"CS356","unit":13,"topic":"Dynamic Linking Mechanism","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit13_Linking","slide_number":38,"chunk_index":0,"title":"Exercise: Build Shared Library libf.so","summary":"Exercise slide for compiling PIC objects and creating a shared library, then linking an app.","main_text":"This exercise provides command-line steps to build a shared library. It includes compiling library modules with `-fpic`, creating a shared object (libf.so), and linking an application against it. The listed steps (cd lib, rm *.o *.a, gcc -c -fpic …, gcc -shared …) guide students through the dynamic/shared library build pipeline.","notes_text":"","keywords":["exercise","shared library","libf.so","-fpic","gcc -shared","dynamic linking build"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":13,"topic":"Shared Library Exercise","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":1,"chunk_index":0,"title":"Unit 14: Pipelining and Processor Architecture","summary":"Opens Unit 14 by introducing CPU pipelining as an “assembly line” approach to improving processor throughput.","main_text":"This title slide introduces Unit 14, focused on pipelining and processor architecture. The central framing is that pipelining works like an assembly line inside the CPU: different parts of instruction execution are overlapped so multiple instructions can be in progress at once. The deck will build from hardware basics up through processor datapaths and pipeline control, explaining how modern CPUs increase performance via parallelism of instruction stages.","notes_text":"","keywords":["Unit 14","pipelining","processor architecture","CPU assembly line","instruction throughput"],"images":[{"description":"Cover visual representing pipelining/CPU assembly line concept.","labels":[],"position":{"x":0.1,"y":0.15,"width":0.8,"height":0.7}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":14,"topic":"Pipelining Overview","importance_score":6,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":2,"chunk_index":0,"title":"Basic Hardware (HW) Review","summary":"Briefly signals a hardware fundamentals review needed before discussing pipelines.","main_text":"This slide marks the start of a quick review of basic hardware concepts that are prerequisites for understanding pipelining. The unit assumes familiarity with core digital logic, registers, and timing, which will be used to reason about datapaths and instruction stage overlap in later slides.","notes_text":"","keywords":["hardware review","digital logic","registers","timing","pipeline prerequisites"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Hardware Foundations","importance_score":5,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":3,"chunk_index":0,"title":"Logic Circuits: Combinational vs Sequential","summary":"Defines combinational and sequential logic and explains why both are essential CPU building blocks.","main_text":"The slide distinguishes two major classes of logic circuits. **Combinational logic** performs a specific function mapping inputs to outputs, has no internal state, and for any fixed input produces a deterministic output after a propagation delay. **Sequential logic** includes storage elements (registers) that hold state across time. Registers are described as fundamental building blocks: they remember bits for later use, behaving like variables in software. This distinction is critical because CPU datapaths use combinational blocks to compute results and sequential blocks to store intermediate values between clock cycles.","notes_text":"","keywords":["logic circuits","combinational logic","sequential logic","registers","state","propagation delay"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Logic Basics","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":4,"chunk_index":0,"title":"Combinational Logic Gates","summary":"Introduces basic logic gates and truth-table behavior used to build larger CPU components.","main_text":"This slide reviews fundamental combinational logic gates such as AND, OR, and NOT. It emphasizes that circuits composed of these gates implement Boolean functions. Truth-table examples illustrate how different input combinations map to outputs. These gates serve as the primitive units from which more complex hardware structures (like adders, multiplexers, and ALUs) are constructed.","notes_text":"","keywords":["logic gates","AND","OR","NOT","truth table","Boolean functions","combinational circuits"],"images":[{"description":"Diagram of AND/OR/NOT gates with truth-table style input/output labels.","labels":["OR gate","AND gate","NOT gate"],"position":{"x":0.1,"y":0.25,"width":0.8,"height":0.6}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Logic Gates","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":5,"chunk_index":0,"title":"Propagation Delay","summary":"Explains that real logic circuits take time to settle, motivating clocked designs.","main_text":"The slide defines **propagation delay**: digital logic does not produce outputs instantly. After inputs change, outputs stabilize only after some gate delay. The slide highlights that circuit delay accumulates across multiple gates, so deeper combinational logic increases total compute time. This matters for CPU timing because the clock period must be long enough for all combinational paths between registers to finish before the next clock edge.","notes_text":"","keywords":["propagation delay","gate delay","timing","critical path","clock period","combinational depth"],"images":[{"description":"Timing/logic example illustrating input changes and delayed output stabilization.","labels":[],"position":{"x":0.1,"y":0.25,"width":0.8,"height":0.6}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":14,"topic":"Timing and Delay","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":6,"chunk_index":0,"title":"ALUs (Arithmetic Logic Units)","summary":"Shows how logic gates combine into an ALU that performs arithmetic and Boolean operations.","main_text":"This slide introduces the **ALU**, built from combinational logic gates. The ALU performs arithmetic (e.g., addition, subtraction) and logical operations (AND, OR, XOR, compare) on binary inputs. The slide notes that inputs are multi-bit words (e.g., 32-bit), and the ALU operates bit-wise in parallel across those words. ALUs are central to instruction execution because most operations in the datapath flow through an ALU stage.","notes_text":"","keywords":["ALU","arithmetic logic unit","binary arithmetic","logic operations","datapath","32-bit words"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"ALU Basics","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":7,"chunk_index":0,"title":"Sequential Devices: Registers","summary":"Describes registers as clocked storage that capture values between combinational stages.","main_text":"The slide introduces registers as **sequential devices** that store state. A register captures its D input on a clock edge and holds that value until the next edge. Registers are used to separate combinational logic into stages: the ALU computes a result, which is then stored in a register so it can be used in later cycles. This clocked storage model is essential for pipelining, where each pipeline stage ends with registers to hold intermediate results.","notes_text":"","keywords":["registers","sequential logic","clock edge","state storage","pipeline stages","D input"],"images":[{"description":"Diagram showing ALU output latched into a register on a clock pulse; may include example assembly flow into %rax.","labels":["clock","register","ALU"],"position":{"x":0.08,"y":0.25,"width":0.84,"height":0.6}}],"layout":{"num_text_boxes":3,"num_images":2,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":14,"topic":"Registers","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":8,"chunk_index":0,"title":"Clock Signal and Cycle Time","summary":"Defines the clock waveform and relates cycle time to processor frequency.","main_text":"This slide explains the **clock signal** as an alternating high/low voltage pulse that coordinates sequential updates. The **cycle time** (clock period) is the time between clock edges; frequency is its inverse. The slide gives a numeric example connecting GHz to nanoseconds per cycle. The key idea is that all combinational logic between registers must complete within one cycle for correct operation.","notes_text":"","keywords":["clock signal","cycle time","frequency","GHz","clock period","timing constraint"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Clocking","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":9,"chunk_index":0,"title":"From x86 to RISC","summary":"Transitions from complex x86/CISC ideas toward RISC principles used in pipeline design.","main_text":"This slide marks the conceptual shift from x86 (historically CISC-style) toward RISC design thinking. The rest of the unit will rely on RISC-like uniform instruction stages, which are easier to pipeline and reason about than highly variable-latency CISC instructions.","notes_text":"","keywords":["x86","RISC transition","CISC vs RISC","pipeline friendliness","uniform stages"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"RISC Motivation","importance_score":6,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":10,"chunk_index":0,"title":"From CISC to RISC","summary":"Compares CISC and RISC philosophies and why RISC aligns with pipelining.","main_text":"The slide contrasts **CISC** and **RISC** architectures. CISC machines use complex instructions that may do more work per instruction but take varying time to execute, reducing the number of instructions needed per task. RISC machines favor simpler instructions that execute in a more uniform way and often in the same number of steps, enabling faster instruction throughput even if more instructions are required overall. This uniformity is a key reason RISC-style pipelines are cleaner and more efficient to design.","notes_text":"","keywords":["CISC","RISC","instruction complexity","uniform execution","pipeline design","throughput tradeoff"],"images":[{"description":"Comparison visual summarizing CISC vs RISC execution characteristics.","labels":["CISC","RISC"],"position":{"x":0.12,"y":0.28,"width":0.76,"height":0.55}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"comparison"},"metadata":{"course":"CS356","unit":14,"topic":"CISC vs RISC","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":11,"chunk_index":0,"title":"A RISC Subset of x86","summary":"Defines a simplified RISC-style subset of x86 by restricting memory access and register operations.","main_text":"This slide explains how to view a RISC-style subset of x86 by imposing constraints that make instruction execution more uniform. First, any mov that accesses memory is split into explicit load and store instructions: **ld** (load/read from memory) and **st** (store/write to memory). Second, ld/st are limited to at most indirect addressing with a displacement, so complex addressing modes like `ld 0x04(%rdi,%rsi,4), %rax` are disallowed as “too much work.” Allowed forms look like `ld 0x40(%rdi), %rax` or `st %rax, 0x40(%rdi)`. Third, arithmetic/logic instructions may only operate on register values. Memory-operand ALU ops are forbidden (e.g., `add (%rsp), %rax`), so the RISC equivalent requires an ld to bring the memory value into a register, an add between registers, then possibly an st back. The point is to standardize instruction behavior into clean stages suitable for pipelining.","notes_text":"","keywords":["RISC subset","x86 restrictions","ld","st","no complex addressing","register-only ALU","pipeline-friendly ISA"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"RISC Constraints for Pipelining","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":12,"chunk_index":0,"title":"Developing a Processor Organization","summary":"Lists core hardware components per instruction type to motivate a unified datapath.","main_text":"This slide inventories the major hardware blocks needed to execute different instruction classes in the simplified ISA. For **ALU-type** ops like `add %rax, %rbx`, the processor uses the **PC (rip)** to fetch, the **Register File** to read operands, and the **ALU** to compute results and update condition codes (flags). For **LD** instructions such as `ld 8(%rax), %rbx`, the datapath needs PC/I-Cache fetch, Register File read for the base register, the ALU to compute an effective address (`%rax + 8`), the **D-Cache** to read memory, and then writeback to a destination register. For **ST** like `st %rbx, 8(%rax)`, it similarly computes an address in the ALU and writes data to D-Cache. For conditional jumps (**JE**), the datapath fetches and decodes the instruction, checks condition codes, and if the condition is true updates `PC = PC + disp`. The slide’s takeaway is that a small repeated set of components supports all instruction types, enabling a consistent staged pipeline design.","notes_text":"","keywords":["processor organization","PC","register file","ALU","I-Cache","D-Cache","LD/ST/JE datapath"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":14,"topic":"Datapath Components","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":13,"chunk_index":0,"title":"Processor Block Diagram (Overall)","summary":"Shows a five-stage processor block diagram and stage timing that determines cycle time.","main_text":"This slide presents the overall processor block diagram organized into classic five stages: **Fetch**, **Decode**, **Execute**, **Mem**, and **WB (Writeback)**. The datapath includes an **I-Cache** for instruction fetch, a **Register File** for operand access, an **ALU** for effective address and arithmetic/logic work, a **D-Cache** for data reads/writes, and condition-code outputs (ZF, OF, CF, SF). Control signals derived during decode steer ALU operations and cache read/write behavior. The slide indicates each stage’s delay (about 10 ns per stage) and notes that the **clock cycle time equals the worst-case path delay**, summing to roughly **50 ns** through the full non-pipelined datapath. This diagram is the baseline that later slides specialize per instruction type and then pipeline by inserting registers between stages.","notes_text":"","keywords":["processor block diagram","5-stage datapath","Fetch","Decode","Execute","Mem","WB","cycle time","critical path"],"images":[{"description":"Overall 5-stage datapath diagram with I-Cache, RegFile, ALU, D-Cache, control signals, and condition codes; stage delays labeled ~10 ns each.","labels":["Fetch","Decode","Exec.","Mem","WB","I-Cache","D-Cache","ALU","RegFile","Control Signals","ZF","OF","CF","SF"],"position":{"x":0.05,"y":0.12,"width":0.9,"height":0.76}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Baseline Datapath","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":14,"chunk_index":0,"title":"Processor Block Diagram (add)","summary":"Specializes the datapath for an ALU instruction and shows where work happens by stage.","main_text":"This slide overlays the overall processor block diagram with the specific flow for an ALU-type add, e.g., `add %rax, %rdx` with machine code `[48 01 c2]`. In **Fetch**, the instruction is read from I-Cache using PC. In **Decode**, the register file reads the source and destination registers (%rax and %rdx) and the control unit identifies this as an ALU operation. In **Execute**, the ALU computes `%rax + %rdx`. There is no data-memory access in the **Mem** stage for add, so it is effectively idle. In **WB**, the computed result is written back to the destination register `%rdx`. The slide emphasizes the clean per-stage division that makes pipelining possible.","notes_text":"","keywords":["add datapath","ALU instruction","stage behavior","Fetch/Decode/Execute/Mem/WB","writeback"],"images":[{"description":"Block diagram annotated for add instruction path; highlights RegFile read, ALU add, and WB to destination register.","labels":["add %rax,%rdx","ALU op","WB result"],"position":{"x":0.06,"y":0.12,"width":0.88,"height":0.76}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Add Instruction Datapath","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":15,"chunk_index":0,"title":"Processor Block Diagram (ld)","summary":"Shows the datapath stages for a load instruction including effective-address calc and D-Cache read.","main_text":"This slide specializes the datapath for a load instruction, using an example whose machine code is `[48 8b 43 40]` (load from memory at base+0x40 into a register). In **Fetch**, the ld instruction is obtained from I-Cache. In **Decode**, the register file reads the base register (e.g., %rbx) and control identifies a load. In **Execute**, the ALU computes the effective address `base + displacement` (e.g., `%rbx + 0x40`). In **Mem**, the D-Cache is read at that address to obtain data. In **WB**, the loaded data is written into the destination register (e.g., `%rax = data`). The slide highlights that unlike add, ld uses the memory stage, which impacts pipeline hazards later.","notes_text":"","keywords":["ld datapath","load instruction","effective address","D-Cache read","writeback","displacement addressing"],"images":[{"description":"Block diagram annotated for ld instruction; ALU computes address, D-Cache supplies data, WB writes destination register.","labels":["ld","addr","data","0x40"],"position":{"x":0.06,"y":0.12,"width":0.88,"height":0.76}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Load Instruction Datapath","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":16,"chunk_index":0,"title":"Processor Block Diagram (st)","summary":"Shows the datapath stages for a store instruction including address calc and D-Cache write.","main_text":"This slide specializes the datapath for a store instruction, with example machine code `[48 89 43 40]` (store a register value to memory at base+0x40). In **Fetch**, the st instruction is read from I-Cache. In **Decode**, the register file supplies both the base register for addressing (e.g., %rbx) and the data register to store (e.g., %rax). In **Execute**, the ALU computes the effective address `%rbx + 0x40`. In **Mem**, the D-Cache performs a **write** of the source data to that address. There is no meaningful register writeback in **WB** for st. The slide reinforces the stage-by-stage decomposition shared with ld, differing mainly by memory read vs write and lack of WB.","notes_text":"","keywords":["st datapath","store instruction","effective address","D-Cache write","no WB","displacement"],"images":[{"description":"Block diagram annotated for st instruction; ALU computes address, D-Cache writes data from a register.","labels":["st","addr","data","0x40"],"position":{"x":0.06,"y":0.12,"width":0.88,"height":0.76}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Store Instruction Datapath","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":17,"chunk_index":0,"title":"Processor Block Diagram (je)","summary":"Specializes the datapath for a conditional jump based on condition codes and PC update.","main_text":"This slide specializes the datapath for a conditional jump, example `je` with displacement (e.g., `disp = 0x08`) and machine code `[74 08]`. In **Fetch**, the instruction is obtained from I-Cache using PC. In **Decode**, control logic recognizes a conditional branch while condition codes (such as ZF) are available from previous ALU results. In **Execute**, the ALU (or branch logic) evaluates the branch condition; if true, it computes the target `PC + disp`. The **Mem** stage is not used for je. In **WB**, there is no register writeback; the key effect is the potential PC update. The slide makes clear why branches are special in pipelines: the next PC may depend on a condition resolved only after decode/execute.","notes_text":"","keywords":["je datapath","conditional branch","PC update","displacement","condition codes","ZF"],"images":[{"description":"Block diagram annotated for je path; condition check leads to PC+disp target update.","labels":["je","PC + disp","disp=0x08","74 08"],"position":{"x":0.06,"y":0.12,"width":0.88,"height":0.76}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Branch Instruction Datapath","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":18,"chunk_index":0,"title":"Pipelining","summary":"Section header introducing pipelining as the next major topic.","main_text":"This slide is a section divider titled “Pipelining.” It marks the transition from baseline datapaths and per-instruction stage behavior to the idea of overlapping those stages across multiple instructions. The following slides will explain why pipelining improves throughput and how stage registers enable overlap.","notes_text":"","keywords":["pipelining","section header","throughput","stage overlap"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Pipelining Intro","importance_score":5,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":19,"chunk_index":0,"title":"Example (Non-Pipelined Timing)","summary":"Uses a loop example to show total time when stages are not overlapped.","main_text":"This slide presents a simple loop example: `for(i=0; i < 100; i++) C[i] = (A[i] + B[i]) / 4;`. It assumes a non-pipelined organization where each full instruction sequence for one iteration takes a fixed time per input set. The slide states an example timing like **10 ns per input set**, leading to **1000 ns total** for 100 iterations. A memory diagram labels arrays A[i], B[i], and C[i], along with loop index i and counter state. The purpose is to establish the baseline cost before pipelining so later slides can show how overlapping stages reduces total execution time.","notes_text":"","keywords":["timing example","non-pipelined cost","loop","A[i]","B[i]","C[i]","baseline throughput"],"images":[{"description":"Memory/loop schematic labeling A[i], B[i], C[i], and loop counter i to visualize per-iteration work.","labels":["A[i]","B[i]","C[i]","i","Cntr"],"position":{"x":0.12,"y":0.32,"width":0.76,"height":0.5}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":14,"topic":"Baseline Timing Example","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":20,"chunk_index":0,"title":"Pipelining Example","summary":"Introduces pipelining as inserting registers to split combinational logic into overlappable stages.","main_text":"Continuing the loop example `C[i] = (A[i] + B[i]) / 4`, this slide introduces pipelining at a high level. It depicts at least two stages (Stage 1 and Stage 2) and explains that **pipelining inserts registers** between pieces of combinational logic so execution is divided into multiple stages. Once split, different instructions can occupy different stages simultaneously, overlapping in time like an **assembly line**. The key takeaway is that pipelining improves **throughput** by allowing multiple iterations or instructions to be in flight at once, even though each individual instruction still passes through the same stages.","notes_text":"","keywords":["pipelining example","stage registers","overlap","assembly line","throughput improvement"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Pipelining Concept","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":21,"chunk_index":0,"title":"Need for Registers","summary":"Explains why registers are necessary in pipelined hardware to stabilize values while combinational logic operates.","main_text":"This slide introduces the motivation for pipeline registers in a processor datapath. Combinational logic requires stable inputs to compute correctly. Without registers, values would continuously ripple through circuits, making correct, timed operation impossible. Registers provide separation between combinational stages by capturing outputs and holding them fixed for one clock cycle. This enables predictable timing, ensures correct data propagation, and forms the foundation of synchronous pipelining. By storing intermediate values, registers allow different pipeline stages to work concurrently while preventing interference from upstream changes.","notes_text":"Core concept: registers isolate logic and make clocked, staged execution feasible.","keywords":["pipeline registers","combinational logic","timing","stabilizing values","synchronous design","pipeline stages"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Pipeline Registers","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":22,"chunk_index":0,"title":"Processors & Pipelines","summary":"Introduces how processors overlap instruction execution across stages from multiple viewpoints.","main_text":"This slide emphasizes that processors use pipelining to overlap the execution of multiple instructions. It presents pipelining from two viewpoints: the **instruction view**, where each instruction passes through stages like Fetch, Decode, Execute, Mem, and Writeback; and the **stage view**, where at any cycle, different instructions occupy different stages. These perspectives reinforce the idea that pipelining increases throughput by keeping all functional units working simultaneously. Rather than completing one full instruction before starting the next, pipelining allows concurrent progression, much like an industrial assembly line.","notes_text":"","keywords":["pipelining","overlap","instruction stages","throughput","instruction view","stage view"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Pipelining Overview","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":23,"chunk_index":0,"title":"Balancing Pipeline Stages","summary":"Shows why pipeline stages must be balanced so the clock period matches the longest stage delay.","main_text":"This slide explains that the clock period of a pipeline must be at least as long as the slowest pipeline stage. Several diagrams (Fig. 1–Fig. 4) illustrate how splitting logic into stages affects performance. If one stage contains too much logic compared to others, the clock must slow down to accommodate it, limiting throughput. Proper balancing requires distributing logic as evenly as possible so no single stage becomes the bottleneck. The slide emphasizes that the longest-stage delay determines the achievable clock frequency, and pipeline design must account for optimizing stage delays.","notes_text":"Key design challenge: minimize the maximum stage delay.","keywords":["pipeline balancing","clock period","critical path","stage delay","throughput","pipeline design"],"images":[{"description":"Four small diagrams labeled Fig.1–Fig.4 showing different ways logic can be partitioned among pipeline stages.","labels":["Fig.1","Fig.2","Fig.3","Fig.4"],"position":{"x":0.05,"y":0.25,"width":0.9,"height":0.55}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Stage Balancing","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":24,"chunk_index":0,"title":"Balancing Pipeline Stages (Main Points)","summary":"Compares pipeline depths and shows latency vs throughput for 1-, 2-, 4-, and 8-stage pipelines.","main_text":"This slide expands on stage balancing by listing key observations: (1) The latency of any single instruction equals the sum of all pipeline stage delays; deeper pipelines do not reduce per-instruction latency. (2) Throughput improves as long as each stage's delay decreases enough to justify the additional pipeline overhead. The slide shows example comparisons: a 1-stage (non-pipelined) system with 20 ns latency; a 2-stage pipeline achieving 2× throughput; a 4-stage pipeline achieving 4× throughput; and an 8-stage pipeline reaching 8× throughput. These examples illustrate that pipelining primarily improves throughput, not latency, and requires careful control of stage delay growth.","notes_text":"Deeper pipelines suffer diminishing returns due to overhead.","keywords":["pipeline stages","throughput","latency","multi-stage pipeline","performance scaling","pipeline depth"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Pipeline Depth","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":25,"chunk_index":0,"title":"Throughput and Latency","summary":"Defines throughput and latency using a timing table showing clock period and instructions per second.","main_text":"This slide contains a table comparing pipeline depth (n), clock period in picoseconds, and throughput in GIPS (billion instructions per second). The formulas given are: **throughput = 1/clock**, and **latency = n × clock**. Increasing pipeline depth can increase throughput by reducing clock period, but latency still grows linearly with the number of stages. The slide reinforces that pipelining trades off latency for throughput: even though the clock gets faster, each instruction still must traverse n stages, each with its own register overhead.","notes_text":"","keywords":["throughput","latency","clock period","GIPS","pipeline depth","timing table"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Latency vs Throughput","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":26,"chunk_index":0,"title":"5-Stage Pipeline Overview","summary":"Defines the 5 pipeline stages and maps them to machine code, operands, and example instructions.","main_text":"This slide introduces the classical 5-stage RISC pipeline: **Fetch**, **Decode**, **Execute**, **Memory**, and **Writeback**. It shows how machine code instructions, operands, and control signals flow through these stages. The table lists common operations: **ADD** (add reg to reg), **LD** (load from memory), **ST** (store to memory), and **JE** (conditional jump). Each instruction is decomposed into how it uses registers, ALU operations, displacement fields, and memory accesses. This slide forms the structural reference for all subsequent pipeline timing diagrams.","notes_text":"","keywords":["5-stage pipeline","Fetch","Decode","Execute","Mem","WB","instruction breakdown","ISA mapping"],"images":[{"description":"Pipeline diagram with five labeled stages and arrows showing operand and control-flow movement through the datapath.","labels":["Fetch","Decode","Exec","Mem","WB"],"position":{"x":0.08,"y":0.22,"width":0.84,"height":0.65}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"5-Stage Pipeline","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":27,"chunk_index":0,"title":"Pipelining — Execution of a Sequence","summary":"Introduces the idea of visualizing instruction overlap in a pipeline through cycle-by-cycle diagrams.","main_text":"This slide sets up a multi-instruction pipeline trace by stating: “Let’s see how a sequence of instructions can be executed.” It signals a transition into examples that show how instructions progress through Fetch, Decode, Execute, Mem, and WB over time. These examples illustrate how pipelining increases throughput by keeping all stages busy once the pipeline is filled.","notes_text":"","keywords":["pipeline sequence","instruction overlap","cycle diagram","pipeline fill"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Pipeline Tracing","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":28,"chunk_index":0,"title":"Sample Sequence — 1","summary":"Shows the first cycle-by-cycle pipeline diagram for a sample LD instruction.","main_text":"This slide begins the detailed pipeline walk-through for a sample instruction sequence. It charts how an LD instruction proceeds through the stages Fetch, Decode, Exec, Mem, and WB. The diagram includes the datapath components used by the LD: D-Cache, ALU, Register File, PC, and condition code outputs (ZF, OF, CF, SF). This visualization provides the first concrete illustration of overlapping activities in the pipeline as subsequent slides add additional instructions.","notes_text":"","keywords":["pipeline example","load instruction","stage timing","D-Cache","ALU","RegFile"],"images":[{"description":"Pipeline timing chart showing LD instruction moving through stages with datapath components displayed.","labels":["LD","Fetch","Decode","Exec","Mem","WB","D-Cache","ALU","RegFile","PC"],"position":{"x":0.05,"y":0.18,"width":0.9,"height":0.7}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Pipeline Example 1","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":29,"chunk_index":0,"title":"Sample Sequence — 2","summary":"Adds an ADD instruction into the pipeline, showing overlapping execution with the preceding LD.","main_text":"This slide continues the pipeline trace by adding the next instruction—an ADD—that begins Fetch while the LD is still in later stages. The diagram shows both instructions occupying different pipeline stages simultaneously. The datapath blocks (I-Cache, D-Cache, ALU, Register File, PC) illustrate how hardware is reused each cycle by different instructions. This slide demonstrates the essential throughput gain: new instructions enter the pipeline every cycle once the pipeline is filled.","notes_text":"","keywords":["pipeline overlap","ADD instruction","multi-instruction trace","timing diagram","throughput"],"images":[{"description":"Pipeline timing chart showing LD followed by ADD occupying staggered stages.","labels":["LD","ADD","Fetch","Decode","Exec","Mem","WB"],"position":{"x":0.05,"y":0.2,"width":0.9,"height":0.68}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Pipeline Example 2","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":30,"chunk_index":0,"title":"Sample Sequence — 3","summary":"Extends the example with additional instructions, showing deeper pipeline fill patterns.","main_text":"This slide deepens the pipeline example by adding more instructions and showing their progression across the five stages. Instructions include an ADD operation (`ADD %rcx, %rdx`) and a load involving displacement (`0x40 / %rbx / READ`). The diagram shows how the ADD uses the ALU in Execute, while the LD performs address calculation and then memory access. Condition codes (ZF, OF, CF, SF) update as the ADD executes. As additional instructions enter the pipeline, the chart reveals the steady-state overlap where nearly all stages are busy each cycle.","notes_text":"","keywords":["pipeline timing","ADD","LD with displacement","stage progression","condition codes","steady-state pipeline"],"images":[{"description":"Pipeline chart showing ADD and LD with displacement across multiple stages with ALU and D-Cache involvement.","labels":["ADD","LD","0x40","%rbx","READ","ZF","OF","CF","SF"],"position":{"x":0.06,"y":0.18,"width":0.88,"height":0.7}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Pipeline Example 3","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":31,"chunk_index":0,"title":"Sample Sequence — 4","summary":"Adds additional overlapping instructions to deepen the pipeline example, showing multiple operations in flight.","main_text":"This slide continues the pipeline execution trace by adding a third and fourth instruction, further illustrating how the pipeline fills over time. The stages Fetch, Decode, Exec, Mem, and WB are shown across multiple cycles with instructions staggered in a diagonal pattern. The datapath blocks (I-Cache, D-Cache, ALU, Register File, PC) appear repeatedly to show resource reuse. The deeper pipeline view demonstrates that once full, the pipeline retires one instruction per cycle, even though each instruction individually still takes five cycles to complete.","notes_text":"","keywords":["pipeline fill","overlapping instructions","timing diagram","throughput","multi-instruction pipeline"],"images":[{"description":"Pipeline timing chart with four instructions staggered across stages, showing full pipeline operation.","labels":["Fetch","Decode","Exec","Mem","WB"],"position":{"x":0.05,"y":0.2,"width":0.9,"height":0.7}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Pipeline Example 4","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":32,"chunk_index":0,"title":"Register File Usage in Pipeline","summary":"Shows when pipeline stages read or write registers and how hazards arise from this timing.","main_text":"This slide highlights how instructions interact with the register file in a pipelined processor. The Decode stage reads register operands, while the Writeback stage writes results. Because these operations occur in different cycles, dependencies between instructions can cause data hazards. The slide visually maps read and write operations to their respective cycles in the pipeline timeline. It serves as a precursor to identifying Read-After-Write (RAW) hazards, demonstrating how the pipeline might attempt to read stale data before previous instructions have updated the register file.","notes_text":"","keywords":["register file","pipeline hazard","RAW hazard","decode stage","writeback stage"],"images":[{"description":"Diagram showing timing of register reads during Decode and writes during WB across multiple instructions.","labels":["RegFile","read","write","Decode","WB"],"position":{"x":0.08,"y":0.22,"width":0.84,"height":0.65}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Register File Timing","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":33,"chunk_index":0,"title":"ISA Constraints on Pipelining","summary":"Describes how the instruction set architecture must support safe pipelining behavior.","main_text":"This slide states: “The ISA must be defined to work correctly with pipelining.” Some architectures are easier to pipeline than others. For example, instructions must not overwrite registers before dependent instructions can safely read them; condition codes must update in an orderly and predictable manner; memory dependencies must be well-defined; and the execution semantics must allow overlapping without violating program order. Many RISC architectures were explicitly designed with these constraints in mind, whereas CISC architectures like x86 require more complex hardware mechanisms to preserve correct behavior when pipelined.","notes_text":"","keywords":["ISA constraints","pipeline design","instruction semantics","program order","architectural hazards"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"ISA Constraints","importance_score":7,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":34,"chunk_index":0,"title":"Data Hazards Overview","summary":"Introduces hazards as pipeline conditions where parallel execution causes incorrect behavior.","main_text":"This slide introduces hazards broadly: situations where parallel execution in a pipeline could violate program semantics. It lists the three major categories: (1) **Data Hazards**, which occur when instructions depend on the results of prior ones; (2) **Control Hazards**, associated with branches and changes to the PC; and (3) **Structural Hazards**, which occur when hardware resources are insufficient to support all simultaneous operations. The slide sets the stage for focusing specifically on data hazards next, which occur when the pipeline reads operands before they have been produced.","notes_text":"","keywords":["hazards","data hazard","control hazard","structural hazard","pipeline correctness"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Hazards Overview","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":35,"chunk_index":0,"title":"Data Hazards","summary":"Explains RAW, WAR, and WAW hazards and notes which occur in classic 5-stage RISC pipelines.","main_text":"This slide enumerates the three types of data hazards:\n• **RAW (Read After Write)**: The most common, occurring when an instruction tries to read a register before a previous instruction writes it.\n• **WAR (Write After Read)**: Occurs when an instruction writes a register before a previous instruction reads it.\n• **WAW (Write After Write)**: Occurs when two instructions write the same register out of order.\nIn classic 5-stage RISC pipelines, only RAW hazards occur, because register reads happen early (Decode) and writes occur late (Writeback). WAR and WAW hazards are eliminated by design through in-order execution and fixed pipeline stage behavior.","notes_text":"","keywords":["data hazard","RAW hazard","WAR hazard","WAW hazard","pipeline hazards","in-order pipeline"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Data Hazards","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":36,"chunk_index":0,"title":"RAW Hazard Example","summary":"Demonstrates a RAW hazard where an instruction reads a register before a previous one writes it.","main_text":"This slide presents a concrete RAW hazard example using register operations r2 ← r1 + r3 followed by r4 ← r2 + r5. The consumer instruction needs the updated value of r2, but in the pipeline it may attempt to read r2 during Decode before the producing instruction reaches Writeback. The diagram illustrates that without hazard mitigation, the consumer instruction would retrieve a stale value. The slide sets up the need for forwarding or stalling as solutions.","notes_text":"","keywords":["RAW hazard","register dependency","decode stage","writeback stage","data dependency"],"images":[{"description":"Dependency diagram showing two instructions where the second uses r2 produced by the first.","labels":["r2","dependency","RAW"],"position":{"x":0.1,"y":0.25,"width":0.8,"height":0.55}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"RAW Hazards","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":37,"chunk_index":0,"title":"Pipeline Diagram: RAW Hazard","summary":"Shows timeline visualization of a RAW hazard without forwarding or stalling.","main_text":"In this slide, the pipeline timing chart illustrates the RAW hazard across cycles. The producer instruction writes its result in the WB stage, which occurs after the consumer instruction attempts to read the register in Decode. This mismatch in timing creates the hazard. The diagram highlights the specific cycles where the Decode stage attempts to read a not-yet-written value. This visual reinforces why naive pipelining cannot guarantee correctness when dependent instructions are close together.","notes_text":"","keywords":["pipeline timing","RAW hazard","cycle diagram","producer-consumer dependency"],"images":[{"description":"Pipeline timing chart showing Decode of consumer preceding Writeback of producer.","labels":["Decode","WB","hazard"],"position":{"x":0.06,"y":0.22,"width":0.88,"height":0.65}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"RAW Timing","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":38,"chunk_index":0,"title":"Fixing Data Hazards","summary":"Introduces forwarding and stalling as the two primary mechanisms to fix RAW hazards.","main_text":"This slide states: “Data hazards can be fixed with: Forwarding — use the result early. Stalling — wait for result to be ready.” Forwarding allows the pipeline to directly route results from later stages (e.g., Exec or Mem) to earlier stages (e.g., Decode or Exec) without waiting for Writeback. Stalling introduces pipeline bubbles when forwarding cannot solve the hazard, especially for load-use hazards. The slide sets up the detailed diagrams explaining both techniques in the next slides.","notes_text":"","keywords":["data hazard fix","forwarding","stalling","pipeline bubble","RAW hazard solution"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Hazard Solutions","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":39,"chunk_index":0,"title":"Forwarding","summary":"Shows how forwarding bypasses the register file to supply results to dependent instructions earlier.","main_text":"Forwarding (also called bypassing) solves many RAW hazards by routing ALU or memory results to earlier pipeline stages before Writeback completes. The slide shows datapath arrows from the ALU/Mem outputs back into the Execute stage of a following instruction. This avoids reading stale values from the register file during Decode. The key idea is that even though the register file is not yet updated, the correct result exists earlier in the pipeline and can be forwarded to dependent instructions.","notes_text":"","keywords":["forwarding","bypassing","RAW hazard fix","ALU bypass","data dependency"],"images":[{"description":"Datapath diagram showing forwarding paths from ALU and Mem stages back into pipeline input.","labels":["ALU","Mem","forward","bypass"],"position":{"x":0.08,"y":0.22,"width":0.84,"height":0.65}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Forwarding Mechanism","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":40,"chunk_index":0,"title":"Forwarding Examples","summary":"Gives concrete examples showing when forwarding succeeds and when it cannot fully resolve hazards.","main_text":"This slide gives multiple examples of forwarding paths for operations such as r2 ← r1 + r3 followed by r4 ← r2 + r5. It shows that ALU results can be forwarded directly to the next instruction’s Execute stage. Another example shows forwarding of memory-loaded values, but also highlights that load-use hazards cannot always be solved by forwarding alone because loaded data becomes available only after the Mem stage. In such cases, a one-cycle stall is required. The diagrams visualize arrows representing the forwarded values entering the consumer instruction’s Execute stage.","notes_text":"","keywords":["forwarding examples","ALU result forwarding","load-use hazard","stall requirement","pipeline dependencies"],"images":[{"description":"Example diagrams showing different forwarding paths and one load-use case requiring a stall.","labels":["forward","ALU","load-use","stall"],"position":{"x":0.06,"y":0.2,"width":0.88,"height":0.7}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Forwarding Examples","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":41,"chunk_index":0,"title":"Forwarding: Solution 2","summary":"Shows forwarding hardware that bypasses register writes to supply ALU results directly to dependent instructions.","main_text":"This slide introduces forwarding (also called bypassing) as a hardware solution to data hazards in pipelined processors. When an instruction in the Execute stage produces a value that a following instruction needs, the processor can route this value directly to the dependent instruction without waiting for it to reach the Writeback stage. The diagram shows instruction i+2 in Fetch, i+1 in Decode, and an ADD instruction in Execute receiving forwarded operands. Forwarding eliminates many RAW hazards and reduces the number of pipeline stalls required.","notes_text":"","keywords":["forwarding","bypassing","data hazard","RAW hazard","pipeline","ALU result","dependency"],"images":[{"description":"Diagram showing three instructions in different stages and a forwarding path sending an ALU output back to an earlier stage.","labels":["Fetch","Decode","Exec","ADD","forwarding path"],"position":{"x":0.1,"y":0.25,"width":0.8,"height":0.55}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Forwarding","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":42,"chunk_index":0,"title":"Solving Data Hazards","summary":"Explains that forwarding solves most RAW hazards and introduces the special case of the load-use hazard.","main_text":"This slide summarizes how forwarding reduces most data hazards by supplying results as soon as they are computed. However, forwarding alone cannot handle the load-use hazard, where an instruction immediately following a load needs the loaded value before it is available in the pipeline. Because the loaded data is only ready after the Memory stage, the next instruction cannot receive it in time through forwarding alone. Thus, while forwarding is powerful, additional hazard detection and stalling hardware are required for load-use situations.","notes_text":"","keywords":["data hazard","RAW hazard","load-use hazard","forwarding","stall","pipeline dependency"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Data Hazards","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":43,"chunk_index":0,"title":"Load-Use Hazard","summary":"Shows the classic load-use hazard where forwarding cannot supply the loaded value in time.","main_text":"The load-use hazard occurs when an instruction performs a load from memory and the very next instruction needs that loaded value. The pipeline diagram shows that the load's result is not ready until the Memory stage, while the dependent instruction requires it in the Execute stage one cycle earlier. Because forwarding cannot provide data that does not yet exist, the processor must insert a bubble (stall) to delay the dependent instruction. This slide emphasizes that load-use hazards are unavoidable without stalling.","notes_text":"","keywords":["load-use hazard","load instruction","stall","bubble","pipeline timing","forwarding limitation"],"images":[{"description":"Pipeline chart showing a load instruction followed by a dependent instruction, with a stall cycle inserted.","labels":["Load","Dependent instruction","stall","Mem","Exec"],"position":{"x":0.1,"y":0.22,"width":0.8,"height":0.6}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Load-Use Hazard","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":44,"chunk_index":0,"title":"Load-Use Hazard: Insert a Bubble","summary":"Demonstrates inserting one stall cycle to allow the load instruction to finish before the dependent instruction executes.","main_text":"To resolve the load-use hazard, the processor inserts one bubble (stall cycle) between the load instruction and the dependent instruction. The diagram shows that the dependent instruction is held in the Decode stage while the load proceeds to Memory. After the loaded value is available, forwarding can supply it to the dependent instruction in the Execute stage. This slide illustrates exactly how the single-cycle stall enables correct execution without violating the pipeline’s timing constraints.","notes_text":"","keywords":["bubble","stall","hazard resolution","pipeline stall","load-use dependence","Decode stage stall"],"images":[{"description":"Diagram showing Decode stage stall while the load proceeds, creating a bubble in the pipeline.","labels":["stall","bubble","Decode","Mem","Exec"],"position":{"x":0.08,"y":0.2,"width":0.84,"height":0.6}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Load-Use Stall","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":45,"chunk_index":0,"title":"Hazard Detection Unit","summary":"Introduces hardware that detects when an instruction depends on a load in the immediately preceding cycle.","main_text":"The hazard detection unit monitors the pipeline for load-use hazards. It identifies when the instruction in the Decode stage needs a register that is the destination of a load instruction in the Execute stage. If such a condition is detected, the hazard detection unit asserts control signals that stall the Decode stage, insert a bubble into the Execute stage, and prevent the Fetch stage from advancing. This makes the pipeline pause just long enough for the load value to become available.","notes_text":"","keywords":["hazard detection unit","stall logic","control signals","load dependency","pipeline control","Decode stall"],"images":[{"description":"Block diagram showing hazard detection logic comparing register IDs and triggering stall signals.","labels":["Hazard Detection","stall","register match"],"position":{"x":0.1,"y":0.25,"width":0.8,"height":0.55}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Hazard Detection","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":46,"chunk_index":0,"title":"Hazard Detection Logic (Detail)","summary":"Shows the exact condition that triggers a stall when a load-use hazard is detected.","main_text":"This slide details the boolean logic used by the hazard detection unit. A stall is required if the instruction in the Execute stage is a load, and the Decode stage instruction reads a register that matches the load's destination register. In this case, the pipeline must freeze the PC and IF/ID pipeline register, and insert a NOP into the ID/EX register. The slide typically includes the exact conditions comparing register numbers and load control signals.","notes_text":"","keywords":["hazard logic","stall condition","load-use detection","boolean logic","pipeline registers","NOP insertion"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Hazard Logic","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":47,"chunk_index":0,"title":"Control Hazards","summary":"Introduces hazards caused by branches and changes in control flow.","main_text":"Control hazards occur when the pipeline does not know the next instruction to fetch because of a branch instruction. Since branches determine the next PC value, the Fetch stage may continue fetching the wrong instructions until the branch resolves. This slide states that control hazards are a major performance limiter in deeper pipelines and motivates the need for techniques like stalling, flushing, branch prediction, and delayed branching.","notes_text":"","keywords":["control hazard","branch hazard","pipeline flush","branch resolution","PC update","wrong-path instructions"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Control Hazards","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":48,"chunk_index":0,"title":"Branch Hazard Timeline","summary":"Shows pipeline timing for branches and highlights when the branch condition is known.","main_text":"This slide shows the cycle-by-cycle pipeline behavior for a branch instruction. The branch instruction is fetched and decoded normally, but its condition and target are not known until a later stage—typically the Execute stage. Instructions fetched after the branch may be invalid if the branch is taken, requiring a flush of those pipeline stages. The timeline illustrates where these wrong-path instructions appear and why the pipeline must remove them.","notes_text":"","keywords":["branch hazard","timing diagram","pipeline flush","branch resolution","wrong-path fetch"],"images":[{"description":"Timeline chart for branch execution showing when the branch is resolved and where wrong-path instructions appear.","labels":["branch","flush","timeline"],"position":{"x":0.1,"y":0.2,"width":0.8,"height":0.6}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Branch Timing","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":49,"chunk_index":0,"title":"Flushing the Pipeline","summary":"Explains flushing wrong-path instructions after a branch is resolved.","main_text":"When a branch instruction resolves and its outcome differs from the predicted or default path, all instructions fetched after the branch but before its resolution must be flushed. Flushing replaces these instructions with NOPs to prevent them from modifying the machine state. The slide shows how the flush control signal clears the IF/ID and ID/EX pipeline registers. This ensures the pipeline restarts from the correct path without executing invalid instructions.","notes_text":"","keywords":["pipeline flush","branch misprediction","control hazard","NOP injection","pipeline registers"],"images":[{"description":"Diagram showing control signals clearing pipeline registers after a branch resolves.","labels":["flush","pipeline register","branch"],"position":{"x":0.1,"y":0.28,"width":0.8,"height":0.5}}],"layout":{"num_text_boxes":1,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":14,"topic":"Flushing","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":50,"chunk_index":0,"title":"Branch Prediction (Intro)","summary":"Introduces the idea of branch prediction to reduce control hazard penalties.","main_text":"Because waiting for branches to resolve causes pipeline stalls and flushes, processors use branch prediction to guess the outcome of branches before they execute. If the prediction is correct, the pipeline continues without interruption. If incorrect, the pipeline flushes the mispredicted instructions and resumes from the correct target. This slide motivates why prediction is essential for high performance in modern pipelined processors.","notes_text":"","keywords":["branch prediction","control hazard","pipeline performance","misprediction","prediction accuracy"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Branch Prediction","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":51,"chunk_index":0,"title":"A Look Ahead: Branch Prediction","summary":"Explains that the pipeline’s current handling of branches is too slow and motivates the need for more advanced prediction techniques.","main_text":"This slide points out that the pipeline currently assumes branches resolve late, which causes stalls and incorrect-path execution. Because many programs contain frequent branches, this naive approach wastes cycles and lowers throughput. The slide introduces the idea that real processors must predict the outcome of branches earlier, either through static heuristics or dynamic history-based mechanisms. It emphasizes that improving prediction accuracy is essential for reducing pipeline flushes and maintaining high instruction-per-cycle performance.","notes_text":"","keywords":["branch prediction","pipeline stalls","control hazards","static prediction","dynamic prediction","performance impact"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Branch Prediction Motivation","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":52,"chunk_index":0,"title":"Static Branch Prediction","summary":"Introduces fixed prediction strategies such as always-taken and always-not-taken and discusses their limitations.","main_text":"This slide describes static branch prediction, where the processor uses a predetermined rule to guess branch outcomes. Common strategies include always predicting not-taken, always predicting taken, or choosing based on branch direction (e.g., backward branches taken for loops). While simple and low-cost, static predictors cannot adapt to changing program behavior. As a result, their accuracy is limited, especially for branches whose behavior varies over time. The slide sets up the need for dynamic predictors that learn patterns at runtime.","notes_text":"","keywords":["static prediction","always taken","always not taken","branch behavior","pipeline control hazards","prediction limitations"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Static Branch Prediction","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":53,"chunk_index":0,"title":"Dynamic Branch Prediction (Overview)","summary":"Introduces hardware that learns branch behavior using runtime feedback rather than fixed rules.","main_text":"The slide explains that dynamic branch prediction improves performance by monitoring actual branch behavior over time. Hardware records recent outcomes and uses that history to make better guesses for future executions of the same branch. Unlike static strategies, dynamic predictors can adapt to patterns such as alternating behavior or phase changes in loops. This adaptability significantly increases prediction accuracy, reducing the number of pipeline flushes caused by mispredictions.","notes_text":"","keywords":["dynamic prediction","branch history","runtime learning","misprediction reduction","adaptive hardware","pipeline performance"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Dynamic Prediction Intro","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":54,"chunk_index":0,"title":"1-Bit Branch History Predictor","summary":"Describes the simplest dynamic predictor that tracks the last outcome of a branch and reuses it.","main_text":"This slide introduces the 1-bit branch predictor, which stores a single bit per branch indicating whether it was last taken or not taken. When the branch is encountered again, the predictor repeats the previous outcome. Although simple, this approach fails on loop-closing branches because a single misprediction causes the predictor to flip incorrectly, leading to two consecutive wrong guesses. The slide highlights the weakness of relying solely on the most recent outcome.","notes_text":"","keywords":["1-bit predictor","branch history bit","taken/not taken","loop branches","misprediction","dynamic prediction"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"1-Bit Predictors","importance_score":8,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":55,"chunk_index":0,"title":"2-Bit Saturating Counter Predictor","summary":"Explains how using two bits reduces mispredictions caused by temporary behavior changes.","main_text":"This slide describes the widely used 2-bit saturating counter predictor. Instead of remembering only the most recent branch outcome, the predictor maintains a four-state machine ranging from strongly-not-taken to strongly-taken. The predictor changes states gradually, requiring two consecutive opposite outcomes to flip prediction direction. This added stability significantly improves accuracy for loop branches and reduces incorrect flips caused by one-time anomalies.","notes_text":"","keywords":["2-bit predictor","saturating counter","state machine","branch prediction","loop behavior","accuracy improvement"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"2-Bit Saturating Predictor","importance_score":10,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit14_Pipeline","slide_number":56,"chunk_index":0,"title":"Summary: Branch Prediction Importance","summary":"Reviews how prediction minimizes control hazards and enables high pipeline throughput.","main_text":"This slide concludes the branch prediction section by reiterating that control hazards severely limit pipeline efficiency if branches are resolved late. Accurate prediction—whether static or dynamic—allows the pipeline to continue fetching and decoding instructions without waiting for the branch result. Dynamic predictors, especially multi-bit and history-based mechanisms, dramatically reduce misprediction penalties. The slide emphasizes that effective branch prediction is essential for modern high-performance pipelined processors.","notes_text":"","keywords":["branch prediction summary","control hazards","pipeline throughput","misprediction cost","dynamic predictors","performance"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":14,"topic":"Branch Prediction Summary","importance_score":9,"file_hash":"sha256_placeholder"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":1,"chunk_index":0,"title":"Unit 15: Superscalar CPUs","summary":"Unit 15: Superscalar CPUs slide explaining Through Static and Dynamic Scheduling","main_text":"Through Static and Dynamic Scheduling","notes_text":"","keywords":["unit","superscalar","cpus","through","static","dynamic","scheduling"],"images":[{"description":"Diagram or figure related to: Unit 15: Superscalar CPUs","labels":[],"position":{"x":0.40017366579177605,"y":0.27167541557305336,"width":0.1996527777777778,"height":0.3194444444444444}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Superscalar CPUs","importance_score":5,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":2,"chunk_index":0,"title":"Full Pipeline: 1 IPC","summary":"Full Pipeline: 1 IPC slide explaining Instruc. i+2 Machine Code Instruc i+1 operands New PC %rdx / sum I-Cache D-Cache ALU Reg. File WB (ADD) PC Decode Fetch (i+3) Decode (i+2) Exec. (i+1) Mem (JE) Write word to %rdx Update PC Use the ALU Fetch next instruc i+3 Decode instruction i+2 and fetch operands ZF OF CF SF Much better than 0.2 IPC (1 instr. / 5 clock cycles) without pipeline Can we do better than 1 IPC? Yes!","main_text":"Instruc. i+2 Machine Code Instruc i+1 operands New PC %rdx / sum I-Cache D-Cache ALU Reg. File WB (ADD) PC Decode Fetch (i+3) Decode (i+2) Exec. (i+1) Mem (JE) Write word to %rdx Update PC Use the ALU Fetch next instruc i+3 Decode instruction i+2 and fetch operands ZF OF CF SF Much better than 0.2 IPC (1 instr. / 5 clock cycles) without pipeline Can we do better than 1 IPC? Yes!","notes_text":"","keywords":["ipc","pc","alu","wb","add","je","zf","of","cf","sf"],"images":[],"layout":{"num_text_boxes":26,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Superscalar CPUs","importance_score":5,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":3,"chunk_index":0,"title":"Superscalar CPU","summary":"Superscalar CPU slide explaining When airplanes broke the sound barrier we said they were supersonic When a CPU can complete more than 1 IPC we say that it is superscalar (k-way superscalar = up to k IPC) Superscalar CPUs exploit instruction-level parallelism (ILP) Execution of instructions of a single program/thread in parallel Other forms of parallelism: execute threads on different cores (thread-level), use vector instructions to process multiple data in parallel (data-level, SIMD).","main_text":"When airplanes broke the sound barrier we said they were supersonic When a CPU can complete more than 1 IPC we say that it is superscalar (k-way superscalar = up to k IPC) Superscalar CPUs exploit instruction-level parallelism (ILP) Execution of instructions of a single program/thread in parallel Other forms of parallelism: execute threads on different cores (thread-level), use vector instructions to process multiple data in parallel (data-level, SIMD)","notes_text":"","keywords":["superscalar","cpu","ipc","ilp","instruction-level","parallelism","thread-level","simd","data-level"],"images":[{"description":"Diagram or figure related to: Superscalar CPU","labels":[],"position":{"x":0.13480832910276073,"y":0.2290909090909091,"width":0.7303833417944785,"height":0.6545454545454545}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Superscalar CPUs","importance_score":5,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":4,"chunk_index":0,"title":"Instruction-Level Parallelism","summary":"Instruction-Level Parallelism slide explaining A program defines a sequential ordering of instructions, but in reality many instructions can be executed in parallel. ILP = finding instructions of a program/thread that can be executed in parallel, out-of-order with respect to the original execution. Not always possible to reorder / parallelize! Data Dependencies: Some instructions need data produced by others (“read after write”, RAW). Control Hazards: Cannot execute instructions that are after a conditional jump (unless we have a way to “undo”).","main_text":"A program defines a sequential ordering of instructions, but in reality many instructions can be executed in parallel. ILP = finding instructions of a program/thread that can be executed in parallel, out-of-order with respect to the original execution. Not always possible to reorder / parallelize! Data Dependencies: Some instructions need data produced by others (“read after write”, RAW) Control Hazards: Cannot execute instructions that are after a conditional jump (unless we have a way to “undo”)","notes_text":"","keywords":["instruction-level","parallelism","ilp","out-of-order","data","dependencies","raw","control","hazards"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Instruction-Level Parallelism","importance_score":5,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":5,"chunk_index":0,"title":"Example: Parallel Scheduling","summary":"Example: Parallel Scheduling slide explaining ld 0(%r8), %r9 and %r10, %r11 or %r11, %r13 sub %r14, %r15 add %r10, %r12 je $0, %r12, L1 xor %r15, %rax write %r11 read %r11 write %r15 read %r15 write %r12 read %r12 LD ADD SUB AND JE XOR OR Dependency Graph Note: Our CPU has special “compare & jump” instructions, e.g., “je $0,%r12,L1” jumps to L1 if %r12 is equal to 0.","main_text":"ld 0(%r8), %r9 and %r10, %r11 or %r11, %r13 sub %r14, %r15 add %r10, %r12 je $0, %r12, L1 xor %r15, %rax write %r11 read %r11 write %r15 read %r15 write %r12 read %r12 LD ADD SUB AND JE XOR OR Dependency Graph Note: Our CPU has special “compare & jump” instructions, e.g., “je $0,%r12,L1” jumps to L1 if %r12 is equal to 0","notes_text":"","keywords":["parallel","scheduling","dependency","graph","ld","add","sub","and","je","xor"],"images":[{"description":"Diagram or figure related to: Example: Parallel Scheduling","labels":[],"position":{"x":0.14785745219730942,"y":0.17173913043478262,"width":0.70443168284629,"height":0.7120000000000001}}],"layout":{"num_text_boxes":5,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Instruction-Level Parallelism","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":6,"chunk_index":0,"title":"Dynamic vs. Static Scheduling","summary":"Dynamic vs. Static Scheduling slide explaining Dynamic Scheduling: The compiler produces a sequence instructions, reordered by the CPU at runtime to execute instructions in parallel using its independent execution units. Static Scheduling: The compiler knows about CPU HW units and decides at compile time the parallel execution schedule. Static scheduling is difficult to compile and cannot exploit runtime events like cache misses.","main_text":"Dynamic Scheduling The compiler produces a sequence instructions, reordered by the CPU at runtime to execute instructions in parallel using its independent execution units (HW for integer ops, float ops, memory load/store) Static Scheduling The compiler knows about CPU HW units and decides (at compile time) the parallel execution schedule Very difficult to write compilers, cannot exploit runtime events of CPU (e.g., do something during a cache miss)","notes_text":"","keywords":["dynamic","static","scheduling","compiler","runtime","execution","units","cache","miss"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Static Scheduling and VLIW","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":7,"chunk_index":0,"title":"Static Scheduling","summary":"Static Scheduling slide explaining The dependency graph example is scheduled across multiple execution units over three clock cycles; instructions are placed to respect RAW hazards and maximize parallel issue.","main_text":"ld 0(%r8), %r9 and %r10, %r11 or %r11, %r13 sub %r14, %r15 add %r10, %r12 je $0, %r12, L1 xor %r15, %rax write %r11 read %r11 write %r15 read %r15 write %r12 read %r12 LD ADD SUB AND JE XOR OR Clock Cycle 1 Clock Cycle 2 Clock Cycle 3 Execution Unit 1 Execution Unit 2 Execution Unit 3 Execution Unit 4 $ Static Scheduling","notes_text":"","keywords":["static","scheduling","clock","cycle","execution","unit","ld","add","sub","je"],"images":[{"description":"Diagram or figure related to: Static Scheduling","labels":[],"position":{"x":0.1231084142394822,"y":0.14973958333333334,"width":0.7537831715210356,"height":0.7486979166666666}}],"layout":{"num_text_boxes":8,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Static Scheduling and VLIW","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":8,"chunk_index":0,"title":"VLIW CPUs","summary":"VLIW CPUs slide explaining Very Long Instruction Word machines issue multi-instruction packets per cycle, one per execution unit; the compiler forms these packets and reorders code to achieve up to k IPC with full scheduling.","main_text":"Very Long Instruction Words Issue packets containing many instructions, one for each execution unit of the CPU (LD, AND, SUB, ADD) (—, OR, —, JE) (—, —, XOR, —) The compiler reorders and organizes the instructions in packets. The CPU fetches one packet during each clock cycle and issues its instructions into a pipeline k instr/packet ⇒ k IPC (with full pipeline and full schedule)","notes_text":"","keywords":["vliw","very","long","instruction","words","issue","packets","compiler","ipc"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Static Scheduling and VLIW","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":9,"chunk_index":0,"title":"Example: 2-Way VLIW CPU","summary":"Example: 2-Way VLIW CPU slide explaining Architecture with separate integer/branch and load/store slots, wide register file access, and an address calculation unit enabling parallel ALU and memory ops in one packet.","main_text":"One issue slot for INT/BRANCH operations & another for LD/ST instructions I-Cache reads out an entire issue packet (1 or 2 instructions) HW is added to allow many registers to be accessed at one time Additional “Address Calculation Unit” (just a simple adder) to add offset in parallel with integer ALU calculations (e.g., may have ld & add in a packet) Issue Packet = More than 1 instruction Integer Slot LD/ST Slot","notes_text":"","keywords":["vliw","2-way","issue","packet","integer","branch","ld/st","address","calc"],"images":[{"description":"Diagram or figure related to: Example: 2-Way VLIW CPU","labels":[],"position":{"x":0.0862890249194333,"y":0.2706388888888889,"width":0.32708333333333334,"height":0.53125}},{"description":"Diagram or figure related to: Example: 2-Way VLIW CPU","labels":[],"position":{"x":0.5087070782030702,"y":0.2706388888888889,"width":0.3270833333333333,"height":0.53125}}],"layout":{"num_text_boxes":6,"num_images":2,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Static Scheduling and VLIW","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":10,"chunk_index":0,"title":"Scheduling Rules","summary":"Scheduling Rules slide explaining Rules for VLIW issue packets: no forwarding within a packet, full forwarding to later packets, and a mandatory one-cycle stall after a load when the next instruction depends on it.","main_text":"1. No forwarding between instructions in an issue packet 2. Full forwarding to instructions that follow (behind in the pipeline) 3. 1 stall cycle necessary when LD is followed by a dependent instr. (we need a clock cycle to retrieve data from cache)","notes_text":"","keywords":["scheduling","rules","forwarding","issue","packet","stall","ld","dependent"],"images":[{"description":"Diagram or figure related to: Scheduling Rules","labels":[],"position":{"x":0.11613654223968567,"y":0.17447916666666666,"width":0.7677269155206287,"height":0.55}}],"layout":{"num_text_boxes":4,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Static Scheduling and VLIW","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":11,"chunk_index":0,"title":"Example: Rules 1 & 2","summary":"Example: Rules 1 & 2 slide explaining Shows RAW vs RAR/WAR cases demonstrating that stores in the same packet can’t see results of adds, so dependent stores must be scheduled in later packets.","main_text":"In the schedule at the top, st %r9,0(%rdi) will not receive the new value of %r9 produced by add $5, %r9! It will use the old value… (no forwarding between instructions in an issue packet) Since the intent of the program is to store the new value, we need to schedule st in the next packet (example at the bottom) The new value will be forwarded on the pipeline (full forwarding to instructions that follow) Original program (RAW dependency) In this example, st %r9,0(%rdi) should use the old value of %r9 (before the change by add). We can schedule st in a previous packet (top schedule) In the same packet as add, since it won’t see the change anyway (bottom schedule) Original program (RAR/WAR dependency)","notes_text":"","keywords":["raw","rar","war","rules","forwarding","packet","store","add"],"images":[{"description":"Diagram or figure related to: Example: Rules 1 & 2","labels":[],"position":{"x":0.11912569395442359,"y":0.1171875,"width":0.7617486120911528,"height":0.796875}}],"layout":{"num_text_boxes":7,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Data Hazards and Register Renaming","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":12,"chunk_index":0,"title":"Example: Rule 3","summary":"Example: Rule 3 slide explaining Demonstrates the stall requirement after a load when the immediately following instruction is RAW-dependent on the loaded value.","main_text":"In this example, ld 0(%rdi), %r9 reads a value from the cache which is needed by add $5, %r9 We cannot schedule add in the packet immediately after ld (1 stall cycle necessary when LD is followed by a dependent instr.) We have to skip at least one clock cycle or more (bottom schedule) Original program (RAW dependency)","notes_text":"","keywords":["rule","stall","load","raw","dependent","packet","cache"],"images":[{"description":"Diagram or figure related to: Example: Rule 3","labels":[],"position":{"x":0.09914169863072966,"y":0.2265625,"width":0.8017166027385407,"height":0.654296875}}],"layout":{"num_text_boxes":5,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Static Scheduling and VLIW","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":13,"chunk_index":0,"title":"Example: Scheduling","summary":"Example: Scheduling slide explaining Presents a loop in C and assembly and a static schedule achieving IPC≈1.2 without changing code, by moving independent instructions into parallel issue packets.","main_text":"# %rdi = A # %esi = n = # of iterations L1: ld 0(%rdi),%r9 add $5,%r9 st %r9,0(%rdi) add $4,%rdi add $-1,%esi jne $0,%esi,L1 void f1(int *A, int n) { do { *A += 5; A++; n--; } while (n != 0); } w/o modifying original code but with code movement IPC = 6 instrucs. / 5 cycles = 1.2","notes_text":"","keywords":["scheduling","ipc","loop","code","movement","issue","packet","assembly"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Static Scheduling and VLIW","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":14,"chunk_index":0,"title":"Example: Execution On The Pipeline","summary":"Example: Execution On The Pipeline slide explaining Shows how the scheduled packets flow through integer and load/store slots, with nops inserted to satisfy forwarding and stall rules.","main_text":"Issue Packet = More than 1 instruction Integer Slot LD/ST Slot INT/BRANCH LD/ST add $4,%rdi st %r9,0(%rdi) ld 0(%rdi),%r9 add $-1,%esi add $5,%r9 jne $0,%esi,L1 nop nop nop nop","notes_text":"","keywords":["pipeline","issue","packet","integer","ld/st","nops","forwarding","stall"],"images":[{"description":"Diagram or figure related to: Example: Execution On The Pipeline","labels":[],"position":{"x":0.06918710230518818,"y":0.16927083333333334,"width":0.8616257953896237,"height":0.6760416666666668}}],"layout":{"num_text_boxes":4,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Static Scheduling and VLIW","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":15,"chunk_index":0,"title":"Example: Better Schedule","summary":"Example: Better Schedule slide explaining Compares original vs modified code plus movement, improving IPC from ~1.2 to ~1.5 by changing instructions and packing them more efficiently.","main_text":"w/o modifying original code but with code movement IPC = 6 instrucs. / 5 cycles = 1.2 w/ modifications and code movement IPC = 6 instrucs. / 4 cycle = 1.5","notes_text":"","keywords":["better","schedule","ipc","code","movement","modifications"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Static Scheduling and VLIW","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":16,"chunk_index":0,"title":"Loop Unrolling","summary":"Loop Unrolling slide explaining Introduces unrolling to expose ILP across independent loop iterations, reducing branch/overhead and enabling more parallel scheduling.","main_text":"Often not enough ILP in a single iteration (body) of a loop However, different iterations of the loop are often independent and can thus be run in parallel This parallelism can be exposed in static issue machines via loop unrolling Copy the body of the loop k times and iterate only n/k times Instructions from different body iterations can be run in parallel void f1(int *A, int n) { // assume n is a multiple of 4 do { // loop unrolled 4 times *A += 5; *(A+1) += 5; *(A+2) += 5; *(A+3) += 5; A += 4; n -= 4; } while (n != 0); }","notes_text":"","keywords":["loop","unrolling","ilp","iterations","parallel","static","issue","machines"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Static Scheduling and VLIW","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":17,"chunk_index":0,"title":"Loop Unrolling: Instructions","summary":"Loop Unrolling: Instructions slide explaining Shows original vs unrolled assembly, highlighting reduced overhead but shared temp register %r9 creating name dependencies.","main_text":"Original Code Unrolled Code Side effect of unrolling: less overhead (branches & counter/pointer updates) We cannot execute the unrolled instructions in parallel yet … Data dependency: they all use the register %r9 to store a temporary value! We can use different registers for each unrolled body of the for loop We must use 12-16=-4 as the last st offset because of the update to %rdi","notes_text":"","keywords":["unrolled","assembly","overhead","branches","name","dependencies","register","r9"],"images":[],"layout":{"num_text_boxes":3,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Data Hazards and Register Renaming","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":18,"chunk_index":0,"title":"Loop Unrolling: Register Renaming","summary":"Loop Unrolling: Register Renaming slide explaining Uses distinct registers (r9–r12) for each unrolled iteration to remove false dependencies, yielding IPC≈1.875.","main_text":"# %rdi = A # %esi = n = # of iterations L1: ld 0(%rdi), %r9 add $5, %r9 st %r9,0(%rdi) ld 4(%rdi), %r10 add $5, %r10 st %r10,4(%rdi) ld 8(%rdi), %r11 add $5, %r11 st %r11,8(%rdi) ld 12(%rdi), %r12 add $5, %r12 st %r12,12(%rdi) add $16,%rdi add $-4,%esi jne $0,%esi,L1 With Loop Unrolling and Register Renaming: IPC = 15 instrucs. / 8 cycle = 1.875","notes_text":"","keywords":["register","renaming","unrolling","ipc","false","dependencies","r9","r10","r11","r12"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Data Hazards and Register Renaming","importance_score":7,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":19,"chunk_index":0,"title":"More on Data Hazards: WAR & WAW","summary":"More on Data Hazards: WAR & WAW slide explaining Defines WAR and WAW as anti-dependencies (name hazards) that arise under out-of-order execution and are solved via register renaming.","main_text":"When executing code out-of-order (e.g., the unrolled bodies in parallel), we must deal with WAR (Write-After-Read) and WAW (Write-After-Write) hazards in addition to RAW hazards In loop unrolling, there were only WAR/WAW hazards between unrolled bodies These are not true hazards (no real dependency) but simply conflicts because we want to use the same register… called “name dependencies” or anti-dependences!","notes_text":"","keywords":["war","waw","raw","anti-dependencies","name","hazards","out-of-order","renaming"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Data Hazards and Register Renaming","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":20,"chunk_index":0,"title":"Summary: Data Dependency Hazards","summary":"Summary: Data Dependency Hazards slide explaining Summarizes RAW as true dependencies, WAR/WAW as anti-dependencies fixed by renaming, and RAR as harmless.","main_text":"RAW = Only real data dependency Must be respected in terms of code movement and ordering Forwarding reduces latency of dependent instructions WAW and WAR = Anti-dependencies Solved by using register renaming RAR = No issues / dependencies","notes_text":"","keywords":["raw","war","waw","rar","forwarding","latency","renaming","dependencies"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Data Hazards and Register Renaming","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":21,"chunk_index":0,"title":"Loop Unrolling: Limitations","summary":"Loop Unrolling: Limitations slide explaining Notes tradeoffs: larger code size, more registers needed, and memory hazards when addresses can’t be disambiguated, limiting safe reordering.","main_text":"Loop unrolling increases code size Register renaming may require more hardware registers Must have independence between loop bodies Dependencies can occur through memory (memory hazard) If the compiler cannot disambiguate memory references (i.e., ensure that they use different addresses), it cannot reorder ld/st (RAW), st/ld (WAR), st/st (WAW) We moved the 2nd ‘ld’ up to enhance performance... Is it equivalent? No! Need to wait for st %edx,0(%rsi)!!","notes_text":"","keywords":["limitations","code","size","memory","hazard","disambiguate","reorder","raw","war","waw"],"images":[{"description":"Diagram or figure related to: Loop Unrolling: Limitations","labels":[],"position":{"x":0.0957664995822894,"y":0.47265625,"width":0.808466, "height":0.39}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Static Scheduling and VLIW","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":22,"chunk_index":0,"title":"Static Scheduling: Summary","summary":"Static Scheduling: Summary slide explaining Recaps compiler-led reordering/unrolling for multi-issue CPUs, with pros (simpler HW) and cons (complex compilers, recompilation, no runtime adaptation).","main_text":"Compiler is in charge of reordering, renaming, unrolling original program code to achieve better performance CPU is designed to fetch/decode/execute multiple instructions per cycle in the order determined by the compiler HW can be simpler and faster/smaller (more cores, higher clock rate) Requires recompilation for different HW architectures Complex compilers Cannot exploit runtime events (e.g., do something else during cache miss) Itanium2 Case Study 6 ways (IPC <= 6) Units: 6 Int, 4 LD/ST, 3 Branch, 2 FP 128 64-bit registers 128 float registers 12 MB L3 cache Don Knuth quote criticizing compiler difficulty.","notes_text":"","keywords":["static","scheduling","compiler","multi-issue","itanium","ipc","recompilation","runtime"],"images":[{"description":"Diagram or figure related to: Static Scheduling: Summary","labels":[],"position":{"x":0.06024115384207595,"y":0.56875,"width":0.8795176923158481,"height":0.334375}}],"layout":{"num_text_boxes":4,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Static Scheduling and VLIW","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":23,"chunk_index":0,"title":"Dynamic Scheduling","summary":"Dynamic Scheduling slide explaining Introduces dynamic scheduling as HW-driven reordering to overlap independent instructions and hide stalls, especially from memory latency.","main_text":"Dynamic Scheduling","notes_text":"","keywords":["dynamic","scheduling"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":5,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":24,"chunk_index":0,"title":"Overcoming Memory Latency","summary":"Overcoming Memory Latency slide explaining Shows that cache misses force in-order pipelines to stall all following instructions, potentially for hundreds of cycles.","main_text":"What happens to instruction execution if we have a cache miss? All instructions behind us need to stall! Could take potentially hundreds of clock cycles to fetch the data ld 0(%rdi),%rcx Miss STALL STALL STALL","notes_text":"","keywords":["memory","latency","cache","miss","stall","in-order"],"images":[{"description":"Diagram or figure related to: Overcoming Memory Latency","labels":[],"position":{"x":0.055,"y":0.28,"width":0.89,"height":0.5}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":25,"chunk_index":0,"title":"Out-of-Order CPUs","summary":"Out-of-Order CPUs slide explaining Explains executing independent instructions past stalled ones by building dependency graphs at runtime, known as out-of-order (dynamic) scheduling.","main_text":"Idea: Processor can find dependencies as instructions are fetched/decoded and execute independent instructions that come after stalled instructions Known as Out-of-Order Execution or Dynamic Scheduling HW will determine the “dependency” graph at runtime and as long as an instruction isn't waiting for an earlier instruction, let it execute!","notes_text":"","keywords":["out-of-order","dynamic","scheduling","dependencies","runtime","graph","stall"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":26,"chunk_index":0,"title":"Solution: Tomasulo’s Algorithm","summary":"Solution: Tomasulo’s Algorithm slide explaining Presents a simplified Tomasulo block diagram with instruction queues, functional units, register status tracking, and a common data bus for dynamic scheduling.","main_text":"Block Diagram adapted from Prof. Dubois. Components: I-Cache, Register Status Table, Instruction Queue, Issue Unit, Dispatch, Integer/Branch unit, D-Cache, Div, Mul, per-unit queues (Int, L/S, Div, Mult), Common Data Bus. Fetch multiple instructions per cycle in program order. Decode/dispatch tracking dependencies. Instructions wait in queues until unit free and operands ready. Multiple results broadcast/written back per cycle. Register status tracks latest producer to solve RAW/WAR/WAW. Instructions with all data may execute out of order.","notes_text":"","keywords":["tomasulo","algorithm","instruction","queue","register","status","common","data","bus","dynamic"],"images":[{"description":"Diagram or figure related to: Solution: Tomasulo’s Algorithm","labels":[],"position":{"x":0.05,"y":0.13,"width":0.9,"height":0.74}}],"layout":{"num_text_boxes":6,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":27,"chunk_index":0,"title":"Execution (1/10)","summary":"Execution (1/10) slide explaining Step-by-step Tomasulo dispatch where the first load is tagged and placed in a queue while updating the register status table.","main_text":"Shows initial dispatch of instruction 1: ld 0(%rdx), %r8. The LD is assigned unique tag 1 and updates the register status table entry for r8 to indicate a pending producer. Later instructions will reference tag 1 for r8.","notes_text":"","keywords":["execution","dispatch","load","tag","register","status","table","tomasulo"],"images":[{"description":"Diagram or figure related to: Execution (1/10)","labels":[],"position":{"x":0.05,"y":0.12,"width":0.9,"height":0.76}}],"layout":{"num_text_boxes":7,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":28,"chunk_index":0,"title":"Execution (2/10)","summary":"Execution (2/10) slide explaining Dispatches an ADD dependent on the earlier LD, assigns tag 2, and updates r8’s latest-producer entry.","main_text":"Instruction 2 (add $1,%r8) dispatches with tag 2. It sees r8 is being produced by tag 1 so it waits for that result, then becomes the latest producer of r8 by updating the register status table to tag 2.","notes_text":"","keywords":["execution","add","dependent","tag","latest","producer","r8"],"images":[{"description":"Diagram or figure related to: Execution (2/10)","labels":[],"position":{"x":0.05,"y":0.12,"width":0.9,"height":0.76}}],"layout":{"num_text_boxes":8,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":29,"chunk_index":0,"title":"Execution (3/10)","summary":"Execution (3/10) slide explaining Dispatches a store waiting on the ADD result via tag matching; stores don’t update the register status table.","main_text":"Instruction 3 (st %r8,0(%rdx)) dispatches. It checks RST, sees r8 produced by tag 2, and waits for that value. Since ST writes memory not a register, it does not change RST.","notes_text":"","keywords":["execution","store","rst","tag","waiting","memory"],"images":[{"description":"Diagram or figure related to: Execution (3/10)","labels":[],"position":{"x":0.05,"y":0.12,"width":0.9,"height":0.76}}],"layout":{"num_text_boxes":8,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":30,"chunk_index":0,"title":"Execution (4/10)","summary":"Execution (4/10) slide explaining Explains that the cache-missing LD stalls dependent ADD/ST while independent later instructions may proceed.","main_text":"Because instruction 1 LD misses in cache, it may take hundreds of cycles. Dependent instructions 2 (ADD) and 3 (ST) stall waiting for tagged results, and cannot execute until LD completes.","notes_text":"","keywords":["execution","cache","miss","stall","dependent","ld","add","st"],"images":[{"description":"Diagram or figure related to: Execution (4/10)","labels":[],"position":{"x":0.05,"y":0.12,"width":0.9,"height":0.76}}],"layout":{"num_text_boxes":8,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":31,"chunk_index":0,"title":"Execution (5/10)","summary":"Execution (5/10) slide explaining Dispatches the next LD/ADD/ST sequence; RST entries update to new tags while stalled ops wait.","main_text":"A second LD (tag 4), ADD (tag 5), and ST (tag 6) are dispatched. The LD sets r9’s producer to 4, then ADD updates it to 5, showing how RST tracks latest producers across multiple in-flight instructions.","notes_text":"","keywords":["execution","dispatch","rst","tags","producer","in-flight"],"images":[{"description":"Diagram or figure related to: Execution (5/10)","labels":[],"position":{"x":0.05,"y":0.12,"width":0.9,"height":0.76}}],"layout":{"num_text_boxes":9,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":32,"chunk_index":0,"title":"Execution (6/10)","summary":"Execution (6/10) slide explaining Shows that LD #4 can execute out of order while earlier miss is pending, assuming the cache supports multiple outstanding misses.","main_text":"LD #4 has all operands and its cache request can proceed even though LD #1 is stalled. This demonstrates out-of-order execution and memory-level parallelism.","notes_text":"","keywords":["execution","out-of-order","ld","cache","miss","parallelism"],"images":[{"description":"Diagram or figure related to: Execution (6/10)","labels":[],"position":{"x":0.05,"y":0.12,"width":0.9,"height":0.76}}],"layout":{"num_text_boxes":9,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":33,"chunk_index":0,"title":"Execution (7/10)","summary":"Execution (7/10) slide explaining Broadcast of LD #4’s result enables dependent ADD #5; meanwhile newer independent instructions are dispatched.","main_text":"Once LD #4 reads from cache, it broadcasts its value on the common data bus. ADD #5 captures it and can execute; subsequent instructions continue to dispatch during the stall window.","notes_text":"","keywords":["execution","broadcast","common","data","bus","dependent","add"],"images":[{"description":"Diagram or figure related to: Execution (7/10)","labels":[],"position":{"x":0.05,"y":0.12,"width":0.9,"height":0.76}}],"layout":{"num_text_boxes":9,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":34,"chunk_index":0,"title":"Execution (8/10)","summary":"Execution (8/10) slide explaining ADD #5 executes after receiving operands, broadcasts its result, and clears r9’s RST entry as the latest producer commits.","main_text":"ADD #5 now has operands, executes, broadcasts tag/result, enabling ST #6 to take the value. The true register r9 is updated and its RST entry is reset to nil.","notes_text":"","keywords":["execution","add","broadcast","st","rst","commit"],"images":[{"description":"Diagram or figure related to: Execution (8/10)","labels":[],"position":{"x":0.05,"y":0.12,"width":0.9,"height":0.76}}],"layout":{"num_text_boxes":9,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":35,"chunk_index":0,"title":"Execution (9/10)","summary":"Execution (9/10) slide explaining ST #6 and independent pointer update ADD #7 execute out of order without WAR hazards due to value capture at dispatch.","main_text":"ST #6 can execute since it has data. ADD #7 updating rdi is independent; ST already captured the old rdi address so there is no WAR hazard.","notes_text":"","keywords":["execution","store","war","hazard","independent","address","capture"],"images":[{"description":"Diagram or figure related to: Execution (9/10)","labels":[],"position":{"x":0.05,"y":0.12,"width":0.9,"height":0.76}}],"layout":{"num_text_boxes":9,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":36,"chunk_index":0,"title":"Execution (10/10)","summary":"Execution (10/10) slide explaining When the original miss resolves, LD #1 broadcasts, allowing its dependent ADD #2 then ST #3 to complete.","main_text":"MISS RESOLVED: Once LD #1 completes, it broadcasts its value; dependent ADD #2 executes next and then ST #3 can proceed. Shows final completion order under Tomasulo.","notes_text":"","keywords":["execution","miss","resolved","broadcast","dependent","completion"],"images":[{"description":"Diagram or figure related to: Execution (10/10)","labels":[],"position":{"x":0.05,"y":0.12,"width":0.9,"height":0.76}}],"layout":{"num_text_boxes":9,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":37,"chunk_index":0,"title":"Dynamic Scheduling: Summary","summary":"Dynamic Scheduling: Summary slide explaining Summarizes HW-driven runtime dependency detection and out-of-order execution to keep units busy despite stalls.","main_text":"Burden of scheduling code for parallelism is placed on the CPU HW and is performed dynamically at runtime (as the program runs) Goal is for HW to determine data dependencies and let independent instructions execute even if previous instructions are stalled","notes_text":"","keywords":["dynamic","scheduling","runtime","dependencies","out-of-order","hw"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":38,"chunk_index":0,"title":"Speculative Execution","summary":"Speculative Execution slide explaining Introduces control dependencies, basic blocks, and why branch prediction is needed to extend out-of-order scheduling across blocks.","main_text":"Control dependencies really limit our window of possible instructions to overlap Cannot reorder an instruction if we are unsure it should execute Basic Block = Sequence of instructions that will always be executed together No conditional branches out No branch targets coming in Also called straight-line code Average size: 5–7 instructions Instructions in a basic block can be overlapped if there are no data dependencies","notes_text":"","keywords":["speculative","execution","control","dependencies","basic","block","branch","prediction"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Speculative Execution and Reorder Buffer","importance_score":7,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":39,"chunk_index":0,"title":"Issue: Branch Prediction","summary":"Issue: Branch Prediction slide explaining Explains speculative execution across basic blocks and the need to undo completed wrong-path instructions.","main_text":"To apply out-of-order execution across basic blocks we need to predict outcomes of conditional jumps (speculative execution). If predictions are wrong, we can’t just flush a pipeline because wrong-path instructions may have completed. Need a way to undo effects.","notes_text":"","keywords":["branch","prediction","speculative","execution","wrong-path","undo","basic","blocks"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Speculative Execution and Reorder Buffer","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":40,"chunk_index":0,"title":"Issue: Hardware Exceptions","summary":"Issue: Hardware Exceptions slide explaining Shows problem of exceptions in older instructions after younger ones complete; motivates speculative execution with commit/rollback.","main_text":"If an exception (e.g., page fault) occurs in an earlier instruction after later ones completed, restarting can cause younger instructions to execute twice. Solution: speculative execution with commit unit and ability to rollback.","notes_text":"","keywords":["hardware","exceptions","page","fault","rollback","commit","speculative"],"images":[{"description":"Diagram or figure related to: Issue: Hardware Exceptions","labels":[],"position":{"x":0.06,"y":0.27,"width":0.88,"height":0.55}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Speculative Execution and Reorder Buffer","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":41,"chunk_index":0,"title":"Reorder Buffer","summary":"Reorder Buffer slide explaining Introduces ROB as a commit unit buffering out-of-order results and committing in-order to support rollback on exceptions/mispredictions.","main_text":"Re-Order Buffer (ROB): temporarily buffers results of instructions until earlier (older) instructions complete and write back in order. Provides in-order commit, queues for functional units, and allows flushing younger entries on error.","notes_text":"","keywords":["reorder","buffer","rob","commit","in-order","out-of-order","flush"],"images":[{"description":"Diagram or figure related to: Reorder Buffer","labels":[],"position":{"x":0.05,"y":0.12,"width":0.9,"height":0.74}}],"layout":{"num_text_boxes":6,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Speculative Execution and Reorder Buffer","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":42,"chunk_index":0,"title":"ROB: Operation","summary":"ROB: Operation slide explaining Details tail allocation, result buffering, head commit, and flushing younger instructions on misprediction/exception.","main_text":"ROB tail entry allocated for each issued instruction, keeping program order. Completed results are forwarded and stored in ROB until reaching head. Only fully completed head instructions commit. On exception/misprediction, flush everyone behind the faulting branch/instruction and restart on correct path.","notes_text":"","keywords":["rob","tail","head","commit","flush","misprediction","exception"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Speculative Execution and Reorder Buffer","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":43,"chunk_index":0,"title":"Speculative Execution (ROB Flushing)","summary":"Speculative Execution (ROB Flushing) slide explaining Shows ROB states for predicted branches and pipeline flush/recovery when a branch is mispredicted.","main_text":"Predict branches and execute most likely path. If mispredicted, simply flush ROB entries after the branch and refill with correct path. Requires good prediction to be worthwhile.","notes_text":"","keywords":["speculative","execution","rob","flush","branch","prediction","recovery"],"images":[{"description":"Diagram or figure related to: Speculative Execution (ROB Flushing)","labels":[],"position":{"x":0.03,"y":0.08,"width":0.94,"height":0.8}}],"layout":{"num_text_boxes":4,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Speculative Execution and Reorder Buffer","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":44,"chunk_index":0,"title":"Wrong-Path Execution / Exceptions & Mispredictions","summary":"Wrong-Path Execution / Exceptions & Mispredictions slide explaining Connects ROB flushing to handling divide-by-zero/page faults and branch mispredictions safely.","main_text":"ROB allows throwing away younger instructions after an exception (e.g., DIV by zero, ST page fault) and replaying them. For speculative execution, ROB enables discarding wrong-path results after a mispredicted JEQ.","notes_text":"","keywords":["wrong-path","exceptions","mispredictions","rob","replay","discard"],"images":[{"description":"Diagram or figure related to: Wrong-Path Execution / Exceptions & Mispredictions","labels":[],"position":{"x":0.05,"y":0.13,"width":0.9,"height":0.72}}],"layout":{"num_text_boxes":5,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Speculative Execution and Reorder Buffer","importance_score":8,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":45,"chunk_index":0,"title":"Case 1: Correctly Predicted Branch","summary":"Case 1: Correctly Predicted Branch slide explaining Shows normal ROB/queue commit when JEQ prediction is correct and younger ops can retire in order once ready.","main_text":"If the JEQ is correctly predicted, normal execution proceeds and instructions after the JEQ can commit when ready. Functional unit queues and ROB manage out-of-order completion with in-order retirement.","notes_text":"","keywords":["case","correctly","predicted","branch","jeq","rob","commit"],"images":[{"description":"Diagram or figure related to: Case 1: Correctly Predicted Branch","labels":[],"position":{"x":0.04,"y":0.1,"width":0.92,"height":0.78}}],"layout":{"num_text_boxes":6,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Speculative Execution and Reorder Buffer","importance_score":7,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":46,"chunk_index":0,"title":"Case 2: Mispredicted Branch","summary":"Case 2: Mispredicted Branch slide explaining Shows that when JEQ is wrong, younger ROB and queue entries are flushed and execution restarts at correct target.","main_text":"If the JEQ is mispredicted, all later (younger) instructions in the ROB and functional unit queues are flushed. Then fetch and execution resume at the correct target basic block.","notes_text":"","keywords":["case","mispredicted","branch","flush","rob","resume","target"],"images":[{"description":"Diagram or figure related to: Case 2: Mispredicted Branch","labels":[],"position":{"x":0.04,"y":0.1,"width":0.92,"height":0.78}}],"layout":{"num_text_boxes":6,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Speculative Execution and Reorder Buffer","importance_score":7,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":47,"chunk_index":0,"title":"Attacks to Out-of-Order Execution","summary":"Attacks to Out-of-Order Execution slide explaining Introduces Meltdown and Spectre as vulnerabilities caused by speculative execution and cache side effects.","main_text":"Meltdown & Spectre Meltdown allows a program to access memory/secrets of other programs and OS by exploiting out-of-order execution before permission checks. Spectre tricks correct programs into speculatively accessing secret-dependent array indices, leaving cache traces. Both exploit that speculation/caching effects are not undone.","notes_text":"","keywords":["attacks","out-of-order","meltdown","spectre","speculation","cache","side-channel"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Security Attacks on Speculation","importance_score":7,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":48,"chunk_index":0,"title":"Meltdown Idea","summary":"Meltdown Idea slide explaining Step-by-step attack using exception delay plus cache probing to infer kernel data via timing of a shared probe array.","main_text":"Start processes A and B sharing probe array (4096*256 bytes). A flushes cache. B triggers exception (slow handler) then speculatively reads secret kernel data and uses it to index probe array. After B dies, A reads all pages; the fastest page reveals the secret index. Code sketch uses clflush, access_kernel_mem, and timing reads.","notes_text":"","keywords":["meltdown","exception","cache","probe","array","timing","kernel","secret"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Security Attacks on Speculation","importance_score":7,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":49,"chunk_index":0,"title":"Spectre Idea","summary":"Spectre Idea slide explaining Describes speculative bounds-check bypass to read arbitrary in-process data and leak it through cache timing on array2.","main_text":"Evict array2 from cache. Train branch predictor with x < array1_size. Then use x > array1_size to speculatively read arbitrary data via array1, and access array2 at secret-dependent offset. After rollback, cache state remains. Measure array2 access times to exfiltrate secret.","notes_text":"","keywords":["spectre","branch","predictor","speculative","bounds","bypass","cache","timing"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Security Attacks on Speculation","importance_score":7,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":50,"chunk_index":0,"title":"Conclusions","summary":"Conclusions slide explaining Summarizes root design issues enabling attacks and common mitigations with performance costs.","main_text":"Exploited CPU design issues: ROB does not undo cache fills; permission bits checked after speculation (Meltdown). Solutions: OS patches and hardware mitigations can cost up to ~25% performance; isolate browser tabs into separate processes; reduce JS timing resolution.","notes_text":"","keywords":["conclusions","rob","cache","permissions","mitigations","performance"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Summary / Conclusions","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":51,"chunk_index":0,"title":"Final Example #1","summary":"Final Example #1 slide explaining Practice problem about stalls/execution ordering when a DIV takes >16 cycles and writes to %rdx:%rax.","main_text":"Assume the first div instruction requires > 16 cycles to execute. Indicate which instructions will be able to execute and which will also stall. Recall divide instruction puts results in %rdx:%rax.","notes_text":"","keywords":["final","example","div","stall","ordering","rdx","rax"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":5,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":52,"chunk_index":0,"title":"Final Example #2","summary":"Final Example #2 slide explaining Practice problem about which instructions proceed under a 100-cycle load miss in all cache levels.","main_text":"Assume the first load instruction misses in all levels of cache and will take at least 100 cycles to retrieve the desired data from memory. Indicate which instructions will be able to execute and which will also stall.","notes_text":"","keywords":["final","example","load","miss","cache","stall","ordering"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Dynamic Scheduling / Out-of-Order Execution","importance_score":5,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":53,"chunk_index":0,"title":"Control Dependencies","summary":"Control Dependencies slide explaining Defines basic blocks and illustrates control limits on ILP with a branch-terminated sequence.","main_text":"This is a basic block (starts with target, ends with branch): ld 0(%r8),%r9 and %r10,%r11 L1: add %r8,%r12 or %r11,%r13 sub %r14,%r10 jeq %r12,%r14,L1 xor %r10,%r15.","notes_text":"","keywords":["control","dependencies","basic","block","branch","ilp"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Speculative Execution and Reorder Buffer","importance_score":6,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":54,"chunk_index":0,"title":"Speculative Path vs Correct Path","summary":"Speculative Path vs Correct Path slide explaining Visual comparison of ROB filling along predicted path, flushing on mispredict, and refilling on correct basic blocks.","main_text":"Graphical depiction contrasting speculative path and correct path across multiple basic blocks, showing times of prediction, misprediction discovery, flush, and recovery.","notes_text":"","keywords":["speculative","path","correct","path","flush","recovery","basic","blocks"],"images":[{"description":"Diagram or figure related to: Speculative Path vs Correct Path","labels":[],"position":{"x":0.02,"y":0.05,"width":0.96,"height":0.86}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Speculative Execution and Reorder Buffer","importance_score":7,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":55,"chunk_index":0,"title":"Correct Path Resume Example","summary":"Correct Path Resume Example slide explaining Microexample showing resume at L1 after mispredicted JEQ, with DIV/ADD/LD on the correct target.","main_text":"After flushing younger instructions behind JEQ, fetch and execution resume at L1: div %r10; add %rdx,%r10; ld %r14,0(%r13). Shows correct-path continuation after rollback.","notes_text":"","keywords":["resume","correct","path","jeq","flush","rollback","div","add","ld"],"images":[{"description":"Diagram or figure related to: Correct Path Resume Example","labels":[],"position":{"x":0.06,"y":0.18,"width":0.88,"height":0.6}}],"layout":{"num_text_boxes":4,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":15,"topic":"Speculative Execution and Reorder Buffer","importance_score":7,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit15_Superscalar","slide_number":56,"chunk_index":0,"title":"End / Transition","summary":"End / Transition slide explaining Closing slide signaling end of unit content.","main_text":"","notes_text":"","keywords":["end","transition"],"images":[],"layout":{"num_text_boxes":0,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":15,"topic":"Superscalar CPUs","importance_score":5,"file_hash":"832bcbf6416184b2c4bb4720eed39d5860057d586681310645ba438e054f42bf"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":1,"chunk_index":0,"title":"Unit 16: Cache Coherence","summary":"Introduces Unit 16 and states the core goal: keeping private multicore caches consistent with shared memory.","main_text":"Unit 16: Cache Coherence. Keeping multicore caches in sync.","notes_text":"","keywords":["cache coherence","multicore","sync","unit 16","shared memory"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":16,"topic":"Cache Coherence Overview","importance_score":5,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":2,"chunk_index":0,"title":"Parallelism Landscape","summary":"Contrasts instruction-level and thread-level parallelism and motivates multicore growth.","main_text":"Instruction-Level Parallelism (Pipeline, Dynamic Scheduling). Diminishing returns in 2000–2005: silicon and energy costs growing faster than performance, inefficient after a point. Thread-Level Parallelism: “Replicate CPU cores with same design, use multiple threads.” Works well with data parallelism (scientific computing), or with request-level parallelism (multiuser OS, web server, database). Parallelism.","notes_text":"","keywords":["parallelism","ILP","thread-level parallelism","multicore","diminishing returns"],"images":[{"description":"Visual showing a multi-core CPU with replicated cores.", "labels":["Core (w/registers)","Multi-core CPU"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":4,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":16,"topic":"Parallelism Motivation","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":3,"chunk_index":0,"title":"Thread Pools and SMP","summary":"Uses a web-server thread pool to illustrate symmetric multiprocessing with shared memory.","main_text":"Thread Pool of Apache Web Server. Get page /index.html. SMP: Symmetric Multi-Processing. Most multicore systems are SMP. Share a single memory. Using a single address space. Uniform memory access (UMA) latency from all cores. Only up to approx. 32 cores. What about AWS x1.32xlarge with 64 cores? Multi-socket! It uses 4 × (16-core CPU). Memory/address space is still shared, but latency is not uniform (NUMA, see next slide). Shared-memory: each core can access/address the entire memory; Symmetric: uniform access time.","notes_text":"","keywords":["SMP","UMA","NUMA","thread pool","shared memory","multi-socket"],"images":[{"description":"Diagram contrasting thread pool requests with an SMP multicore layout.", "labels":["Thread Pool","SMP","UMA"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":5,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":16,"topic":"SMP Basics","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":4,"chunk_index":0,"title":"SMP Cache Hierarchy","summary":"Shows private L1/L2 caches per core, shared L3, and a single shared memory address space.","main_text":"Core (w/registers), L1 Cache, L2 Cache for each core. L3 Cache. Main Memory. Addresses 0x0000 to 0xFFFF. Multi-core CPU. on-chip interconnect.","notes_text":"","keywords":["L1","L2","L3","shared memory","address space","on-chip interconnect"],"images":[{"description":"Cache hierarchy diagram: per-core L1/L2 private caches, shared L3, shared main memory.", "labels":["L1 Cache","L2 Cache","L3 Cache","Main Memory"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"Multicore Cache Organization","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":5,"chunk_index":0,"title":"DSM: Distributed Shared Memory","summary":"Introduces distributed shared memory for scalability and bandwidth, while keeping a shared address space.","main_text":"DSM: Distributed Shared-Memory. Shared-memory: each core can address the entire memory; Distributed: more bandwidth; memory faster if local. Multi-core CPU with multiple on-chip interconnects and a CPU interconnect (ring, mesh, e.g., Intel Xeon UPI or AMD Epyc Infinity Fabric) on motherboard. Main Memory Addresses 0x00000 to 0x0FFFF and 0x10000 to 0x1FFFF.","notes_text":"","keywords":["DSM","distributed shared memory","bandwidth","local memory","UPI","Infinity Fabric"],"images":[{"description":"Diagram showing DSM across multiple sockets with shared address space but non-uniform latency.", "labels":["DSM","CPU interconnect","Main Memory"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":4,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"DSM / NUMA","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":6,"chunk_index":0,"title":"Distributed Systems vs Shared Memory","summary":"Contrasts distributed systems with DSM/SMP by noting memory is not shared and communication uses networking/RPC.","main_text":"Distributed System. Memory is not shared! Nodes have different address spaces; use RPCs to exchange data. I/O subsystem and network card. TCP/IP over Ethernet.","notes_text":"","keywords":["distributed system","no shared memory","RPC","TCP/IP","address spaces"],"images":[{"description":"Diagram showing two multicore nodes connected by Ethernet, emphasizing separate address spaces.", "labels":["TCP/IP over Ethernet","Memory is not shared"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"Distributed Systems Contrast","importance_score":5,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":7,"chunk_index":0,"title":"Challenges of Parallelism: Amdahl’s Law","summary":"Explains speedup limits from finite parallel fractions using Amdahl’s law and a 64-core example.","main_text":"Challenges of Parallelism. Not enough parallelism in programs. Amdahl’s law (you can only speedup the parallel fraction): T(n) = runtime with n cores = T(1) [(parallel fraction)/n + 1 − (parallel fraction)]. Example: if 99% of the program can run in parallel, using 64 cores is only T(1)/T(n) = 1/(0.99/64 + 0.01) = 39 times faster. Solution: Find algorithms that can be parallelized better!","notes_text":"","keywords":["Amdahl’s law","speedup","parallel fraction","64 cores","runtime"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":16,"topic":"Parallel Speedup Limits","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":8,"chunk_index":0,"title":"Challenges of Parallelism: Communication Latency","summary":"Quantifies core-to-core latency effects and motivates caching of shared data.","main_text":"Challenges of Parallelism. Slow communication between cores. 35–50 clock cycles to communicate with cores in same CPU. 100–300 to communicate with cores on separate CPU chips. Example: 4 GHz, base CPI of 0.5, 100 ns delay for remote references, 0.2% of remote references; effective CPI is base CPI + penalty for remote references = 0.5 + 0.002 × 100 ns / (0.25 ns) = 1.3. Solution: Restructure programs or cache shared data.","notes_text":"","keywords":["communication latency","remote references","CPI","NUMA cost","cache shared data"],"images":[{"description":"Diagram showing cores and caches connected by on-chip interconnect, illustrating remote latency.", "labels":["on-chip interconnect"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":16,"topic":"Communication Costs","importance_score":6,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":9,"chunk_index":0,"title":"Coherence Problem in SMP Caches","summary":"Defines the coherence problem arising from private caches on different cores holding the same block.","main_text":"Coherence of SMP Caches. Coherence Problem: Threads of the same program can share memory while running on different cores (or switch core). What if two cores have the same block (same memory address) in their L1/L2 private caches? For read-only, it’s fine. What if one core writes to block 0 in its L1/L2 cache? The L1/L2 cache of the other core has stale data! Write-back policy alone isn’t safe.","notes_text":"","keywords":["coherence problem","private caches","stale data","write-back","SMP"],"images":[{"description":"Diagram of two cores with private caches and shared L3/memory, highlighting duplicated blocks.", "labels":["Core 1","Core 2","L1","L2","L3"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"Cache Coherence Motivation","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":10,"chunk_index":0,"title":"Example of Coherence Issues","summary":"Walks through a read/read/write sequence that leads to stale reads and multiple inconsistent versions of a block.","main_text":"Example of Coherence Issues. a) Core1 reads blocks 0,1,2,3. b) Core2 reads blocks 0,1,4,5. c) Core1 writes to block 0 in its L1 cache (write-back policy). d) Core2 reads block 0 in its L1 cache: data read by Core2 is stale/wrong! e) Core2 writes block 0 in its L1 cache: now we have two versions of block 0! We must propagate changes made to the local cache of one core to local caches of other cores (or invalidate them).","notes_text":"","keywords":["stale read","two versions","invalidate","propagate writes","example"],"images":[{"description":"Multi-step cache state diagram showing reads, write-back, stale data, and divergence.", "labels":["blocks 0–5","stale","invalidate"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"Coherence Failure Example","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":11,"chunk_index":0,"title":"Coherence vs. Consistency","summary":"Distinguishes what value may be read (coherence) from when it becomes visible (consistency).","main_text":"Coherence vs Consistency. Goal: “Any read of data returns the most recently written value.” Coherence = what values can be read after writes to X. Concurrent writes are serialized (e.g., during bus access) so that all cores see them in same order. Consistency = when new values of X can be read. Takes time to propagate writes to all cores. We assume to have waited for propagation.","notes_text":"","keywords":["coherence","consistency","serialization","propagation delay","visibility"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":16,"topic":"Coherence vs Consistency","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":12,"chunk_index":0,"title":"Cache Line States","summary":"Introduces the basic coherence states tracked per cache line.","main_text":"Goal: Track state of each cache line, in the cache of each core. Possible states: Invalid: This core doesn’t have a copy. Shared: This core has a copy that other cores may have as well (without changes). Modified: Only this core has copy and has made changes to it.","notes_text":"","keywords":["cache line states","Invalid","Shared","Modified","directory"],"images":[],"layout":{"num_text_boxes":1,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":16,"topic":"Coherence States (MSI Family)","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":13,"chunk_index":0,"title":"Directory Snapshot Example","summary":"Shows a directory view of blocks per core after a write, illustrating state assignments.","main_text":"Directory example after Core1 writes block 0. In L1 cache of Core 1: Block 4 invalid (not present), Block 1 shared (Core 2 has it too), Block 0 modified (Core 2 has a copy, but it shouldn’t!). Directory: Core 1 B0 modified, B1–3 shared, B4 invalid. Core 2 B0–1 shared, B2–3 invalid, B4–5 shared.","notes_text":"","keywords":["directory","modified","shared","invalid","block states"],"images":[{"description":"Diagram linking per-core caches to a directory state table at L3.", "labels":["Directory","B0 modified","B1 shared"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":16,"topic":"Directory Tracking","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":14,"chunk_index":0,"title":"Approaches: Directory vs Snooping","summary":"Compares the two main families of coherence protocols and where they are used.","main_text":"Approaches. Directory-Based (common in SMPs like i7 and DSMs like Xeon): Reads/writes update state of each block in a directory (usually at L3). The directory knows who has the block, propagates changes. Single directory for all cores in SMPs, distributed directory in DSMs. Snooping (common in SMPs, also some DSMs like Opteron): L1/L2 cache controllers of each core “snoop” read/write events, broadcast by others, track block states locally, invalidate caches.","notes_text":"","keywords":["directory-based","snooping","L3 directory","broadcast","invalidate"],"images":[{"description":"Side-by-side diagrams of directory-based and snooping coherence.", "labels":["Directory-Based","Snooping"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"comparison"},"metadata":{"course":"CS356","unit":16,"topic":"Coherence Approaches","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":15,"chunk_index":0,"title":"How Snooping Works (Example)","summary":"Step example where cores broadcast misses and invalidations, and a modified core supplies dirty data.","main_text":"Example of Snooping. a) Core1 reads blocks 0,1,2,3; read miss is broadcast by Core1. b) Core2 reads blocks 0,1,4,5; read miss is broadcast by Core2. c) To write to block 0 in L1, Core1 broadcasts an invalidate message; Core2 picks it up and invalidates; then Core1 can modify block 0. d.1) Core2 broadcasts a read miss of 0; Core1 snoops it and sends dirty block. d.2) L3 cache and Core2 receive the modified version. Key points: Core1 tells Core2 to invalidate its copy; Core1 snoops a miss and gives dirty block data to L3/Core2.","notes_text":"","keywords":["snooping","invalidate","read miss","dirty block","broadcast"],"images":[{"description":"Annotated multi-step snooping coherence diagram with misses and invalidates.", "labels":["miss","invalidate","dirty block"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":4,"num_images":1,"dominant_visual_type":"flowchart"},"metadata":{"course":"CS356","unit":16,"topic":"Snooping Example","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":16,"chunk_index":0,"title":"MSI Snooping Protocol Overview","summary":"Defines MSI states and describes read/write miss handling under snooping.","main_text":"MSI Snooping Protocol. Possible block states at the local L1/L2 cache of each core: Invalid (not cached at this core), Shared (cached, same value as L3; other cores can have it too), Modified (cached, different from L3, invalid at other cores). Only one core can have a cache block in the modified state. Protocol: When writing to a shared block, ask other cores to invalidate (and wait). Local state becomes modified; others become invalid. Further writes without notifying. When an invalid block is needed: Read miss asks if another core has it modified; if so, modified value broadcast to L3 and this core (state becomes S). Write miss: core with modified state must writeback & invalidate; state becomes M at this core.","notes_text":"","keywords":["MSI","Invalid","Shared","Modified","read miss","write miss"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":16,"topic":"MSI Protocol","importance_score":9,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":17,"chunk_index":0,"title":"MSI State Machine","summary":"Presents the MSI transition diagram and how to read local vs remote events.","main_text":"MSI Snooping Protocol state transitions among Invalid, Shared, Modified. Local events: local read miss broadcasts miss and gets from L3/bus; local write hit broadcasts invalidate then writes; local write miss broadcasts write miss and gets from L3/bus. Remote events: remote read miss may trigger write back/broadcast of value; remote write miss or invalidate forces transitions to Invalid. HOW TO READ: transition initiated by local event vs remote event with actions taken. Valid states with 2 cores: both invalid; one invalid one shared; one invalid one modified; both shared.","notes_text":"","keywords":["state machine","transitions","local event","remote event","invalidate","writeback"],"images":[{"description":"MSI finite-state diagram for cache-line coherence under snooping.", "labels":["Invalid","Shared","Modified"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"MSI State Transitions","importance_score":9,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":18,"chunk_index":0,"title":"Remote Read of Shared Block","summary":"Shows MSI behavior when a core reads a block that is Shared elsewhere.","main_text":"Remote Read of Shared Block. Core1 has an unmodified copy of L3 data, Core2 wants to read that. Core2 broadcasts read miss, reads from L3, new block state is Shared. Block state at Core1: Shared. Block state at Core2: Shared.","notes_text":"","keywords":["remote read","shared block","read miss","MSI"],"images":[{"description":"Two-core MSI example for remote read while block is Shared.", "labels":["Shared","read miss"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"MSI Examples","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":19,"chunk_index":0,"title":"Remote Read of Modified Block","summary":"Explains MSI behavior when another core reads a block in Modified state.","main_text":"Remote Read of Modified Block. Core1 has a modified value and Core2 broadcasts a miss. Core1 writes back/broadcasts the value; block goes to Shared for both cores. Block state at Core1: Shared. Block state at Core2: Shared.","notes_text":"","keywords":["remote read","modified block","write back","broadcast value","Shared"],"images":[{"description":"Two-core MSI example for remote read miss serviced by Modified owner.", "labels":["Modified","Shared","write back"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"MSI Examples","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":20,"chunk_index":0,"title":"Local Write of Shared Block","summary":"Shows that a writer must invalidate other Shared copies before writing.","main_text":"Local Write of Shared Block. Both cores have an unmodified copy of L3 data. To write, Core1 broadcasts invalidate, Core2 invalidates, and Core1 state becomes Modified.","notes_text":"","keywords":["local write","invalidate","Shared to Modified","MSI"],"images":[{"description":"MSI example of local write to Shared block requiring invalidation.", "labels":["invalidate","Modified"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"MSI Examples","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":21,"chunk_index":0,"title":"Remote Write of Modified Block","summary":"Explains the MSI role reversal when another core writes a block currently Modified elsewhere.","main_text":"Remote Write of Modified. Core1 has a Modified block and Core2 wants to write to it. Core2 broadcasts write miss; Core1 writes back/broadcasts modified value to L3/Core2; block states invert, and Core2 becomes Modified owner.","notes_text":"","keywords":["remote write","write miss","state inversion","Modified owner"],"images":[{"description":"MSI example of remote write miss causing writeback and ownership transfer.", "labels":["write miss","write back","Modified"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"MSI Examples","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":22,"chunk_index":0,"title":"Variants of MSI","summary":"Lists common MSI extensions that add new stable states to reduce traffic.","main_text":"Variants of MSI. MESI adds an Exclusive state: only this core has block (like Modified) but clean (same as L3). No need to send invalidates to modify; just mark as Modified. Exclusive changes to Shared on remote read miss. MESIF (Intel i7) adds Forward state, a specialized Shared state to designate the core that should respond to data requests. MOESI (AMD Opteron) adds Owned state to indicate block is owned by core cache and stale in L3; avoids writebacks upon remote read misses of a locally modified block (core broadcasts value).","notes_text":"","keywords":["MESI","Exclusive","MESIF","Forward","MOESI","Owned"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":16,"topic":"MSI Variants","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":23,"chunk_index":0,"title":"Coherence vs Atomicity","summary":"Warns that coherence actions aren’t fully atomic and can lead to deadlocks or non-atomic instruction behavior.","main_text":"We assumed all coherence operations to be atomic: a write/read miss is broadcast and detected, acquires bus, receives up-to-date value as a single atomic action. In practice, these are distinct operations; non-atomic actions introduce possibility of deadlocks. Coherence protocols are more complex to avoid deadlocks. Even then, coherence does not make assembly instructions atomic! Two threads executing addl $1,0x124 on different cores can both read x, compute x+1, and each write back, resulting in increment by 1 not 2.","notes_text":"","keywords":["atomicity","deadlocks","non-atomic coherence","addl race","bus acquisition"],"images":[{"description":"Timeline/diagram showing non-atomic add across two cores despite coherence.", "labels":["invalidate","miss","x+1"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":16,"topic":"Atomicity Pitfalls","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":24,"chunk_index":0,"title":"Locks Ensure Atomicity","summary":"Shows x86 lock-prefixed instructions and a spinlock using cmpxchg to enforce atomic updates.","main_text":"Locks Ensure Atomicity. Locking individual x86 instructions: lock addl $1,0x124 holds the cache line exclusively (M) by the core during execution; for other cores, it is invalid. Locking multiple instruction with lock cmpxchgl %edx,0x456: “if (*0x456 == %eax) then (*0x456 = %edx) else (%eax = *0x456)”. Spinlock example: get_lock loop reads lock variable, then uses lock cmpxchgl to set it to 1 atomically; execute critical section; release with movl $0,lock_address.","notes_text":"","keywords":["locks","lock prefix","cmpxchg","spinlock","atomic"],"images":[],"layout":{"num_text_boxes":2,"num_images":0,"dominant_visual_type":"text-heavy"},"metadata":{"course":"CS356","unit":16,"topic":"Atomic Operations","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":25,"chunk_index":0,"title":"Directory-Based Coherence Motivation","summary":"Motivates directories for DSM because snooping broadcasts don’t scale.","main_text":"Directory-Based Coherence. DSM distributes memory among cores. Each CPU has its own memory and DDR4 channels (34 GB/s). We can write more data in parallel (more bandwidth). But snooping broadcasts are not scalable with many CPUs/cores. Solution: Directory-Based Coherence Protocols. Each CPU has a directory with state of blocks of its memory. It knows which local/remote cores have copies of blocks. It forwards invalidate/data-fetch requests to those cores only. Easy to implement at L3 cache; used also for SMPs (e.g., Intel i7).","notes_text":"","keywords":["directory-based coherence","DSM scalability","no broadcast","L3 directory","invalidate forwarding"],"images":[{"description":"Diagram showing per-CPU directories controlling coherence for their memory blocks.", "labels":["Directory","DSM"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":2,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"Directory Coherence","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":26,"chunk_index":0,"title":"Directory-Based Example","summary":"Step example showing misses go to directory, directory sends invalidates, and owners write back on demand.","main_text":"Directory-Based Example. a) Core1 reads blocks 0,1,2,3; read miss to directory; data received. b) Core2 reads blocks 0,1,4,5; read miss to directory; data received. c) To write block 0 in L1, Core1 asks directory to send invalidate to nodes with the block; Core2 invalidates; then Core1 modifies block 0. d.1) Core2 sends read miss to directory, which asks Core1 to writeback to L3. d.2) Directory sends modified version to Core2. The directory forwards invalidate/data requests only to cores with the specific block (better scalability, more latency).","notes_text":"","keywords":["directory example","invalidate","owner writeback","read miss","scalability"],"images":[{"description":"Multi-step directory coherence diagram with directory-mediated invalidates and fetches.", "labels":["miss","invalidate","writeback"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"flowchart"},"metadata":{"course":"CS356","unit":16,"topic":"Directory Protocol Example","importance_score":8,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":27,"chunk_index":0,"title":"Protocol at Cores (Directory-Based)","summary":"Gives the per-core state machine for directory coherence, paralleling MSI but via directory messages.","main_text":"Protocol at Cores. States: Uncached (no core has block), Shared (many clean copies), Modified (cached at owner core). Local read miss: send miss to directory. Local write hit: send invalidate to directory, wait, then write. Local write miss: send miss (new tag) to directory, receive. Remote directory commands include invalidate and fetch-invalidate (owner writes back). Similar to snooping but invalidation/fetch interacts with directory, and commands received only for blocks managed by this core.","notes_text":"","keywords":["core protocol","Uncached","Shared","Modified","fetch-invalidate","directory miss"],"images":[{"description":"Directory-based per-core state transition diagram.", "labels":["Uncached","Shared","Modified"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"Directory Core FSM","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":28,"chunk_index":0,"title":"Protocol at L3 Directory","summary":"Provides the directory’s own state transitions for remote read/write misses and writebacks.","main_text":"Protocol at L3 Directory. States: Uncached (no core has block), Shared (copies at core set S), Modified (cached at owner core). Remote read miss at i: if Uncached/Shared, update S and send data. Remote write miss at i: send invalidate to S or fetch-invalidate to owner, set S={i}, send data. Remote writeback clears ownership and may set S={} . Transitions initiated by remote events from core i; directory sends invalidate or fetch commands accordingly.","notes_text":"","keywords":["L3 directory protocol","S set","fetch-invalidate","remote miss","writeback"],"images":[{"description":"Directory state machine describing S-set updates and owner interactions.", "labels":["Uncached","Shared","Modified","S={i}"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"diagram"},"metadata":{"course":"CS356","unit":16,"topic":"Directory FSM","importance_score":7,"file_hash":"<sha256 placeholder>"}}
{"deck_name":"CS356_Unit16_CacheCoherence","slide_number":29,"chunk_index":0,"title":"Coherence Pitfalls: False Sharing","summary":"Explains false sharing and why coherence traffic can devastate performance even without logical data sharing.","main_text":"Coherence Pitfalls for Programmers. False Sharing: When independent data of two threads (i.e., not shared) ends up on a cache line. Performance penalty: each thread is writing only part of the cache line, but coherence forces invalidations when threads run on separate cores. Example code toggling x and y in separate threads causes repeated invalidations because X and Y share a cache line. One solution: 64-byte alignment puts y on different line: int y __attribute__((aligned (64))) = 0.","notes_text":"","keywords":["false sharing","cache line","invalidations","alignment","performance pitfall"],"images":[{"description":"Diagram showing X and Y in one cache line leading to M/I ping-pong; contrasted with aligned separation.", "labels":["Cache Line","X","Y","M","I"],"position":{"x":0.0,"y":0.0,"width":1.0,"height":1.0}}],"layout":{"num_text_boxes":3,"num_images":1,"dominant_visual_type":"mixed"},"metadata":{"course":"CS356","unit":16,"topic":"Programming Pitfalls","importance_score":7,"file_hash":"<sha256 placeholder>"}}

